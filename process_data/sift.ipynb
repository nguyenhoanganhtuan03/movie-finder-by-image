{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e36f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Ki·ªÉm tra phi√™n b·∫£n TensorFlow\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# C·∫•u h√¨nh memory growth ƒë·ªÉ s·ª≠ d·ª•ng GPU hi·ªáu qu·∫£\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"T√¨m th·∫•y {len(gpus)} GPU:\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "    \n",
    "    # C·∫•u h√¨nh memory growth\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # M·ªôt s·ªë t√πy ch·ªçn ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t cho GPU T4\n",
    "        os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "        os.environ['TF_GPU_THREAD_COUNT'] = '2'  # T∆∞∆°ng ·ª©ng v·ªõi s·ªë GPU\n",
    "        \n",
    "        # Hi·ªÉn th·ªã c√°c GPU logic\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"S·ªë l∆∞·ª£ng GPU v·∫≠t l√Ω: {len(gpus)}, s·ªë l∆∞·ª£ng GPU logic: {len(logical_gpus)}\")\n",
    "        \n",
    "        # Th√¥ng tin chi ti·∫øt v·ªÅ GPU\n",
    "        from tensorflow.python.client import device_lib\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        gpu_list = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "        print(f\"Danh s√°ch GPU: {gpu_list}\")\n",
    "        \n",
    "        # Hi·ªÉn th·ªã th√¥ng tin CUDA v√† cuDNN\n",
    "        build_info = tf.sysconfig.get_build_info()\n",
    "        print(f\"CUDA version: {build_info.get('cuda_version', 'N/A')}\")\n",
    "        print(f\"cuDNN version: {build_info.get('cudnn_version', 'N/A')}\")\n",
    "        \n",
    "        # Ki·ªÉm tra xem GPU c√≥ th·ª±c s·ª± ƒë∆∞·ª£c s·ª≠ d·ª•ng hay kh√¥ng\n",
    "        print(\"\\nX√°c nh·∫≠n GPU ƒëang ho·∫°t ƒë·ªông b·∫±ng ph√©p t√≠nh nh·ªè:\")\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "            b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(f\"T√≠nh to√°n tr√™n GPU: {c}\")\n",
    "            print(f\"ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: {c.device}\")\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        print(f\"L·ªói khi c·∫•u h√¨nh GPU: {e}\")\n",
    "else:\n",
    "    print(\"Kh√¥ng t√¨m th·∫•y GPU! ƒêang s·ª≠ d·ª•ng CPU.\")\n",
    "    \n",
    "    # Ki·ªÉm tra th√¥ng tin CPU\n",
    "    cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "    print(f\"T√¨m th·∫•y {len(cpu_devices)} CPU: {cpu_devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a071fd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import gc\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm \n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, Flatten, Dropout, AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, Resizing\n",
    "from tensorflow.keras.layers import MaxPooling2D, Activation, BatchNormalization, Attention, Reshape, RepeatVector, Lambda, Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import joblib\n",
    "\n",
    "import onnxruntime as ort\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "# B·ªè qua c√°c c·∫£nh b√°o\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# In phi√™n b·∫£n TensorFlow hi·ªán t·∫°i\n",
    "print('TensorFlow Version ' + tf.__version__)\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    # Thi·∫øt l·∫≠p seed ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh t√°i l·∫≠p (reproducibility)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# G·ªçi h√†m seed_everything ƒë·ªÉ thi·∫øt l·∫≠p seed m·∫∑c ƒë·ªãnh\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca54413",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "n_classes = 45\n",
    "batch_size = 32\n",
    "\n",
    "classes_train = {\n",
    "    1: \"21_Ngay_Yeu_Em\",\n",
    "    2: \"4_Nam_2_Chang_1_Tinh_Yeu\",\n",
    "    3: \"An_Tet_Ben_Con\",\n",
    "    4: \"Bay_Ngot_Ngao\",\n",
    "    5: \"Benh_Vien_Ma\",\n",
    "    6: \"Bi_Mat_Lai_Bi_Mat\",\n",
    "    7: \"Bi_Mat_Trong_Suong_Mu\",\n",
    "    8: \"Bo_Tu_Oan_Gia\",\n",
    "    9: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    10: \"Chu_Tich_Giao_Hang\",\n",
    "    11: \"Chuyen_Tet\",\n",
    "    12: \"Co_Ba_Sai_Gon\",\n",
    "    13: \"Dao_Pho_Va_Piano\",\n",
    "    14: \"Dat_Rung_Phuong_Nam\",\n",
    "    15: \"Dia_Dao\",\n",
    "    16: \"Dinh_Menh_Thien_Y\",\n",
    "    17: \"Doi_Mat_Am_Duong\",\n",
    "    18: \"Em_Chua_18\",\n",
    "    19: \"Em_La_Cua_Em\",\n",
    "    20: \"Gai_Gia_Lam_Chieu_3\",\n",
    "    21: \"Gia_Ngheo_Gap_Phat\",\n",
    "    22: \"Hem_Cut\",\n",
    "    23: \"Hoan_Doi\",\n",
    "    24: \"Ke_An_Danh\",\n",
    "    25: \"Ke_An_Hon\",\n",
    "    26: \"Lam_Giau_Voi_Ma\",\n",
    "    27: \"Lat_Mat_1\",\n",
    "    28: \"Lo_Mat\",\n",
    "    29: \"Ma_Da\",\n",
    "    30: \"Mat_Biec\",\n",
    "    31: \"Nghe_Sieu_De\",\n",
    "    32: \"Nhung_Nu_Hon_Ruc_Ro\",\n",
    "    33: \"Ong_Ngoai_Tuoi_30\",\n",
    "    34: \"Phap_Su_Tap_Su\",\n",
    "    35: \"Quy_Cau\",\n",
    "    36: \"Quy_Co_Thua_Ke\",\n",
    "    37: \"Ra_Mat_Gia_Tien\",\n",
    "    38: \"Sieu_Lua_Gap_Sieu_Lay\",\n",
    "    39: \"Sieu_Tro_Ly\",\n",
    "    40: \"Tam_Cam_Chuyen_Chua_Ke\",\n",
    "    41: \"Taxi_Em_Ten_Gi\",\n",
    "    42: \"The_Call\",\n",
    "    43: \"Thien_Menh_Anh_Hung\",\n",
    "    44: \"Tieu_Thu_Va_Ba_Dau_Gau\",\n",
    "    45: \"Tren_Ban_Nhau_Duoi_Ban_Muu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40718beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_test = {\n",
    "    1: \"21_Ngay_Yeu_Em\",\n",
    "    2: \"4_Nam_2_Chang_1_Tinh_Yeu\",\n",
    "    3: \"An_Tet_Ben_Con\",\n",
    "    4: \"Bay_Ngot_Ngao\",\n",
    "    5: \"Benh_Vien_Ma\",\n",
    "    6: \"Bi_Mat_Lai_Bi_Mat\",\n",
    "    7: \"Bi_Mat_Trong_Suong_Mu\",\n",
    "    8: \"Bo_Tu_Oan_Gia\",\n",
    "    9: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    10: \"Chu_Tich_Giao_Hang\",\n",
    "    11: \"Chuyen_Tet\",\n",
    "    12: \"Co_Ba_Sai_Gon\",\n",
    "    13: \"Dao_Pho_Va_Piano\",\n",
    "    14: \"Dat_Rung_Phuong_Nam\",\n",
    "    15: \"Dia_Dao\",\n",
    "    16: \"Dinh_Menh_Thien_Y\",\n",
    "    17: \"Doi_Mat_Am_Duong\",\n",
    "    18: \"Em_Chua_18\",\n",
    "    19: \"Em_La_Cua_Em\",\n",
    "    20: \"Gai_Gia_Lam_Chieu_3\",\n",
    "    21: \"Gia_Ngheo_Gap_Phat\",\n",
    "    22: \"Hem_Cut\",\n",
    "    23: \"Hoan_Doi\",\n",
    "    24: \"Ke_An_Danh\",\n",
    "    25: \"Ke_An_Hon\",\n",
    "    26: \"Lam_Giau_Voi_Ma\",\n",
    "    27: \"Lat_Mat_1\",\n",
    "    28: \"Lo_Mat\",\n",
    "    29: \"Ma_Da\",\n",
    "    30: \"Mat_Biec\",\n",
    "    31: \"Nghe_Sieu_De\",\n",
    "    32: \"Nhung_Nu_Hon_Ruc_Ro\",\n",
    "    33: \"Ong_Ngoai_Tuoi_30\",\n",
    "    34: \"Phap_Su_Tap_Su\",\n",
    "    35: \"Quy_Cau\",\n",
    "    36: \"Quy_Co_Thua_Ke\",\n",
    "    37: \"Ra_Mat_Gia_Tien\",\n",
    "    38: \"Sieu_Lua_Gap_Sieu_Lay\",\n",
    "    39: \"Sieu_Tro_Ly\",\n",
    "    40: \"Tam_Cam_Chuyen_Chua_Ke\",\n",
    "    41: \"Taxi_Em_Ten_Gi\",\n",
    "    42: \"The_Call\",\n",
    "    43: \"Thien_Menh_Anh_Hung\",\n",
    "    44: \"Tieu_Thu_Va_Ba_Dau_Gau\",\n",
    "    45: \"Tren_Ban_Nhau_Duoi_Ban_Muu\",\n",
    "    46: \"Khac\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71e0615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Tr√≠ch xu·∫•t SIFT t·ª´ t·∫≠p Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 21_Ngay_Yeu_Em: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1588/1588 [00:26<00:00, 60.27it/s]\n",
      "Processing 4_Nam_2_Chang_1_Tinh_Yeu: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1511/1511 [00:21<00:00, 69.54it/s]\n",
      "Processing An_Tet_Ben_Con: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1994/1994 [00:28<00:00, 69.87it/s]\n",
      "Processing Bay_Ngot_Ngao: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1352/1352 [00:17<00:00, 77.32it/s]\n",
      "Processing Benh_Vien_Ma: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1081/1081 [00:13<00:00, 78.00it/s]\n",
      "Processing Bi_Mat_Lai_Bi_Mat: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1644/1644 [00:24<00:00, 67.90it/s]\n",
      "Processing Bi_Mat_Trong_Suong_Mu: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3618/3618 [00:49<00:00, 73.78it/s]\n",
      "Processing Bo_Tu_Oan_Gia: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2582/2582 [00:35<00:00, 72.82it/s]\n",
      "Processing Cho_Em_Den_Ngay_Mai: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1048/1048 [00:14<00:00, 72.23it/s]\n",
      "Processing Chu_Tich_Giao_Hang: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2810/2810 [00:38<00:00, 73.35it/s]\n",
      "Processing Chuyen_Tet: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1305/1305 [00:20<00:00, 63.84it/s]\n",
      "Processing Co_Ba_Sai_Gon: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 914/914 [00:15<00:00, 57.52it/s]\n",
      "Processing Dao_Pho_Va_Piano: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 681/681 [00:11<00:00, 58.27it/s]\n",
      "Processing Dat_Rung_Phuong_Nam: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1347/1347 [00:27<00:00, 49.67it/s]\n",
      "Processing Dia_Dao: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1463/1463 [00:33<00:00, 43.87it/s]\n",
      "Processing Dinh_Menh_Thien_Y: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1723/1723 [00:33<00:00, 51.49it/s]\n",
      "Processing Doi_Mat_Am_Duong: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 856/856 [00:15<00:00, 55.53it/s]\n",
      "Processing Em_Chua_18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1683/1683 [00:29<00:00, 56.96it/s]\n",
      "Processing Em_La_Cua_Em: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1395/1395 [00:22<00:00, 61.00it/s]\n",
      "Processing Gai_Gia_Lam_Chieu_3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1110/1110 [00:18<00:00, 59.39it/s]\n",
      "Processing Gia_Ngheo_Gap_Phat: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2101/2101 [00:36<00:00, 57.12it/s]\n",
      "Processing Hem_Cut: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2907/2907 [00:48<00:00, 59.87it/s]\n",
      "Processing Hoan_Doi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1361/1361 [00:20<00:00, 65.59it/s]\n",
      "Processing Ke_An_Danh: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1011/1011 [00:14<00:00, 68.53it/s]\n",
      "Processing Ke_An_Hon: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 684/684 [00:09<00:00, 74.00it/s]\n",
      "Processing Lam_Giau_Voi_Ma: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1090/1090 [00:16<00:00, 64.49it/s]\n",
      "Processing Lat_Mat_1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1442/1442 [00:22<00:00, 63.58it/s]\n",
      "Processing Lo_Mat: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1142/1142 [00:20<00:00, 55.56it/s]\n",
      "Processing Ma_Da: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644/644 [00:09<00:00, 69.34it/s]\n",
      "Processing Mat_Biec: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1487/1487 [00:20<00:00, 71.39it/s]\n",
      "Processing Nghe_Sieu_De: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 639/639 [00:08<00:00, 72.08it/s]\n",
      "Processing Nhung_Nu_Hon_Ruc_Ro: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1477/1477 [00:19<00:00, 74.80it/s]\n",
      "Processing Ong_Ngoai_Tuoi_30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1456/1456 [00:19<00:00, 76.11it/s]\n",
      "Processing Phap_Su_Tap_Su: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2106/2106 [00:27<00:00, 75.98it/s]\n",
      "Processing Quy_Cau: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 769/769 [00:09<00:00, 82.67it/s]\n",
      "Processing Quy_Co_Thua_Ke: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1311/1311 [00:17<00:00, 76.85it/s]\n",
      "Processing Ra_Mat_Gia_Tien: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1434/1434 [00:18<00:00, 76.28it/s]\n",
      "Processing Sieu_Lua_Gap_Sieu_Lay: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1156/1156 [00:15<00:00, 75.69it/s]\n",
      "Processing Sieu_Tro_Ly: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2294/2294 [00:29<00:00, 77.64it/s]\n",
      "Processing Tam_Cam_Chuyen_Chua_Ke: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1814/1814 [00:24<00:00, 73.44it/s]\n",
      "Processing Taxi_Em_Ten_Gi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1466/1466 [00:19<00:00, 73.86it/s]\n",
      "Processing The_Call: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 525/525 [00:06<00:00, 80.79it/s]\n",
      "Processing Thien_Menh_Anh_Hung: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1236/1236 [00:16<00:00, 76.06it/s]\n",
      "Processing Tieu_Thu_Va_Ba_Dau_Gau: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1831/1831 [00:24<00:00, 74.74it/s]\n",
      "Processing Tren_Ban_Nhau_Duoi_Ban_Muu: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1020/1020 [00:13<00:00, 77.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ th√™m 65958 vector v√†o FAISS.\n",
      "üì∏ T·ªïng s·ªë ·∫£nh test: 23549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing with SIFT+FAISS: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23549/23549 [05:56<00:00, 66.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Accuracy: 42.34%\n",
      "‚úÖ ƒê√∫ng: 9944 / ‚ùå Sai: 13542\n",
      "‚è±Ô∏è Avg time: 0.0150 gi√¢y/·∫£nh\n",
      "üñºÔ∏è Confusion matrix saved to confusion_matrix_sift_faiss.jpg\n",
      "üìÑ Report saved to classification_report_sift_faiss.csv\n",
      "\n",
      "üìä Evaluation Metrics:\n",
      "   üéØ Accuracy: 0.4234\n",
      "   üìä Macro Precision: 0.4005\n",
      "   üìä Macro Recall: 0.4563\n",
      "   üìä Macro F1-score: 0.4161\n",
      "   üìä Weighted Precision: 0.4463\n",
      "   üìä Weighted Recall: 0.4234\n",
      "   üìä Weighted F1-score: 0.3887\n",
      "   üöÄ Avg Processing Time: 0.0150 sec/image\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "image_size = 224\n",
    "vector_length = 128  # SIFT descriptor length\n",
    "random.seed(42)\n",
    "\n",
    "train_path = 'E:\\\\Data\\\\Movie_Dataset\\\\Process_Frames_2\\\\Train'\n",
    "test_path = 'E:\\\\Data\\\\Movie_Dataset\\\\Process_Frames_2\\\\Test'\n",
    "\n",
    "index_path = \"faiss_features.index\"\n",
    "label_path = \"faiss_labels.npy\"\n",
    "\n",
    "confusion_output_path = \"confusion_matrix_sift_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_sift_faiss.csv\"\n",
    "similarity_threshold = 0.8\n",
    "\n",
    "# Mapping label (b·∫°n c·∫ßn ƒë·ªãnh nghƒ©a s·∫µn dict n√†y)\n",
    "classes_train = classes_train\n",
    "classes_test = classes_test\n",
    "\n",
    "# ================== H√ÄM TI·ªÜN √çCH ==================\n",
    "def l2_normalize(vectors):\n",
    "    if vectors.ndim == 1:\n",
    "        vectors = vectors.reshape(1, -1)\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)\n",
    "\n",
    "\n",
    "def compute_rootsift(descriptors):\n",
    "    eps = 1e-7\n",
    "    descriptors = descriptors.astype(np.float32)\n",
    "    l1_norm = np.linalg.norm(descriptors, ord=1, axis=1, keepdims=True)\n",
    "    descriptors /= (l1_norm + eps)\n",
    "    descriptors = np.sqrt(descriptors)\n",
    "    return descriptors\n",
    "\n",
    "def extract_sift_mean_vector(img_path, sift):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    if descriptors is None or len(descriptors) == 0:\n",
    "        return None\n",
    "    descriptors = compute_rootsift(descriptors)\n",
    "    descriptors = l2_normalize(descriptors)\n",
    "    mean_vector = np.mean(descriptors, axis=0).astype(np.float32)\n",
    "    return mean_vector\n",
    "\n",
    "# ================== PH·∫¶N 1: TR√çCH XU·∫§T TRAIN & L∆ØU FAISS ==================\n",
    "def build_faiss_index(train_path, classes):\n",
    "    sift = cv2.SIFT_create()\n",
    "    features_list, labels_list = [], []\n",
    "\n",
    "    print(\"üß© Tr√≠ch xu·∫•t SIFT t·ª´ t·∫≠p Train...\")\n",
    "    for label_id, label_name in classes.items():\n",
    "        label_dir = os.path.join(train_path, label_name)\n",
    "        if not os.path.isdir(label_dir):\n",
    "            continue\n",
    "        for filename in tqdm(os.listdir(label_dir), desc=f\"Processing {label_name}\"):\n",
    "            img_path = os.path.join(label_dir, filename)\n",
    "            mean_vector = extract_sift_mean_vector(img_path, sift)\n",
    "            if mean_vector is None:\n",
    "                continue\n",
    "            features_list.append(mean_vector)\n",
    "            labels_list.append(label_id - 1)\n",
    "\n",
    "    features_array = np.array(features_list, dtype=np.float32)\n",
    "    labels_array = np.array(labels_list)\n",
    "\n",
    "    # Chu·∫©n h√≥a L2\n",
    "    normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "    # FAISS index\n",
    "    d = normalized_features.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(normalized_features)\n",
    "\n",
    "    # L∆∞u\n",
    "    faiss.write_index(index, index_path)\n",
    "    np.save(label_path, labels_array)\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ th√™m {index.ntotal} vector v√†o FAISS.\")\n",
    "    return index, labels_array\n",
    "\n",
    "def evaluate_faiss(test_path, classes, index, index_labels):\n",
    "    sift = cv2.SIFT_create()\n",
    "    class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "    idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "    # L·∫•y to√†n b·ªô ·∫£nh test\n",
    "    all_images = []\n",
    "    for class_name in os.listdir(test_path):\n",
    "        class_dir = os.path.join(test_path, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        image_files = [\n",
    "            os.path.join(class_dir, f)\n",
    "            for f in os.listdir(class_dir)\n",
    "            if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "        ]\n",
    "        all_images.extend(image_files)\n",
    "\n",
    "    print(f\"üì∏ T·ªïng s·ªë ·∫£nh test: {len(all_images)}\")\n",
    "\n",
    "    y_true, y_pred, processing_times = [], [], []\n",
    "\n",
    "    for img_path in tqdm(all_images, desc=\"Testing with SIFT+FAISS\"):\n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            continue\n",
    "        start_time = time.time()\n",
    "\n",
    "        feature = extract_sift_mean_vector(img_path, sift)\n",
    "        if feature is None:\n",
    "            continue\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)\n",
    "        feature = feature.reshape(1, -1).astype(np.float32)\n",
    "\n",
    "        D, I = index.search(feature, 1)\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared / 2\n",
    "\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = len(classes)  # \"Kh√°c\"\n",
    "        else:\n",
    "            pred_label = int(index_labels[I[0][0]]) + 1\n",
    "\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        processing_times.append(time.time() - start_time)\n",
    "\n",
    "    # === ƒê√°nh gi√° ===\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "\n",
    "    print(f\"\\n‚úÖ Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"‚úÖ ƒê√∫ng: {np.sum(y_true == y_pred)} / ‚ùå Sai: {np.sum(y_true != y_pred)}\")\n",
    "\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"‚è±Ô∏è Avg time: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"üñºÔ∏è Confusion matrix saved to {confusion_output_path}\")\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=4, output_dict=True, zero_division=0\n",
    "    )\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"üìÑ Report saved to {csv_output_path}\")\n",
    "\n",
    "    # Th·ªëng k√™ macro & weighted\n",
    "    macro_avg = report[\"macro avg\"]\n",
    "    weighted_avg = report[\"weighted avg\"]\n",
    "\n",
    "    print(\"\\nüìä Evaluation Metrics:\")\n",
    "    print(f\"   üéØ Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   üìä Macro Precision: {macro_avg['precision']:.4f}\")\n",
    "    print(f\"   üìä Macro Recall: {macro_avg['recall']:.4f}\")\n",
    "    print(f\"   üìä Macro F1-score: {macro_avg['f1-score']:.4f}\")\n",
    "    print(f\"   üìä Weighted Precision: {weighted_avg['precision']:.4f}\")\n",
    "    print(f\"   üìä Weighted Recall: {weighted_avg['recall']:.4f}\")\n",
    "    print(f\"   üìä Weighted F1-score: {weighted_avg['f1-score']:.4f}\")\n",
    "    print(f\"   üöÄ Avg Processing Time: {avg_processing_time:.4f} sec/image\")\n",
    "\n",
    "    return accuracy, avg_processing_time\n",
    "\n",
    "\n",
    "# ================== MAIN ==================\n",
    "if __name__ == \"__main__\":\n",
    "    # Train + FAISS\n",
    "    index, index_labels = build_faiss_index(train_path, classes_train)\n",
    "\n",
    "    # Test\n",
    "    evaluate_faiss(test_path, classes_test, index, index_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
