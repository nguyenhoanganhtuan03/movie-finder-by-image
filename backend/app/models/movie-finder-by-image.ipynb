{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-10T11:14:24.337942Z",
     "iopub.status.busy": "2025-05-10T11:14:24.337718Z",
     "iopub.status.idle": "2025-05-10T11:16:58.916126Z",
     "shell.execute_reply": "2025-05-10T11:16:58.915462Z",
     "shell.execute_reply.started": "2025-05-10T11:14:24.337919Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-10T13:06:54.531291Z",
     "iopub.status.busy": "2025-05-10T13:06:54.530726Z",
     "iopub.status.idle": "2025-05-10T13:07:19.110663Z",
     "shell.execute_reply": "2025-05-10T13:07:19.109999Z",
     "shell.execute_reply.started": "2025-05-10T13:06:54.531267Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q faiss-cpu\n",
    "!pip install -q tensorflow==2.18.0\n",
    "!pip install -q skl2onnx onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:07:24.198077Z",
     "iopub.status.busy": "2025-05-10T13:07:24.197369Z",
     "iopub.status.idle": "2025-05-10T13:07:37.694540Z",
     "shell.execute_reply": "2025-05-10T13:07:37.693790Z",
     "shell.execute_reply.started": "2025-05-10T13:07:24.198041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Kiểm tra phiên bản TensorFlow\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Cấu hình memory growth để sử dụng GPU hiệu quả\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"Tìm thấy {len(gpus)} GPU:\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "    \n",
    "    # Cấu hình memory growth\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # Một số tùy chọn để tối ưu hiệu suất cho GPU T4\n",
    "        os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "        os.environ['TF_GPU_THREAD_COUNT'] = '2'  # Tương ứng với số GPU\n",
    "        \n",
    "        # Hiển thị các GPU logic\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"Số lượng GPU vật lý: {len(gpus)}, số lượng GPU logic: {len(logical_gpus)}\")\n",
    "        \n",
    "        # Thông tin chi tiết về GPU\n",
    "        from tensorflow.python.client import device_lib\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        gpu_list = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "        print(f\"Danh sách GPU: {gpu_list}\")\n",
    "        \n",
    "        # Hiển thị thông tin CUDA và cuDNN\n",
    "        build_info = tf.sysconfig.get_build_info()\n",
    "        print(f\"CUDA version: {build_info.get('cuda_version', 'N/A')}\")\n",
    "        print(f\"cuDNN version: {build_info.get('cudnn_version', 'N/A')}\")\n",
    "        \n",
    "        # Kiểm tra xem GPU có thực sự được sử dụng hay không\n",
    "        print(\"\\nXác nhận GPU đang hoạt động bằng phép tính nhỏ:\")\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "            b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(f\"Tính toán trên GPU: {c}\")\n",
    "            print(f\"Đang chạy trên thiết bị: {c.device}\")\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        print(f\"Lỗi khi cấu hình GPU: {e}\")\n",
    "else:\n",
    "    print(\"Không tìm thấy GPU! Đang sử dụng CPU.\")\n",
    "    \n",
    "    # Kiểm tra thông tin CPU\n",
    "    cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "    print(f\"Tìm thấy {len(cpu_devices)} CPU: {cpu_devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:07:39.717217Z",
     "iopub.status.busy": "2025-05-10T13:07:39.716717Z",
     "iopub.status.idle": "2025-05-10T13:07:41.728132Z",
     "shell.execute_reply": "2025-05-10T13:07:41.727500Z",
     "shell.execute_reply.started": "2025-05-10T13:07:39.717195Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import gc\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm \n",
    "import shutil\n",
    "\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, Flatten, Dropout, AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, Resizing\n",
    "from tensorflow.keras.layers import MaxPooling2D, Activation, BatchNormalization, Attention, Reshape, RepeatVector, Lambda, Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import joblib\n",
    "\n",
    "import onnxruntime as ort\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "# Bỏ qua các cảnh báo\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# In phiên bản TensorFlow hiện tại\n",
    "print('TensorFlow Version ' + tf.__version__)\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    # Thiết lập seed để đảm bảo tính tái lập (reproducibility)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Gọi hàm seed_everything để thiết lập seed mặc định\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trực quan dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:23:26.807364Z",
     "iopub.status.busy": "2025-05-10T11:23:26.806818Z",
     "iopub.status.idle": "2025-05-10T11:24:34.105943Z",
     "shell.execute_reply": "2025-05-10T11:24:34.105351Z",
     "shell.execute_reply.started": "2025-05-10T11:23:26.807331Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Thư mục Train====\n",
      "==> Tổng số ảnh: 39833\n",
      "====Thư mục Test====\n",
      "==> Tổng số ảnh: 33766\n"
     ]
    }
   ],
   "source": [
    "def count_images_per_folder(root_dir, image_extensions=None):\n",
    "    if image_extensions is None:\n",
    "        image_extensions = ['.jpg']\n",
    "\n",
    "    total = 0\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        count = sum(1 for file in files if any(file.lower().endswith(ext) for ext in image_extensions))\n",
    "        if count > 0:\n",
    "            relative_path = os.path.relpath(subdir, root_dir)\n",
    "            # print(f\"Thư mục '{relative_path}': {count} ảnh\")\n",
    "            total += count\n",
    "\n",
    "    print(f\"==> Tổng số ảnh: {total}\")\n",
    "\n",
    "print(\"====Thư mục Train====\")\n",
    "folder_train_path = '/kaggle/input/frames-film-vietnam-dataset/Frame_Train'\n",
    "count_images_per_folder(folder_train_path)\n",
    "\n",
    "print(\"====Thư mục Test====\")\n",
    "folder_test_path = '/kaggle/input/frames-film-vietnam-dataset/Frame_Test'\n",
    "count_images_per_folder(folder_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:24:39.432764Z",
     "iopub.status.busy": "2025-05-10T11:24:39.432172Z",
     "iopub.status.idle": "2025-05-10T11:24:54.927176Z",
     "shell.execute_reply": "2025-05-10T11:24:54.926525Z",
     "shell.execute_reply.started": "2025-05-10T11:24:39.432739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_images_per_folder(root_dir, image_extensions=None):\n",
    "    if image_extensions is None:\n",
    "        image_extensions = ['.jpg']\n",
    "\n",
    "    folder_counts = {}\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        if subdir == root_dir:\n",
    "            continue  # bỏ qua thư mục gốc\n",
    "        count = sum(1 for file in files if any(file.lower().endswith(ext) for ext in image_extensions))\n",
    "        if count > 0:\n",
    "            folder_name = os.path.basename(subdir)\n",
    "            folder_counts[folder_name] = count\n",
    "\n",
    "    return folder_counts\n",
    "\n",
    "def plot_image_counts(folder_counts):\n",
    "    folders = list(folder_counts.keys())\n",
    "    counts = list(folder_counts.values())\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.bar(folders, counts, color='skyblue')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('Tên thư mục con')\n",
    "    plt.ylabel('Số lượng ảnh')\n",
    "    plt.title('Số lượng ảnh trong từng thư mục con')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Tập Train\n",
    "counts = count_images_per_folder(folder_train_path)\n",
    "plot_image_counts(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T03:58:17.026956Z",
     "iopub.status.busy": "2025-05-10T03:58:17.026136Z",
     "iopub.status.idle": "2025-05-10T03:58:30.128570Z",
     "shell.execute_reply": "2025-05-10T03:58:30.127791Z",
     "shell.execute_reply.started": "2025-05-10T03:58:17.026926Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tập Test\n",
    "counts = count_images_per_folder(folder_test_path)\n",
    "plot_image_counts(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tạo tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:08:03.149034Z",
     "iopub.status.busy": "2025-05-10T13:08:03.148440Z",
     "iopub.status.idle": "2025-05-10T13:08:03.154296Z",
     "shell.execute_reply": "2025-05-10T13:08:03.153530Z",
     "shell.execute_reply.started": "2025-05-10T13:08:03.149009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "n_classes = 42\n",
    "batch_size = 128\n",
    "\n",
    "classes = {\n",
    "    1: \"21_Ngay_Yeu_Em\",\n",
    "    2: \"An_Tet_Ben_Con\",\n",
    "    3: \"Bay_Ngot_Ngao\",\n",
    "    4: \"Benh_Vien_Ma\",\n",
    "    5: \"Bi_Mat_Lai_Bi_Mat\",\n",
    "    6: \"Bi_Mat_Trong_Suong_Mu\",\n",
    "    7: \"Bo_Tu_Oan_Gia\",\n",
    "    8: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    9: \"Chu_Tich_Giao_Hang\",\n",
    "    10: \"Chuyen_Tet\",\n",
    "    11: \"Co_Ba_Sai_Gon\",\n",
    "    12: \"Dao_Pho_Va_Piano\",\n",
    "    13: \"Dat_Phuong_Nam\",\n",
    "    14: \"Dia_Dao\",\n",
    "    15: \"Dinh_Menh_Thien_Y\",\n",
    "    16: \"Em_Chua_18\",\n",
    "    17: \"Em_La_Cua_Em\",\n",
    "    18: \"Gai_Gia_Lam_Chieu_3\",\n",
    "    19: \"Gia_Ngheo_Gap_Phat\",\n",
    "    20: \"Hem_Cut\",\n",
    "    21: \"Hoan_Doi\",\n",
    "    22: \"Ke_An_Danh\",\n",
    "    23: \"Ke_An_Hon\",\n",
    "    24: \"Lam_Giau_Voi_Ma\",\n",
    "    25: \"Lat_Mat_1\",\n",
    "    26: \"Lo_Mat\",\n",
    "    27: \"Ma_Da\",\n",
    "    28: \"Mat_Biec\",\n",
    "    29: \"Nhung_Nu_Hon_Ruc_Ro\",\n",
    "    30: \"Oan_Linh__Phan_1\",\n",
    "    31: \"Ong_Ngoai_Tuoi_30\",\n",
    "    32: \"Phap_Su_Tap_Su\",\n",
    "    33: \"Quy_Co_Thua_Ke\",\n",
    "    34: \"Ra_Mat_Gia_Tien\",\n",
    "    35: \"Sieu_Lua_Gap_Sieu_Lay\",\n",
    "    36: \"Sieu_Quay\",\n",
    "    37: \"Sieu_Tro_Ly\",\n",
    "    38: \"Taxi_Em_Ten_Gi\",\n",
    "    39: \"The_Call\",\n",
    "    40: \"Thien_Menh_Anh_Hung\",\n",
    "    41: \"Tieu_Thu_Va_Ba_Dau_Gau\",\n",
    "    42: \"Tren_Ban_Nhau_Duoi_Ban_Muu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:08:05.702558Z",
     "iopub.status.busy": "2025-05-10T13:08:05.701636Z",
     "iopub.status.idle": "2025-05-10T13:08:18.464287Z",
     "shell.execute_reply": "2025-05-10T13:08:18.463752Z",
     "shell.execute_reply.started": "2025-05-10T13:08:05.702523Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39833 files belonging to 42 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = '/kaggle/input/frames-film-vietnam-dataset/Frame_Train'\n",
    "\n",
    "train_df = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',  \n",
    "    seed=1,\n",
    "    image_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:08:22.399474Z",
     "iopub.status.busy": "2025-05-10T13:08:22.398955Z",
     "iopub.status.idle": "2025-05-10T13:08:22.882416Z",
     "shell.execute_reply": "2025-05-10T13:08:22.881644Z",
     "shell.execute_reply.started": "2025-05-10T13:08:22.399444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_df))  \n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(5, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for img, ax in zip(images[:9], axes): \n",
    "    ax.imshow(img.numpy().astype(\"uint8\"))  \n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xóa file trong thư mục"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T14:11:36.459297Z",
     "iopub.status.busy": "2025-05-10T14:11:36.459019Z",
     "iopub.status.idle": "2025-05-10T14:11:36.482402Z",
     "shell.execute_reply": "2025-05-10T14:11:36.481890Z",
     "shell.execute_reply.started": "2025-05-10T14:11:36.459277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def delete_all_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # xóa file hoặc symbolic link\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # xóa thư mục và toàn bộ nội dung bên trong\n",
    "        except Exception as e:\n",
    "            print(f\"Không thể xóa {file_path}: {e}\")\n",
    "\n",
    "# Ví dụ:\n",
    "folder = \"/kaggle/working/\"\n",
    "delete_all_in_folder(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:57:40.459483Z",
     "iopub.status.busy": "2025-05-10T13:57:40.458787Z",
     "iopub.status.idle": "2025-05-10T13:57:42.739010Z",
     "shell.execute_reply": "2025-05-10T13:57:42.738351Z",
     "shell.execute_reply.started": "2025-05-10T13:57:40.459459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50 + KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:26:08.068758Z",
     "iopub.status.busy": "2025-05-10T11:26:08.068045Z",
     "iopub.status.idle": "2025-05-10T11:27:25.406277Z",
     "shell.execute_reply": "2025-05-10T11:27:25.405392Z",
     "shell.execute_reply.started": "2025-05-10T11:26:08.068733Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:   0%|          | 0/312 [00:00<?, ?it/s]I0000 00:00:1746876371.688650      31 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "Extracting features: 100%|██████████| 312/312 [01:13<00:00,  4.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Số batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Khởi tạo model trong context của strategy\n",
    "with strategy.scope():\n",
    "    base_model_resnet50 = ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Nơi lưu đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# Lặp qua từng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_resnet50(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# Đánh nhãn ứng với đặc trưng\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:28:09.125380Z",
     "iopub.status.busy": "2025-05-10T11:28:09.125099Z",
     "iopub.status.idle": "2025-05-10T11:30:22.425631Z",
     "shell.execute_reply": "2025-05-10T11:30:22.424913Z",
     "shell.execute_reply.started": "2025-05-10T11:28:09.125359Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mô hình KNN đã được lưu dưới dạng ONNX.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Huấn luyện mô hình KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(features_array, labels_array)\n",
    "\n",
    "# Chuyển đổi mô hình KNN sang ONNX\n",
    "onnx_model = convert_sklearn(\n",
    "    knn, \n",
    "    initial_types=[('input', FloatTensorType([None, features_array.shape[1]]))],\n",
    "    options={id(knn): {'zipmap': False}}  \n",
    ")\n",
    "\n",
    "# Lưu mô hình dưới dạng ONNX\n",
    "with open(\"knn_model.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(\"Mô hình KNN đã được lưu dưới dạng ONNX.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-10T11:30:45.092946Z",
     "iopub.status.busy": "2025-05-10T11:30:45.092656Z",
     "iopub.status.idle": "2025-05-10T11:41:43.152741Z",
     "shell.execute_reply": "2025-05-10T11:41:43.152004Z",
     "shell.execute_reply.started": "2025-05-10T11:30:45.092927Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 128\n",
    "test_path = \"/kaggle/input/frames-film-vietnam-dataset/Frame_Test\"\n",
    "onnx_model_path = \"/kaggle/working/knn_model.onnx\"\n",
    "confusion_output_path = \"confusion_matrix_knn.jpg\"\n",
    "csv_output_path = \"classification_report_knn.csv\"\n",
    "n_per_class = 50 \n",
    "\n",
    "# ==== Load mô hình và ONNX ====\n",
    "model = ResNet50(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn mô hình ONNX\n",
    "if not os.path.exists(onnx_model_path):\n",
    "    print(f\"❌ Không tìm thấy mô hình ONNX tại: {onnx_model_path}\")\n",
    "\n",
    "# Khởi tạo session ONNX với providers rõ ràng\n",
    "try:\n",
    "    # Kiểm tra GPU, nếu có sẽ sử dụng CUDA\n",
    "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if 'CUDAExecutionProvider' in ort.get_available_providers() else ['CPUExecutionProvider']\n",
    "    onnx_session = ort.InferenceSession(onnx_model_path, providers=providers)\n",
    "    \n",
    "    # In thông tin đầu vào để kiểm tra\n",
    "    input_details = onnx_session.get_inputs()[0]\n",
    "    print(f\"✅ Mô hình ONNX đã được tải thành công!\")\n",
    "    # print(f\"   - Tên đầu vào: {input_details.name}\")\n",
    "    # print(f\"   - Hình dạng đầu vào: {input_details.shape}\")\n",
    "    # print(f\"   - Kiểu dữ liệu đầu vào: {input_details.type}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải mô hình ONNX: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = {\n",
    "    1: \"21_Ngay_Yeu_Em\", 2: \"An_Tet_Ben_Con\", 3: \"Bay_Ngot_Ngao\", 4: \"Benh_Vien_Ma\",\n",
    "    5: \"Bi_Mat_Lai_Bi_Mat\", 6: \"Bi_Mat_Trong_Suong_Mu\", 7: \"Bo_Tu_Oan_Gia\", 8: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    9: \"Chu_Tich_Giao_Hang\", 10: \"Chuyen_Tet\", 11: \"Co_Ba_Sai_Gon\", 12: \"Dao_Pho_Va_Piano\",\n",
    "    13: \"Dat_Phuong_Nam\", 14: \"Dia_Dao\", 15: \"Dinh_Menh_Thien_Y\", 16: \"Em_Chua_18\",\n",
    "    17: \"Em_La_Cua_Em\", 18: \"Gai_Gia_Lam_Chieu_3\", 19: \"Gia_Ngheo_Gap_Phat\", 20: \"Hem_Cut\",\n",
    "    21: \"Hoan_Doi\", 22: \"Ke_An_Danh\", 23: \"Ke_An_Hon\", 24: \"Lam_Giau_Voi_Ma\",\n",
    "    25: \"Lat_Mat_1\", 26: \"Lo_Mat\", 27: \"Ma_Da\", 28: \"Mat_Biec\",\n",
    "    29: \"Nhung_Nu_Hon_Ruc_Ro\", 30: \"Oan_Linh__Phan_1\", 31: \"Ong_Ngoai_Tuoi_30\",\n",
    "    32: \"Phap_Su_Tap_Su\", 33: \"Quy_Co_Thua_Ke\", 34: \"Ra_Mat_Gia_Tien\",\n",
    "    35: \"Sieu_Lua_Gap_Sieu_Lay\", 36: \"Sieu_Quay\", 37: \"Sieu_Tro_Ly\",\n",
    "    38: \"Taxi_Em_Ten_Gi\", 39: \"The_Call\", 40: \"Thien_Menh_Anh_Hung\",\n",
    "    41: \"Tieu_Thu_Va_Ba_Dau_Gau\", 42: \"Tren_Ban_Nhau_Duoi_Ban_Muu\"\n",
    "}\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "\n",
    "    # Lấy danh sách ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "\n",
    "    # Lấy ngẫu nhiên n ảnh từ folder (hoặc ít hơn nếu không đủ)\n",
    "    selected_images = random.sample(image_files, min(n_per_class, len(image_files)))\n",
    "    all_images.extend(selected_images)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "for img_path in tqdm(all_images, desc=\"Testing with KNN\"):\n",
    "    try:\n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "\n",
    "        # Trích đặc trưng với ResNet50\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        if feature.ndim > 2:\n",
    "            feature = feature.reshape(1, -1)\n",
    "        \n",
    "        # Vì chúng ta đảm bảo đầu vào là float32 để tránh lỗi kiểu dữ liệu\n",
    "        feature = feature.astype(np.float32)\n",
    "\n",
    "        # Lấy tên đầu vào từ mô hình ONNX để đảm bảo đúng\n",
    "        input_name = onnx_session.get_inputs()[0].name\n",
    "        \n",
    "        # Dự đoán với mô hình ONNX\n",
    "        outputs = onnx_session.run(None, {input_name: feature})\n",
    "        \n",
    "        if outputs[0].ndim == 2:  \n",
    "            pred_label = np.argmax(outputs[0][0]) + 1 \n",
    "        else:\n",
    "            pred_label = outputs[0][0]\n",
    "\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# print(f\"Hình dạng đầu ra: {[output.shape for output in outputs]}\")\n",
    "# print(f\"Kiểu đầu ra: {[output.dtype for output in outputs]}\")\n",
    "# print(f\"Giá trị đầu ra đầu tiên: {outputs[0][0]}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    if y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1) + 1\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "\n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (KNN)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "\n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "\n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "\n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "\n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_knn.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_knn.csv'\")\n",
    "\n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df.to_csv('final_classification_report_knn.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_knn.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNetB4 + KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:23:07.841289Z",
     "iopub.status.busy": "2025-05-10T13:23:07.841015Z",
     "iopub.status.idle": "2025-05-10T13:25:44.486068Z",
     "shell.execute_reply": "2025-05-10T13:25:44.485306Z",
     "shell.execute_reply.started": "2025-05-10T13:23:07.841271Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
      "\u001b[1m71686520/71686520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features with EfficientNetB4: 100%|██████████| 312/312 [02:33<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "# Số batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Khởi tạo EfficientNetB4 trong context của strategy\n",
    "with strategy.scope():\n",
    "    base_model_efficientnetb4 = EfficientNetB4(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Nơi lưu đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# Lặp qua từng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features with EfficientNetB4\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_efficientnetb4(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# Kết hợp đặc trưng và nhãn\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:25:52.239273Z",
     "iopub.status.busy": "2025-05-10T13:25:52.238560Z",
     "iopub.status.idle": "2025-05-10T13:27:48.336727Z",
     "shell.execute_reply": "2025-05-10T13:27:48.336022Z",
     "shell.execute_reply.started": "2025-05-10T13:25:52.239249Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mô hình KNN đã được lưu dưới dạng ONNX.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Huấn luyện mô hình KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(features_array, labels_array)\n",
    "\n",
    "# Chuyển đổi mô hình KNN sang ONNX\n",
    "onnx_model = convert_sklearn(\n",
    "    knn, \n",
    "    initial_types=[('input', FloatTensorType([None, features_array.shape[1]]))],\n",
    "    options={id(knn): {'zipmap': False}}  \n",
    ")\n",
    "\n",
    "# Lưu mô hình dưới dạng ONNX\n",
    "with open(\"knn_model.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(\"Mô hình KNN đã được lưu dưới dạng ONNX.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:28:03.169943Z",
     "iopub.status.busy": "2025-05-10T13:28:03.169581Z",
     "iopub.status.idle": "2025-05-10T13:38:33.767574Z",
     "shell.execute_reply": "2025-05-10T13:38:33.766806Z",
     "shell.execute_reply.started": "2025-05-10T13:28:03.169921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 128\n",
    "test_path = \"/kaggle/input/frames-film-vietnam-dataset/Frame_Test\"\n",
    "onnx_model_path = \"/kaggle/working/knn_model.onnx\"\n",
    "confusion_output_path = \"confusion_matrix_knn.jpg\"\n",
    "csv_output_path = \"classification_report_knn.csv\"\n",
    "n_per_class = 50 \n",
    "\n",
    "# ==== Load mô hình và ONNX ====\n",
    "model = EfficientNetB4(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn mô hình ONNX\n",
    "if not os.path.exists(onnx_model_path):\n",
    "    print(f\"❌ Không tìm thấy mô hình ONNX tại: {onnx_model_path}\")\n",
    "\n",
    "# Khởi tạo session ONNX với providers rõ ràng\n",
    "try:\n",
    "    # Kiểm tra GPU, nếu có sẽ sử dụng CUDA\n",
    "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if 'CUDAExecutionProvider' in ort.get_available_providers() else ['CPUExecutionProvider']\n",
    "    onnx_session = ort.InferenceSession(onnx_model_path, providers=providers)\n",
    "    \n",
    "    # In thông tin đầu vào để kiểm tra\n",
    "    input_details = onnx_session.get_inputs()[0]\n",
    "    print(f\"✅ Mô hình ONNX đã được tải thành công!\")\n",
    "    # print(f\"   - Tên đầu vào: {input_details.name}\")\n",
    "    # print(f\"   - Hình dạng đầu vào: {input_details.shape}\")\n",
    "    # print(f\"   - Kiểu dữ liệu đầu vào: {input_details.type}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải mô hình ONNX: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = {\n",
    "    1: \"21_Ngay_Yeu_Em\", 2: \"An_Tet_Ben_Con\", 3: \"Bay_Ngot_Ngao\", 4: \"Benh_Vien_Ma\",\n",
    "    5: \"Bi_Mat_Lai_Bi_Mat\", 6: \"Bi_Mat_Trong_Suong_Mu\", 7: \"Bo_Tu_Oan_Gia\", 8: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    9: \"Chu_Tich_Giao_Hang\", 10: \"Chuyen_Tet\", 11: \"Co_Ba_Sai_Gon\", 12: \"Dao_Pho_Va_Piano\",\n",
    "    13: \"Dat_Phuong_Nam\", 14: \"Dia_Dao\", 15: \"Dinh_Menh_Thien_Y\", 16: \"Em_Chua_18\",\n",
    "    17: \"Em_La_Cua_Em\", 18: \"Gai_Gia_Lam_Chieu_3\", 19: \"Gia_Ngheo_Gap_Phat\", 20: \"Hem_Cut\",\n",
    "    21: \"Hoan_Doi\", 22: \"Ke_An_Danh\", 23: \"Ke_An_Hon\", 24: \"Lam_Giau_Voi_Ma\",\n",
    "    25: \"Lat_Mat_1\", 26: \"Lo_Mat\", 27: \"Ma_Da\", 28: \"Mat_Biec\",\n",
    "    29: \"Nhung_Nu_Hon_Ruc_Ro\", 30: \"Oan_Linh__Phan_1\", 31: \"Ong_Ngoai_Tuoi_30\",\n",
    "    32: \"Phap_Su_Tap_Su\", 33: \"Quy_Co_Thua_Ke\", 34: \"Ra_Mat_Gia_Tien\",\n",
    "    35: \"Sieu_Lua_Gap_Sieu_Lay\", 36: \"Sieu_Quay\", 37: \"Sieu_Tro_Ly\",\n",
    "    38: \"Taxi_Em_Ten_Gi\", 39: \"The_Call\", 40: \"Thien_Menh_Anh_Hung\",\n",
    "    41: \"Tieu_Thu_Va_Ba_Dau_Gau\", 42: \"Tren_Ban_Nhau_Duoi_Ban_Muu\"\n",
    "}\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "\n",
    "    # Lấy danh sách ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "\n",
    "    # Lấy ngẫu nhiên n ảnh từ folder (hoặc ít hơn nếu không đủ)\n",
    "    selected_images = random.sample(image_files, min(n_per_class, len(image_files)))\n",
    "    all_images.extend(selected_images)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "for img_path in tqdm(all_images, desc=\"Testing with KNN\"):\n",
    "    try:\n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "\n",
    "        # Trích đặc trưng với ResNet50\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        if feature.ndim > 2:\n",
    "            feature = feature.reshape(1, -1)\n",
    "        \n",
    "        # Vì chúng ta đảm bảo đầu vào là float32 để tránh lỗi kiểu dữ liệu\n",
    "        feature = feature.astype(np.float32)\n",
    "\n",
    "        # Lấy tên đầu vào từ mô hình ONNX để đảm bảo đúng\n",
    "        input_name = onnx_session.get_inputs()[0].name\n",
    "        \n",
    "        # Dự đoán với mô hình ONNX\n",
    "        outputs = onnx_session.run(None, {input_name: feature})\n",
    "        \n",
    "        if outputs[0].ndim == 2:  \n",
    "            pred_label = np.argmax(outputs[0][0]) + 1 \n",
    "        else:\n",
    "            pred_label = outputs[0][0]\n",
    "\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# print(f\"Hình dạng đầu ra: {[output.shape for output in outputs]}\")\n",
    "# print(f\"Kiểu đầu ra: {[output.dtype for output in outputs]}\")\n",
    "# print(f\"Giá trị đầu ra đầu tiên: {outputs[0][0]}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    if y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1) + 1\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "\n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (KNN)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "\n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "\n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "\n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "\n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_knn.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_knn.csv'\")\n",
    "\n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df.to_csv('final_classification_report_knn.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_knn.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InceptionV3 + KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T12:16:41.954772Z",
     "iopub.status.busy": "2025-05-10T12:16:41.954070Z",
     "iopub.status.idle": "2025-05-10T12:18:14.516721Z",
     "shell.execute_reply": "2025-05-10T12:18:14.515910Z",
     "shell.execute_reply.started": "2025-05-10T12:16:41.954742Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features with InceptionV3: 100%|██████████| 312/312 [01:30<00:00,  3.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "# Giả sử biến train_df là tf.data.Dataset đã chuẩn hóa và batch đúng\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Khởi tạo InceptionV3 trong context của strategy\n",
    "with strategy.scope():\n",
    "    base_model_inceptionv3 = InceptionV3(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Trích đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features with InceptionV3\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_inceptionv3(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "features_array = np.concatenate(all_features, axis=0)   \n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T12:18:46.224602Z",
     "iopub.status.busy": "2025-05-10T12:18:46.223834Z",
     "iopub.status.idle": "2025-05-10T12:21:01.942639Z",
     "shell.execute_reply": "2025-05-10T12:21:01.941882Z",
     "shell.execute_reply.started": "2025-05-10T12:18:46.224578Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mô hình KNN đã được lưu dưới dạng ONNX.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Huấn luyện mô hình KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(features_array, labels_array)\n",
    "\n",
    "# Chuyển đổi mô hình KNN sang ONNX\n",
    "onnx_model = convert_sklearn(\n",
    "    knn, \n",
    "    initial_types=[('input', FloatTensorType([None, features_array.shape[1]]))],\n",
    "    options={id(knn): {'zipmap': False}}  \n",
    ")\n",
    "\n",
    "# Lưu mô hình dưới dạng ONNX\n",
    "with open(\"knn_model.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(\"Mô hình KNN đã được lưu dưới dạng ONNX.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-10T12:21:10.470500Z",
     "iopub.status.busy": "2025-05-10T12:21:10.469776Z",
     "iopub.status.idle": "2025-05-10T12:32:35.053032Z",
     "shell.execute_reply": "2025-05-10T12:32:35.052273Z",
     "shell.execute_reply.started": "2025-05-10T12:21:10.470459Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 128\n",
    "test_path = \"/kaggle/input/frames-film-vietnam-dataset/Frame_Test\"\n",
    "onnx_model_path = \"/kaggle/working/knn_model.onnx\"\n",
    "confusion_output_path = \"confusion_matrix_knn.jpg\"\n",
    "csv_output_path = \"classification_report_knn.csv\"\n",
    "n_per_class = 50 \n",
    "\n",
    "# ==== Load mô hình và ONNX ====\n",
    "model = InceptionV3(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn mô hình ONNX\n",
    "if not os.path.exists(onnx_model_path):\n",
    "    print(f\"❌ Không tìm thấy mô hình ONNX tại: {onnx_model_path}\")\n",
    "\n",
    "# Khởi tạo session ONNX với providers rõ ràng\n",
    "try:\n",
    "    # Kiểm tra GPU, nếu có sẽ sử dụng CUDA\n",
    "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if 'CUDAExecutionProvider' in ort.get_available_providers() else ['CPUExecutionProvider']\n",
    "    onnx_session = ort.InferenceSession(onnx_model_path, providers=providers)\n",
    "    \n",
    "    # In thông tin đầu vào để kiểm tra\n",
    "    input_details = onnx_session.get_inputs()[0]\n",
    "    print(f\"✅ Mô hình ONNX đã được tải thành công!\")\n",
    "    # print(f\"   - Tên đầu vào: {input_details.name}\")\n",
    "    # print(f\"   - Hình dạng đầu vào: {input_details.shape}\")\n",
    "    # print(f\"   - Kiểu dữ liệu đầu vào: {input_details.type}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải mô hình ONNX: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = {\n",
    "    1: \"21_Ngay_Yeu_Em\", 2: \"An_Tet_Ben_Con\", 3: \"Bay_Ngot_Ngao\", 4: \"Benh_Vien_Ma\",\n",
    "    5: \"Bi_Mat_Lai_Bi_Mat\", 6: \"Bi_Mat_Trong_Suong_Mu\", 7: \"Bo_Tu_Oan_Gia\", 8: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    9: \"Chu_Tich_Giao_Hang\", 10: \"Chuyen_Tet\", 11: \"Co_Ba_Sai_Gon\", 12: \"Dao_Pho_Va_Piano\",\n",
    "    13: \"Dat_Phuong_Nam\", 14: \"Dia_Dao\", 15: \"Dinh_Menh_Thien_Y\", 16: \"Em_Chua_18\",\n",
    "    17: \"Em_La_Cua_Em\", 18: \"Gai_Gia_Lam_Chieu_3\", 19: \"Gia_Ngheo_Gap_Phat\", 20: \"Hem_Cut\",\n",
    "    21: \"Hoan_Doi\", 22: \"Ke_An_Danh\", 23: \"Ke_An_Hon\", 24: \"Lam_Giau_Voi_Ma\",\n",
    "    25: \"Lat_Mat_1\", 26: \"Lo_Mat\", 27: \"Ma_Da\", 28: \"Mat_Biec\",\n",
    "    29: \"Nhung_Nu_Hon_Ruc_Ro\", 30: \"Oan_Linh__Phan_1\", 31: \"Ong_Ngoai_Tuoi_30\",\n",
    "    32: \"Phap_Su_Tap_Su\", 33: \"Quy_Co_Thua_Ke\", 34: \"Ra_Mat_Gia_Tien\",\n",
    "    35: \"Sieu_Lua_Gap_Sieu_Lay\", 36: \"Sieu_Quay\", 37: \"Sieu_Tro_Ly\",\n",
    "    38: \"Taxi_Em_Ten_Gi\", 39: \"The_Call\", 40: \"Thien_Menh_Anh_Hung\",\n",
    "    41: \"Tieu_Thu_Va_Ba_Dau_Gau\", 42: \"Tren_Ban_Nhau_Duoi_Ban_Muu\"\n",
    "}\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "\n",
    "    # Lấy danh sách ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "\n",
    "    # Lấy ngẫu nhiên n ảnh từ folder (hoặc ít hơn nếu không đủ)\n",
    "    selected_images = random.sample(image_files, min(n_per_class, len(image_files)))\n",
    "    all_images.extend(selected_images)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "for img_path in tqdm(all_images, desc=\"Testing with KNN\"):\n",
    "    try:\n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "\n",
    "        # Trích đặc trưng với ResNet50\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        if feature.ndim > 2:\n",
    "            feature = feature.reshape(1, -1)\n",
    "        \n",
    "        # Vì chúng ta đảm bảo đầu vào là float32 để tránh lỗi kiểu dữ liệu\n",
    "        feature = feature.astype(np.float32)\n",
    "\n",
    "        # Lấy tên đầu vào từ mô hình ONNX để đảm bảo đúng\n",
    "        input_name = onnx_session.get_inputs()[0].name\n",
    "        \n",
    "        # Dự đoán với mô hình ONNX\n",
    "        outputs = onnx_session.run(None, {input_name: feature})\n",
    "        \n",
    "        if outputs[0].ndim == 2:  \n",
    "            pred_label = np.argmax(outputs[0][0]) + 1 \n",
    "        else:\n",
    "            pred_label = outputs[0][0]\n",
    "\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# print(f\"Hình dạng đầu ra: {[output.shape for output in outputs]}\")\n",
    "# print(f\"Kiểu đầu ra: {[output.dtype for output in outputs]}\")\n",
    "# print(f\"Giá trị đầu ra đầu tiên: {outputs[0][0]}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    if y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1) + 1\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "\n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (KNN)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "\n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "\n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "\n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "\n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_knn.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_knn.csv'\")\n",
    "\n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df.to_csv('final_classification_report_knn.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_knn.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 + KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T14:01:29.305738Z",
     "iopub.status.busy": "2025-05-10T14:01:29.305447Z",
     "iopub.status.idle": "2025-05-10T14:02:05.156482Z",
     "shell.execute_reply": "2025-05-10T14:02:05.155713Z",
     "shell.execute_reply.started": "2025-05-10T14:01:29.305715Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 312/312 [00:35<00:00,  8.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Số batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    base_model_vgg16 = VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Nơi lưu đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_vgg16(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# Nối đặc trưng và nhãn\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T14:02:10.190077Z",
     "iopub.status.busy": "2025-05-10T14:02:10.189795Z",
     "iopub.status.idle": "2025-05-10T14:02:46.379947Z",
     "shell.execute_reply": "2025-05-10T14:02:46.379176Z",
     "shell.execute_reply.started": "2025-05-10T14:02:10.190056Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mô hình KNN đã được lưu dưới dạng ONNX.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Huấn luyện mô hình KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(features_array, labels_array)\n",
    "\n",
    "# Chuyển đổi mô hình KNN sang ONNX\n",
    "onnx_model = convert_sklearn(\n",
    "    knn, \n",
    "    initial_types=[('input', FloatTensorType([None, features_array.shape[1]]))],\n",
    "    options={id(knn): {'zipmap': False}}  \n",
    ")\n",
    "\n",
    "# Lưu mô hình dưới dạng ONNX\n",
    "with open(\"knn_model.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(\"Mô hình KNN đã được lưu dưới dạng ONNX.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T14:02:54.311309Z",
     "iopub.status.busy": "2025-05-10T14:02:54.310995Z",
     "iopub.status.idle": "2025-05-10T14:09:44.090395Z",
     "shell.execute_reply": "2025-05-10T14:09:44.089811Z",
     "shell.execute_reply.started": "2025-05-10T14:02:54.311289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 128\n",
    "test_path = \"/kaggle/input/frames-film-vietnam-dataset/Frame_Test\"\n",
    "onnx_model_path = \"/kaggle/working/knn_model.onnx\"\n",
    "confusion_output_path = \"confusion_matrix_knn.jpg\"\n",
    "csv_output_path = \"classification_report_knn.csv\"\n",
    "n_per_class = 50 \n",
    "\n",
    "# ==== Load mô hình và ONNX ====\n",
    "model = VGG16(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn mô hình ONNX\n",
    "if not os.path.exists(onnx_model_path):\n",
    "    print(f\"❌ Không tìm thấy mô hình ONNX tại: {onnx_model_path}\")\n",
    "\n",
    "# Khởi tạo session ONNX với providers rõ ràng\n",
    "try:\n",
    "    # Kiểm tra GPU, nếu có sẽ sử dụng CUDA\n",
    "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if 'CUDAExecutionProvider' in ort.get_available_providers() else ['CPUExecutionProvider']\n",
    "    onnx_session = ort.InferenceSession(onnx_model_path, providers=providers)\n",
    "    \n",
    "    # In thông tin đầu vào để kiểm tra\n",
    "    input_details = onnx_session.get_inputs()[0]\n",
    "    print(f\"✅ Mô hình ONNX đã được tải thành công!\")\n",
    "    # print(f\"   - Tên đầu vào: {input_details.name}\")\n",
    "    # print(f\"   - Hình dạng đầu vào: {input_details.shape}\")\n",
    "    # print(f\"   - Kiểu dữ liệu đầu vào: {input_details.type}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải mô hình ONNX: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = {\n",
    "    1: \"21_Ngay_Yeu_Em\", 2: \"An_Tet_Ben_Con\", 3: \"Bay_Ngot_Ngao\", 4: \"Benh_Vien_Ma\",\n",
    "    5: \"Bi_Mat_Lai_Bi_Mat\", 6: \"Bi_Mat_Trong_Suong_Mu\", 7: \"Bo_Tu_Oan_Gia\", 8: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    9: \"Chu_Tich_Giao_Hang\", 10: \"Chuyen_Tet\", 11: \"Co_Ba_Sai_Gon\", 12: \"Dao_Pho_Va_Piano\",\n",
    "    13: \"Dat_Phuong_Nam\", 14: \"Dia_Dao\", 15: \"Dinh_Menh_Thien_Y\", 16: \"Em_Chua_18\",\n",
    "    17: \"Em_La_Cua_Em\", 18: \"Gai_Gia_Lam_Chieu_3\", 19: \"Gia_Ngheo_Gap_Phat\", 20: \"Hem_Cut\",\n",
    "    21: \"Hoan_Doi\", 22: \"Ke_An_Danh\", 23: \"Ke_An_Hon\", 24: \"Lam_Giau_Voi_Ma\",\n",
    "    25: \"Lat_Mat_1\", 26: \"Lo_Mat\", 27: \"Ma_Da\", 28: \"Mat_Biec\",\n",
    "    29: \"Nhung_Nu_Hon_Ruc_Ro\", 30: \"Oan_Linh__Phan_1\", 31: \"Ong_Ngoai_Tuoi_30\",\n",
    "    32: \"Phap_Su_Tap_Su\", 33: \"Quy_Co_Thua_Ke\", 34: \"Ra_Mat_Gia_Tien\",\n",
    "    35: \"Sieu_Lua_Gap_Sieu_Lay\", 36: \"Sieu_Quay\", 37: \"Sieu_Tro_Ly\",\n",
    "    38: \"Taxi_Em_Ten_Gi\", 39: \"The_Call\", 40: \"Thien_Menh_Anh_Hung\",\n",
    "    41: \"Tieu_Thu_Va_Ba_Dau_Gau\", 42: \"Tren_Ban_Nhau_Duoi_Ban_Muu\"\n",
    "}\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "\n",
    "    # Lấy danh sách ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "\n",
    "    # Lấy ngẫu nhiên n ảnh từ folder (hoặc ít hơn nếu không đủ)\n",
    "    selected_images = random.sample(image_files, min(n_per_class, len(image_files)))\n",
    "    all_images.extend(selected_images)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "for img_path in tqdm(all_images, desc=\"Testing with KNN\"):\n",
    "    try:\n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "\n",
    "        # Trích đặc trưng với ResNet50\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        if feature.ndim > 2:\n",
    "            feature = feature.reshape(1, -1)\n",
    "        \n",
    "        # Vì chúng ta đảm bảo đầu vào là float32 để tránh lỗi kiểu dữ liệu\n",
    "        feature = feature.astype(np.float32)\n",
    "\n",
    "        # Lấy tên đầu vào từ mô hình ONNX để đảm bảo đúng\n",
    "        input_name = onnx_session.get_inputs()[0].name\n",
    "        \n",
    "        # Dự đoán với mô hình ONNX\n",
    "        outputs = onnx_session.run(None, {input_name: feature})\n",
    "        \n",
    "        if outputs[0].ndim == 2:  \n",
    "            pred_label = np.argmax(outputs[0][0]) + 1 \n",
    "        else:\n",
    "            pred_label = outputs[0][0]\n",
    "\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# print(f\"Hình dạng đầu ra: {[output.shape for output in outputs]}\")\n",
    "# print(f\"Kiểu đầu ra: {[output.dtype for output in outputs]}\")\n",
    "# print(f\"Giá trị đầu ra đầu tiên: {outputs[0][0]}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    if y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1) + 1\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "\n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (KNN)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "\n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "\n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "\n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "\n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_knn.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_knn.csv'\")\n",
    "\n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df.to_csv('final_classification_report_knn.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_knn.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50 + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:08:39.260816Z",
     "iopub.status.busy": "2025-05-10T13:08:39.260089Z",
     "iopub.status.idle": "2025-05-10T13:09:54.489943Z",
     "shell.execute_reply": "2025-05-10T13:09:54.489157Z",
     "shell.execute_reply.started": "2025-05-10T13:08:39.260781Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:   0%|          | 0/312 [00:00<?, ?it/s]I0000 00:00:1746882522.517808      31 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "Extracting features: 100%|██████████| 312/312 [01:12<00:00,  4.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Số batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Khởi tạo model trong context của strategy\n",
    "with strategy.scope():\n",
    "    base_model_resnet50 = ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Nơi lưu đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# Lặp qua từng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_resnet50(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# Đánh nhãn ứng với đặc trưng\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:10:11.228204Z",
     "iopub.status.busy": "2025-05-10T13:10:11.227943Z",
     "iopub.status.idle": "2025-05-10T13:10:11.899661Z",
     "shell.execute_reply": "2025-05-10T13:10:11.898904Z",
     "shell.execute_reply.started": "2025-05-10T13:10:11.228188Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã thêm 39833 vector vào FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Kích thước vector đặc trưng\n",
    "d = features_array.shape[1]  \n",
    "\n",
    "# Khởi tạo index\n",
    "index = faiss.IndexFlatL2(d)  # Dùng khoảng cách Euclidean\n",
    "\n",
    "# Thêm các vector vào index\n",
    "index.add(features_array.astype('float32'))\n",
    "print(\"Đã thêm\", index.ntotal, \"vector vào FAISS index.\")\n",
    "\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:10:28.309080Z",
     "iopub.status.busy": "2025-05-10T13:10:28.308815Z",
     "iopub.status.idle": "2025-05-10T13:10:28.659179Z",
     "shell.execute_reply": "2025-05-10T13:10:28.658379Z",
     "shell.execute_reply.started": "2025-05-10T13:10:28.309060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Đọc lại FAISS index và nhãn\n",
    "index = faiss.read_index(\"faiss_features.index\")\n",
    "labels_array = np.load(\"faiss_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-10T13:15:53.453808Z",
     "iopub.status.busy": "2025-05-10T13:15:53.453260Z",
     "iopub.status.idle": "2025-05-10T13:19:28.404333Z",
     "shell.execute_reply": "2025-05-10T13:19:28.403687Z",
     "shell.execute_reply.started": "2025-05-10T13:15:53.453782Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 128\n",
    "test_path = \"/kaggle/input/frames-film-vietnam-dataset/Frame_Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "n_per_class = 50\n",
    "\n",
    "# ==== Load mô hình và FAISS index ====\n",
    "# Load mô hình ResNet50 (hoặc InceptionV3)\n",
    "model = ResNet50(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn FAISS index và labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"❌ Không tìm thấy FAISS index tại: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"❌ Không tìm thấy nhãn tại: {label_path}\")\n",
    "\n",
    "# Load FAISS index và labels\n",
    "try:\n",
    "    # Load index trực tiếp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"✅ FAISS index đã được tải thành công!\")\n",
    "    print(f\"   - Số lượng vectors: {index.ntotal}\")\n",
    "    print(f\"   - Kích thước vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = {\n",
    "    1: \"21_Ngay_Yeu_Em\", 2: \"An_Tet_Ben_Con\", 3: \"Bay_Ngot_Ngao\", 4: \"Benh_Vien_Ma\",\n",
    "    5: \"Bi_Mat_Lai_Bi_Mat\", 6: \"Bi_Mat_Trong_Suong_Mu\", 7: \"Bo_Tu_Oan_Gia\", 8: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    9: \"Chu_Tich_Giao_Hang\", 10: \"Chuyen_Tet\", 11: \"Co_Ba_Sai_Gon\", 12: \"Dao_Pho_Va_Piano\",\n",
    "    13: \"Dat_Phuong_Nam\", 14: \"Dia_Dao\", 15: \"Dinh_Menh_Thien_Y\", 16: \"Em_Chua_18\",\n",
    "    17: \"Em_La_Cua_Em\", 18: \"Gai_Gia_Lam_Chieu_3\", 19: \"Gia_Ngheo_Gap_Phat\", 20: \"Hem_Cut\",\n",
    "    21: \"Hoan_Doi\", 22: \"Ke_An_Danh\", 23: \"Ke_An_Hon\", 24: \"Lam_Giau_Voi_Ma\",\n",
    "    25: \"Lat_Mat_1\", 26: \"Lo_Mat\", 27: \"Ma_Da\", 28: \"Mat_Biec\",\n",
    "    29: \"Nhung_Nu_Hon_Ruc_Ro\", 30: \"Oan_Linh__Phan_1\", 31: \"Ong_Ngoai_Tuoi_30\",\n",
    "    32: \"Phap_Su_Tap_Su\", 33: \"Quy_Co_Thua_Ke\", 34: \"Ra_Mat_Gia_Tien\",\n",
    "    35: \"Sieu_Lua_Gap_Sieu_Lay\", 36: \"Sieu_Quay\", 37: \"Sieu_Tro_Ly\",\n",
    "    38: \"Taxi_Em_Ten_Gi\", 39: \"The_Call\", 40: \"Thien_Menh_Anh_Hung\",\n",
    "    41: \"Tieu_Thu_Va_Ba_Dau_Gau\", 42: \"Tren_Ban_Nhau_Duoi_Ban_Muu\"\n",
    "}\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # Lấy danh sách ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    # Lấy ngẫu nhiên n ảnh từ folder (hoặc ít hơn nếu không đủ)\n",
    "    selected_images = random.sample(image_files, min(n_per_class, len(image_files)))\n",
    "    all_images.extend(selected_images)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Trích đặc trưng với mô hình\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Đảm bảo đặc trưng có định dạng phù hợp\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # Tìm kiếm k=1 điểm gần nhất trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Lấy nhãn dự đoán từ FAISS\n",
    "        pred_label_data = index_labels[I[0][0]]\n",
    "        if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "            pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "        else:\n",
    "            pred_label = int(pred_label_data)\n",
    "\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "    \n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNetB4 + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:41:14.367216Z",
     "iopub.status.busy": "2025-05-10T13:41:14.366524Z",
     "iopub.status.idle": "2025-05-10T13:43:50.497088Z",
     "shell.execute_reply": "2025-05-10T13:43:50.496329Z",
     "shell.execute_reply.started": "2025-05-10T13:41:14.367193Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features with EfficientNetB4: 100%|██████████| 312/312 [02:33<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "# Số batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Khởi tạo EfficientNetB4 trong context của strategy\n",
    "with strategy.scope():\n",
    "    base_model_efficientnetb4 = EfficientNetB4(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Nơi lưu đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# Lặp qua từng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features with EfficientNetB4\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_efficientnetb4(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# Kết hợp đặc trưng và nhãn\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:43:53.558269Z",
     "iopub.status.busy": "2025-05-10T13:43:53.557981Z",
     "iopub.status.idle": "2025-05-10T13:43:54.158357Z",
     "shell.execute_reply": "2025-05-10T13:43:54.157768Z",
     "shell.execute_reply.started": "2025-05-10T13:43:53.558250Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã thêm 39833 vector vào FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Kích thước vector đặc trưng\n",
    "d = features_array.shape[1]  \n",
    "\n",
    "# Khởi tạo index\n",
    "index = faiss.IndexFlatL2(d)  # Dùng khoảng cách Euclidean\n",
    "\n",
    "# Thêm các vector vào index\n",
    "index.add(features_array.astype('float32'))\n",
    "print(\"Đã thêm\", index.ntotal, \"vector vào FAISS index.\")\n",
    "\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:44:01.654138Z",
     "iopub.status.busy": "2025-05-10T13:44:01.653880Z",
     "iopub.status.idle": "2025-05-10T13:44:01.962370Z",
     "shell.execute_reply": "2025-05-10T13:44:01.961578Z",
     "shell.execute_reply.started": "2025-05-10T13:44:01.654121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Đọc lại FAISS index và nhãn\n",
    "index = faiss.read_index(\"faiss_features.index\")\n",
    "labels_array = np.load(\"faiss_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-10T13:44:04.590184Z",
     "iopub.status.busy": "2025-05-10T13:44:04.589643Z",
     "iopub.status.idle": "2025-05-10T13:47:50.348978Z",
     "shell.execute_reply": "2025-05-10T13:47:50.348328Z",
     "shell.execute_reply.started": "2025-05-10T13:44:04.590158Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 128\n",
    "test_path = \"/kaggle/input/frames-film-vietnam-dataset/Frame_Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "n_per_class = 50\n",
    "\n",
    "# ==== Load mô hình và FAISS index ====\n",
    "model = EfficientNetB4(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn FAISS index và labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"❌ Không tìm thấy FAISS index tại: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"❌ Không tìm thấy nhãn tại: {label_path}\")\n",
    "\n",
    "# Load FAISS index và labels\n",
    "try:\n",
    "    # Load index trực tiếp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"✅ FAISS index đã được tải thành công!\")\n",
    "    print(f\"   - Số lượng vectors: {index.ntotal}\")\n",
    "    print(f\"   - Kích thước vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = {\n",
    "    1: \"21_Ngay_Yeu_Em\", 2: \"An_Tet_Ben_Con\", 3: \"Bay_Ngot_Ngao\", 4: \"Benh_Vien_Ma\",\n",
    "    5: \"Bi_Mat_Lai_Bi_Mat\", 6: \"Bi_Mat_Trong_Suong_Mu\", 7: \"Bo_Tu_Oan_Gia\", 8: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    9: \"Chu_Tich_Giao_Hang\", 10: \"Chuyen_Tet\", 11: \"Co_Ba_Sai_Gon\", 12: \"Dao_Pho_Va_Piano\",\n",
    "    13: \"Dat_Phuong_Nam\", 14: \"Dia_Dao\", 15: \"Dinh_Menh_Thien_Y\", 16: \"Em_Chua_18\",\n",
    "    17: \"Em_La_Cua_Em\", 18: \"Gai_Gia_Lam_Chieu_3\", 19: \"Gia_Ngheo_Gap_Phat\", 20: \"Hem_Cut\",\n",
    "    21: \"Hoan_Doi\", 22: \"Ke_An_Danh\", 23: \"Ke_An_Hon\", 24: \"Lam_Giau_Voi_Ma\",\n",
    "    25: \"Lat_Mat_1\", 26: \"Lo_Mat\", 27: \"Ma_Da\", 28: \"Mat_Biec\",\n",
    "    29: \"Nhung_Nu_Hon_Ruc_Ro\", 30: \"Oan_Linh__Phan_1\", 31: \"Ong_Ngoai_Tuoi_30\",\n",
    "    32: \"Phap_Su_Tap_Su\", 33: \"Quy_Co_Thua_Ke\", 34: \"Ra_Mat_Gia_Tien\",\n",
    "    35: \"Sieu_Lua_Gap_Sieu_Lay\", 36: \"Sieu_Quay\", 37: \"Sieu_Tro_Ly\",\n",
    "    38: \"Taxi_Em_Ten_Gi\", 39: \"The_Call\", 40: \"Thien_Menh_Anh_Hung\",\n",
    "    41: \"Tieu_Thu_Va_Ba_Dau_Gau\", 42: \"Tren_Ban_Nhau_Duoi_Ban_Muu\"\n",
    "}\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # Lấy danh sách ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    # Lấy ngẫu nhiên n ảnh từ folder (hoặc ít hơn nếu không đủ)\n",
    "    selected_images = random.sample(image_files, min(n_per_class, len(image_files)))\n",
    "    all_images.extend(selected_images)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Trích đặc trưng với mô hình\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Đảm bảo đặc trưng có định dạng phù hợp\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # Tìm kiếm k=1 điểm gần nhất trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Lấy nhãn dự đoán từ FAISS\n",
    "        pred_label_data = index_labels[I[0][0]]\n",
    "        if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "            pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "        else:\n",
    "            pred_label = int(pred_label_data)\n",
    "\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "    \n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InceptionV3 + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:49:34.579594Z",
     "iopub.status.busy": "2025-05-10T13:49:34.579303Z",
     "iopub.status.idle": "2025-05-10T13:51:07.470453Z",
     "shell.execute_reply": "2025-05-10T13:51:07.469583Z",
     "shell.execute_reply.started": "2025-05-10T13:49:34.579575Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features with InceptionV3: 100%|██████████| 312/312 [01:29<00:00,  3.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "# Giả sử biến train_df là tf.data.Dataset đã chuẩn hóa và batch đúng\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Khởi tạo InceptionV3 trong context của strategy\n",
    "with strategy.scope():\n",
    "    base_model_inceptionv3 = InceptionV3(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Trích đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features with InceptionV3\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_inceptionv3(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "features_array = np.concatenate(all_features, axis=0)   \n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:52:00.842383Z",
     "iopub.status.busy": "2025-05-10T13:52:00.841673Z",
     "iopub.status.idle": "2025-05-10T13:52:01.517223Z",
     "shell.execute_reply": "2025-05-10T13:52:01.516639Z",
     "shell.execute_reply.started": "2025-05-10T13:52:00.842355Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã thêm 39833 vector vào FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Kích thước vector đặc trưng\n",
    "d = features_array.shape[1]  \n",
    "\n",
    "# Khởi tạo index\n",
    "index = faiss.IndexFlatL2(d)  # Dùng khoảng cách Euclidean\n",
    "\n",
    "# Thêm các vector vào index\n",
    "index.add(features_array.astype('float32'))\n",
    "print(\"Đã thêm\", index.ntotal, \"vector vào FAISS index.\")\n",
    "\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Đọc lại FAISS index và nhãn\n",
    "index = faiss.read_index(\"faiss_features.index\")\n",
    "labels_array = np.load(\"faiss_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-10T13:52:24.254872Z",
     "iopub.status.busy": "2025-05-10T13:52:24.254330Z",
     "iopub.status.idle": "2025-05-10T13:56:14.868810Z",
     "shell.execute_reply": "2025-05-10T13:56:14.868202Z",
     "shell.execute_reply.started": "2025-05-10T13:52:24.254849Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 128\n",
    "test_path = \"/kaggle/input/frames-film-vietnam-dataset/Frame_Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "n_per_class = 50\n",
    "\n",
    "# ==== Load mô hình và FAISS index ====\n",
    "model = InceptionV3(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn FAISS index và labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"❌ Không tìm thấy FAISS index tại: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"❌ Không tìm thấy nhãn tại: {label_path}\")\n",
    "\n",
    "# Load FAISS index và labels\n",
    "try:\n",
    "    # Load index trực tiếp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"✅ FAISS index đã được tải thành công!\")\n",
    "    print(f\"   - Số lượng vectors: {index.ntotal}\")\n",
    "    print(f\"   - Kích thước vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = {\n",
    "    1: \"21_Ngay_Yeu_Em\", 2: \"An_Tet_Ben_Con\", 3: \"Bay_Ngot_Ngao\", 4: \"Benh_Vien_Ma\",\n",
    "    5: \"Bi_Mat_Lai_Bi_Mat\", 6: \"Bi_Mat_Trong_Suong_Mu\", 7: \"Bo_Tu_Oan_Gia\", 8: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    9: \"Chu_Tich_Giao_Hang\", 10: \"Chuyen_Tet\", 11: \"Co_Ba_Sai_Gon\", 12: \"Dao_Pho_Va_Piano\",\n",
    "    13: \"Dat_Phuong_Nam\", 14: \"Dia_Dao\", 15: \"Dinh_Menh_Thien_Y\", 16: \"Em_Chua_18\",\n",
    "    17: \"Em_La_Cua_Em\", 18: \"Gai_Gia_Lam_Chieu_3\", 19: \"Gia_Ngheo_Gap_Phat\", 20: \"Hem_Cut\",\n",
    "    21: \"Hoan_Doi\", 22: \"Ke_An_Danh\", 23: \"Ke_An_Hon\", 24: \"Lam_Giau_Voi_Ma\",\n",
    "    25: \"Lat_Mat_1\", 26: \"Lo_Mat\", 27: \"Ma_Da\", 28: \"Mat_Biec\",\n",
    "    29: \"Nhung_Nu_Hon_Ruc_Ro\", 30: \"Oan_Linh__Phan_1\", 31: \"Ong_Ngoai_Tuoi_30\",\n",
    "    32: \"Phap_Su_Tap_Su\", 33: \"Quy_Co_Thua_Ke\", 34: \"Ra_Mat_Gia_Tien\",\n",
    "    35: \"Sieu_Lua_Gap_Sieu_Lay\", 36: \"Sieu_Quay\", 37: \"Sieu_Tro_Ly\",\n",
    "    38: \"Taxi_Em_Ten_Gi\", 39: \"The_Call\", 40: \"Thien_Menh_Anh_Hung\",\n",
    "    41: \"Tieu_Thu_Va_Ba_Dau_Gau\", 42: \"Tren_Ban_Nhau_Duoi_Ban_Muu\"\n",
    "}\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # Lấy danh sách ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    # Lấy ngẫu nhiên n ảnh từ folder (hoặc ít hơn nếu không đủ)\n",
    "    selected_images = random.sample(image_files, min(n_per_class, len(image_files)))\n",
    "    all_images.extend(selected_images)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Trích đặc trưng với mô hình\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Đảm bảo đặc trưng có định dạng phù hợp\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # Tìm kiếm k=1 điểm gần nhất trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Lấy nhãn dự đoán từ FAISS\n",
    "        pred_label_data = index_labels[I[0][0]]\n",
    "        if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "            pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "        else:\n",
    "            pred_label = int(pred_label_data)\n",
    "\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "    \n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T14:10:48.210331Z",
     "iopub.status.busy": "2025-05-10T14:10:48.209711Z",
     "iopub.status.idle": "2025-05-10T14:11:23.829520Z",
     "shell.execute_reply": "2025-05-10T14:11:23.828791Z",
     "shell.execute_reply.started": "2025-05-10T14:10:48.210299Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 312/312 [00:35<00:00,  8.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Số batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    base_model_vgg16 = VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Nơi lưu đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_vgg16(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# Nối đặc trưng và nhãn\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T14:11:44.817248Z",
     "iopub.status.busy": "2025-05-10T14:11:44.816971Z",
     "iopub.status.idle": "2025-05-10T14:11:44.957141Z",
     "shell.execute_reply": "2025-05-10T14:11:44.956533Z",
     "shell.execute_reply.started": "2025-05-10T14:11:44.817227Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã thêm 39833 vector vào FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Kích thước vector đặc trưng\n",
    "d = features_array.shape[1]  \n",
    "\n",
    "# Khởi tạo index\n",
    "index = faiss.IndexFlatL2(d)  # Dùng khoảng cách Euclidean\n",
    "\n",
    "# Thêm các vector vào index\n",
    "index.add(features_array.astype('float32'))\n",
    "print(\"Đã thêm\", index.ntotal, \"vector vào FAISS index.\")\n",
    "\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Đọc lại FAISS index và nhãn\n",
    "index = faiss.read_index(\"faiss_features.index\")\n",
    "labels_array = np.load(\"faiss_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T14:11:56.564589Z",
     "iopub.status.busy": "2025-05-10T14:11:56.563841Z",
     "iopub.status.idle": "2025-05-10T14:14:38.440392Z",
     "shell.execute_reply": "2025-05-10T14:14:38.439606Z",
     "shell.execute_reply.started": "2025-05-10T14:11:56.564563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 128\n",
    "test_path = \"/kaggle/input/frames-film-vietnam-dataset/Frame_Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "n_per_class = 50\n",
    "\n",
    "# ==== Load mô hình và FAISS index ====\n",
    "model = VGG16(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn FAISS index và labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"❌ Không tìm thấy FAISS index tại: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"❌ Không tìm thấy nhãn tại: {label_path}\")\n",
    "\n",
    "# Load FAISS index và labels\n",
    "try:\n",
    "    # Load index trực tiếp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"✅ FAISS index đã được tải thành công!\")\n",
    "    print(f\"   - Số lượng vectors: {index.ntotal}\")\n",
    "    print(f\"   - Kích thước vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = {\n",
    "    1: \"21_Ngay_Yeu_Em\", 2: \"An_Tet_Ben_Con\", 3: \"Bay_Ngot_Ngao\", 4: \"Benh_Vien_Ma\",\n",
    "    5: \"Bi_Mat_Lai_Bi_Mat\", 6: \"Bi_Mat_Trong_Suong_Mu\", 7: \"Bo_Tu_Oan_Gia\", 8: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    9: \"Chu_Tich_Giao_Hang\", 10: \"Chuyen_Tet\", 11: \"Co_Ba_Sai_Gon\", 12: \"Dao_Pho_Va_Piano\",\n",
    "    13: \"Dat_Phuong_Nam\", 14: \"Dia_Dao\", 15: \"Dinh_Menh_Thien_Y\", 16: \"Em_Chua_18\",\n",
    "    17: \"Em_La_Cua_Em\", 18: \"Gai_Gia_Lam_Chieu_3\", 19: \"Gia_Ngheo_Gap_Phat\", 20: \"Hem_Cut\",\n",
    "    21: \"Hoan_Doi\", 22: \"Ke_An_Danh\", 23: \"Ke_An_Hon\", 24: \"Lam_Giau_Voi_Ma\",\n",
    "    25: \"Lat_Mat_1\", 26: \"Lo_Mat\", 27: \"Ma_Da\", 28: \"Mat_Biec\",\n",
    "    29: \"Nhung_Nu_Hon_Ruc_Ro\", 30: \"Oan_Linh__Phan_1\", 31: \"Ong_Ngoai_Tuoi_30\",\n",
    "    32: \"Phap_Su_Tap_Su\", 33: \"Quy_Co_Thua_Ke\", 34: \"Ra_Mat_Gia_Tien\",\n",
    "    35: \"Sieu_Lua_Gap_Sieu_Lay\", 36: \"Sieu_Quay\", 37: \"Sieu_Tro_Ly\",\n",
    "    38: \"Taxi_Em_Ten_Gi\", 39: \"The_Call\", 40: \"Thien_Menh_Anh_Hung\",\n",
    "    41: \"Tieu_Thu_Va_Ba_Dau_Gau\", 42: \"Tren_Ban_Nhau_Duoi_Ban_Muu\"\n",
    "}\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # Lấy danh sách ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    # Lấy ngẫu nhiên n ảnh từ folder (hoặc ít hơn nếu không đủ)\n",
    "    selected_images = random.sample(image_files, min(n_per_class, len(image_files)))\n",
    "    all_images.extend(selected_images)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Trích đặc trưng với mô hình\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Đảm bảo đặc trưng có định dạng phù hợp\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # Tìm kiếm k=1 điểm gần nhất trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Lấy nhãn dự đoán từ FAISS\n",
    "        pred_label_data = index_labels[I[0][0]]\n",
    "        if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "            pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "        else:\n",
    "            pred_label = int(pred_label_data)\n",
    "\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "    \n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_faiss.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7379070,
     "sourceId": 11754070,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
