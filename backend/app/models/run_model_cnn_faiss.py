import os
import time
import numpy as np
import faiss
import cv2
from collections import Counter, OrderedDict

from tensorflow.keras.applications import ResNet50, VGG16
from tensorflow.keras.applications.resnet50 import preprocess_input
# from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image

# ==== C·∫•u h√¨nh ====
image_size = 224
base_dir = os.path.dirname(os.path.abspath(__file__))
index_path = os.path.join(base_dir, "faiss_224/resnet50/faiss_features.index")
label_path = os.path.join(base_dir, "faiss_224/resnet50/faiss_labels.npy")
# similarity_threshold = 0.8

# ==== Load model ResNet50 ====
model = ResNet50(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))
# model = VGG16(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))

# ==== Load FAISS index v√† labels ====
if not os.path.exists(index_path) or not os.path.exists(label_path):
    print("‚ùå Kh√¥ng t√¨m th·∫•y FAISS index ho·∫∑c labels.")
    exit()

index = faiss.read_index(index_path)
index_labels = np.load(label_path)

# ==== Danh s√°ch phim (mapping) ====
classes = {
    1: "21 Ng√†y Y√™u Em", 2: "4 NƒÉm 2 Ch√†ng 1 T√¨nh Y√™u", 3: "ƒÇn T·∫øt B√™n C·ªìn", 4: "B·∫´y Ng·ªçt Ng√†o", 5: "B·ªánh Vi·ªán Ma",
    6: "B√≠ M·∫≠t L·∫°i B·ªã M·∫•t", 7: "B√≠ M·∫≠t Trong S∆∞∆°ng M√π", 8: "B·ªô T·ª© Oan Gia", 9: "Ch·ªù Em ƒê·∫øn Ng√†y Mai", 10: "Ch·ªß T·ªãch Giao H√†ng",
    11: "Chuy·ªán T·∫øt", 12: "C√¥ Ba S√†i G√≤n", 13: "ƒê√†o, Ph·ªü V√† Piano", 14: "ƒê·∫•t R·ª´ng Ph∆∞∆°ng Nam", 15: "ƒê·ªãa ƒê·∫°o",
    16: "ƒê·ªãnh M·ªánh Thi√™n √ù", 17: "ƒê√¥i M·∫Øt √Çm D∆∞∆°ng", 18: "Em Ch∆∞a 18", 19: "Em L√† C·ªßa Em", 20: "G√°i Gi√† L·∫Øm Chi√™u 3",
    21: "Gi·∫£ Ngh√®o G·∫∑p Ph·∫≠t", 22: "H·∫ªm C·ª•t", 23: "Ho√°n ƒê·ªïi", 24: "K·∫ª ·∫®n Danh", 25: "K·∫ª ƒÇn H·ªìn",
    26: "L√†m Gi√†u V·ªõi Ma", 27: "L·∫≠t M·∫∑t 1", 28: "L·ªô M·∫∑t", 29: "Ma Da", 30: "M·∫Øt Bi·∫øc", 31: "Ngh·ªÅ Si√™u D·ªÖ", 32: "Nh·ªØng N·ª• H√¥n R·ª±c R·ª°",
    33: "√îng Ngo·∫°i Tu·ªïi 30", 34: "Ph√°p S∆∞ T·∫≠p S·ª±", 35: "Qu·ª∑ C·∫©u", 36: "Qu√Ω C√¥ Th·ª´a K·∫ø", 37: "Ra M·∫Øt Gia Ti√™n",
    38: "Si√™u L·ª´a G·∫∑p Si√™u L·∫ßy", 39: "Si√™u Tr·ª£ L√Ω", 40: "T·∫•m C√°m Chuy·ªán Ch∆∞a K·ªÉ",
    41: "Taxi Em T√™n G√¨", 42: "The Call", 43: "Thi√™n M·ªánh Anh H√πng", 44: "Ti·ªÉu Th∆∞ V√† Ba ƒê·∫ßu G·∫•u", 45: "Tr√™n B√†n Nh·∫≠u D∆∞·ªõi B√†n M∆∞u",
    46: "Kh√°c"
}

# Chu·∫©n h√≥a L2 cho m·ªói vector (ƒë·ªô d√†i = 1)
def l2_normalize(vectors):
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    return vectors / (norms + 1e-10)  

# ==== H√†m d·ª± ƒëo√°n ====
# H√†m x·ª≠ l√Ω ·∫£nh
def predict_film_from_image(img_path, similarity_threshold, n_movies):
    img = cv2.imread(img_path)
    img = cv2.resize(img, (image_size, image_size))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)

    feature = model.predict(x, verbose=0)
    feature = l2_normalize(feature)
    feature = feature.astype(np.float32)

    k = n_movies
    D, I = index.search(feature, k)

    similarity_scores = 1 - D[0] / 2

    result_labels = []
    seen_labels = set()

    for idx, sim in zip(I[0], similarity_scores):
        if sim < similarity_threshold:
            pred_label = 46
        else:
            pred_label_data = index_labels[idx]
            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:
                pred_label = int(np.argmax(pred_label_data)) + 1
            else:
                pred_label = int(pred_label_data)

        film_name = classes.get(pred_label, "Kh√¥ng x√°c ƒë·ªãnh")

        if film_name not in seen_labels:
            result_labels.append(film_name)
            seen_labels.add(film_name)

    return result_labels

# H√†m x·ª≠ l√Ω video
def predict_film_from_video(video_path, similarity_threshold, n_movies):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return ["‚ùå Kh√¥ng m·ªü ƒë∆∞·ª£c video."]

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if total_frames == 0:
        return ["‚ùå Video kh√¥ng c√≥ frame n√†o."]

    frame_indices = [
        0,
        total_frames // 4,
        total_frames // 2,
        (3 * total_frames) // 4,
        total_frames - 1
    ]
    
    film_occurrences = []  # L∆∞u th·ª© t·ª± xu·∫•t hi·ªán
    film_counts = Counter()

    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if not ret:
            continue

        frame = cv2.resize(frame, (image_size, image_size))
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        x = np.expand_dims(frame, axis=0)
        x = preprocess_input(x)

        feature = model.predict(x, verbose=0)
        feature = l2_normalize(feature)
        feature = feature.astype(np.float32)

        k = n_movies * 2
        D, I = index.search(feature, k)
        similarity_scores = 1 - D[0] / 2

        seen_in_frame = set()
        for idx_db, sim in zip(I[0], similarity_scores):
            if sim < similarity_threshold:
                pred_label = 46
            else:
                pred_label_data = index_labels[idx_db]
                if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:
                    pred_label = int(np.argmax(pred_label_data)) + 1
                else:
                    pred_label = int(pred_label_data)

            film_name = classes.get(pred_label, "Kh√¥ng x√°c ƒë·ªãnh")

            if film_name not in seen_in_frame:
                seen_in_frame.add(film_name)
                film_counts[film_name] += 1
                film_occurrences.append(film_name)

    cap.release()

    if not film_counts:
        return ["‚ùå Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c frame h·ª£p l·ªá n√†o."]

    # S·∫Øp x·∫øp theo: s·ªë l·∫ßn xu·∫•t hi·ªán gi·∫£m d·∫ßn, sau ƒë√≥ theo th·ª© t·ª± xu·∫•t hi·ªán ƒë·∫ßu ti√™n
    unique_ordered_films = list(OrderedDict.fromkeys(film_occurrences))
    sorted_films = sorted(
        film_counts.items(),
        key=lambda item: (-item[1], unique_ordered_films.index(item[0]))
    )

    result = [film for film, _ in sorted_films[:n_movies]]
    return result


# H√†m t·ª± ƒë·ªông nh·∫≠n bi·∫øt lo·∫°i file v√† x·ª≠ l√Ω
def predict_film_auto(input_path, similarity_threshold, n_movies):
    try:
        start_time = time.time()
        ext = os.path.splitext(input_path)[-1].lower()

        if ext in ['.jpg', '.jpeg', '.png']:
            film_name = predict_film_from_image(input_path, similarity_threshold, n_movies)
        elif ext in ['.mp4', '.avi', '.mov', '.mkv']:
            film_name = predict_film_from_video(input_path, similarity_threshold, n_movies)
        else:
            return "‚ùå ƒê·ªãnh d·∫°ng kh√¥ng h·ªó tr·ª£."

        end_time = time.time()
        print("=====ƒê·∫∑c tr∆∞ng CNN=====")
        print(f"‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω: {end_time - start_time:.4f} gi√¢y")
        return film_name

    except Exception as e:
        return f"‚ùå L·ªói khi x·ª≠ l√Ω: {e}"

# ==== Test ====
# if __name__ == "__main__":
#     input_path = os.path.join(base_dir, "img_test/mada.mp4")
#     predicted_film = predict_film_auto(input_path)
#     print(f"üé¨ D·ª± ƒëo√°n: {predicted_film}")
