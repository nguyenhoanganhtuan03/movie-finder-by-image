{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-10T11:14:24.337942Z",
     "iopub.status.busy": "2025-05-10T11:14:24.337718Z",
     "iopub.status.idle": "2025-05-10T11:16:58.916126Z",
     "shell.execute_reply": "2025-05-10T11:16:58.915462Z",
     "shell.execute_reply.started": "2025-05-10T11:14:24.337919Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T07:28:29.940265Z",
     "iopub.status.busy": "2025-06-14T07:28:29.940077Z",
     "iopub.status.idle": "2025-06-14T07:28:46.256695Z",
     "shell.execute_reply": "2025-06-14T07:28:46.255436Z",
     "shell.execute_reply.started": "2025-06-14T07:28:29.940247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q faiss-cpu\n",
    "!pip install -q tensorflow==2.18.0\n",
    "!pip install -q skl2onnx onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T11:27:09.732347Z",
     "iopub.status.busy": "2025-06-11T11:27:09.731728Z",
     "iopub.status.idle": "2025-06-11T11:27:25.000972Z",
     "shell.execute_reply": "2025-06-11T11:27:25.000096Z",
     "shell.execute_reply.started": "2025-06-11T11:27:09.732318Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Ki·ªÉm tra phi√™n b·∫£n TensorFlow\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# C·∫•u h√¨nh memory growth ƒë·ªÉ s·ª≠ d·ª•ng GPU hi·ªáu qu·∫£\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"T√¨m th·∫•y {len(gpus)} GPU:\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "    \n",
    "    # C·∫•u h√¨nh memory growth\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # M·ªôt s·ªë t√πy ch·ªçn ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t cho GPU T4\n",
    "        os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "        os.environ['TF_GPU_THREAD_COUNT'] = '2'  # T∆∞∆°ng ·ª©ng v·ªõi s·ªë GPU\n",
    "        \n",
    "        # Hi·ªÉn th·ªã c√°c GPU logic\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"S·ªë l∆∞·ª£ng GPU v·∫≠t l√Ω: {len(gpus)}, s·ªë l∆∞·ª£ng GPU logic: {len(logical_gpus)}\")\n",
    "        \n",
    "        # Th√¥ng tin chi ti·∫øt v·ªÅ GPU\n",
    "        from tensorflow.python.client import device_lib\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        gpu_list = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "        print(f\"Danh s√°ch GPU: {gpu_list}\")\n",
    "        \n",
    "        # Hi·ªÉn th·ªã th√¥ng tin CUDA v√† cuDNN\n",
    "        build_info = tf.sysconfig.get_build_info()\n",
    "        print(f\"CUDA version: {build_info.get('cuda_version', 'N/A')}\")\n",
    "        print(f\"cuDNN version: {build_info.get('cudnn_version', 'N/A')}\")\n",
    "        \n",
    "        # Ki·ªÉm tra xem GPU c√≥ th·ª±c s·ª± ƒë∆∞·ª£c s·ª≠ d·ª•ng hay kh√¥ng\n",
    "        print(\"\\nX√°c nh·∫≠n GPU ƒëang ho·∫°t ƒë·ªông b·∫±ng ph√©p t√≠nh nh·ªè:\")\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "            b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(f\"T√≠nh to√°n tr√™n GPU: {c}\")\n",
    "            print(f\"ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: {c.device}\")\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        print(f\"L·ªói khi c·∫•u h√¨nh GPU: {e}\")\n",
    "else:\n",
    "    print(\"Kh√¥ng t√¨m th·∫•y GPU! ƒêang s·ª≠ d·ª•ng CPU.\")\n",
    "    \n",
    "    # Ki·ªÉm tra th√¥ng tin CPU\n",
    "    cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "    print(f\"T√¨m th·∫•y {len(cpu_devices)} CPU: {cpu_devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T07:30:38.542085Z",
     "iopub.status.busy": "2025-06-14T07:30:38.541176Z",
     "iopub.status.idle": "2025-06-14T07:30:57.331754Z",
     "shell.execute_reply": "2025-06-14T07:30:57.330957Z",
     "shell.execute_reply.started": "2025-06-14T07:30:38.542050Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import gc\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm \n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, Flatten, Dropout, AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, Resizing\n",
    "from tensorflow.keras.layers import MaxPooling2D, Activation, BatchNormalization, Attention, Reshape, RepeatVector, Lambda, Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import joblib\n",
    "\n",
    "import onnxruntime as ort\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "# B·ªè qua c√°c c·∫£nh b√°o\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# In phi√™n b·∫£n TensorFlow hi·ªán t·∫°i\n",
    "print('TensorFlow Version ' + tf.__version__)\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    # Thi·∫øt l·∫≠p seed ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh t√°i l·∫≠p (reproducibility)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# G·ªçi h√†m seed_everything ƒë·ªÉ thi·∫øt l·∫≠p seed m·∫∑c ƒë·ªãnh\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tr·ª±c quan d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:18:22.540345Z",
     "iopub.status.busy": "2025-05-25T12:18:22.539283Z",
     "iopub.status.idle": "2025-05-25T12:20:49.559835Z",
     "shell.execute_reply": "2025-05-25T12:20:49.559177Z",
     "shell.execute_reply.started": "2025-05-25T12:18:22.540318Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Th∆∞ m·ª•c Train====\n",
      "==> T·ªïng s·ªë ·∫£nh: 89852\n",
      "====Th∆∞ m·ª•c Test====\n",
      "==> T·ªïng s·ªë ·∫£nh: 27612\n"
     ]
    }
   ],
   "source": [
    "def count_images_per_folder(root_dir, image_extensions=None):\n",
    "    if image_extensions is None:\n",
    "        image_extensions = ['.jpg']\n",
    "\n",
    "    total = 0\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        count = sum(1 for file in files if any(file.lower().endswith(ext) for ext in image_extensions))\n",
    "        if count > 0:\n",
    "            relative_path = os.path.relpath(subdir, root_dir)\n",
    "            # print(f\"Th∆∞ m·ª•c '{relative_path}': {count} ·∫£nh\")\n",
    "            total += count\n",
    "\n",
    "    print(f\"==> T·ªïng s·ªë ·∫£nh: {total}\")\n",
    "\n",
    "print(\"====Th∆∞ m·ª•c Train====\")\n",
    "folder_train_path = '/kaggle/input/processed-frames-224/Process_Frames_1/Train'\n",
    "count_images_per_folder(folder_train_path)\n",
    "\n",
    "print(\"====Th∆∞ m·ª•c Test====\")\n",
    "folder_test_path = '/kaggle/input/processed-frames-224/Process_Frames_1/Test'\n",
    "count_images_per_folder(folder_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:54.511900Z",
     "iopub.status.busy": "2025-05-25T12:20:54.511211Z",
     "iopub.status.idle": "2025-05-25T12:21:25.593994Z",
     "shell.execute_reply": "2025-05-25T12:21:25.593247Z",
     "shell.execute_reply.started": "2025-05-25T12:20:54.511876Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_images_per_folder(root_dir, image_extensions=None):\n",
    "    if image_extensions is None:\n",
    "        image_extensions = ['.jpg']\n",
    "\n",
    "    folder_counts = {}\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        if subdir == root_dir:\n",
    "            continue  # b·ªè qua th∆∞ m·ª•c g·ªëc\n",
    "        count = sum(1 for file in files if any(file.lower().endswith(ext) for ext in image_extensions))\n",
    "        if count > 0:\n",
    "            folder_name = os.path.basename(subdir)\n",
    "            folder_counts[folder_name] = count\n",
    "\n",
    "    return folder_counts\n",
    "\n",
    "def plot_image_counts(folder_counts):\n",
    "    folders = list(folder_counts.keys())\n",
    "    counts = list(folder_counts.values())\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.bar(folders, counts, color='skyblue')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('T√™n th∆∞ m·ª•c con')\n",
    "    plt.ylabel('S·ªë l∆∞·ª£ng ·∫£nh')\n",
    "    plt.title('S·ªë l∆∞·ª£ng ·∫£nh trong t·ª´ng th∆∞ m·ª•c con')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# T·∫≠p Train\n",
    "counts = count_images_per_folder(folder_train_path)\n",
    "plot_image_counts(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:21:29.919157Z",
     "iopub.status.busy": "2025-05-25T12:21:29.918847Z",
     "iopub.status.idle": "2025-05-25T12:21:43.343924Z",
     "shell.execute_reply": "2025-05-25T12:21:43.343218Z",
     "shell.execute_reply.started": "2025-05-25T12:21:29.919131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# T·∫≠p Test\n",
    "counts = count_images_per_folder(folder_test_path)\n",
    "plot_image_counts(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T·∫°o t·∫≠p d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T07:31:27.484516Z",
     "iopub.status.busy": "2025-06-14T07:31:27.484161Z",
     "iopub.status.idle": "2025-06-14T07:31:27.491497Z",
     "shell.execute_reply": "2025-06-14T07:31:27.490578Z",
     "shell.execute_reply.started": "2025-06-14T07:31:27.484492Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "n_classes = 45\n",
    "batch_size = 32\n",
    "\n",
    "classes_train = {\n",
    "    1: \"21_Ngay_Yeu_Em\",\n",
    "    2: \"4_Nam_2_Chang_1_Tinh_Yeu\",\n",
    "    3: \"An_Tet_Ben_Con\",\n",
    "    4: \"Bay_Ngot_Ngao\",\n",
    "    5: \"Benh_Vien_Ma\",\n",
    "    6: \"Bi_Mat_Lai_Bi_Mat\",\n",
    "    7: \"Bi_Mat_Trong_Suong_Mu\",\n",
    "    8: \"Bo_Tu_Oan_Gia\",\n",
    "    9: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    10: \"Chu_Tich_Giao_Hang\",\n",
    "    11: \"Chuyen_Tet\",\n",
    "    12: \"Co_Ba_Sai_Gon\",\n",
    "    13: \"Dao_Pho_Va_Piano\",\n",
    "    14: \"Dat_Rung_Phuong_Nam\",\n",
    "    15: \"Dia_Dao\",\n",
    "    16: \"Dinh_Menh_Thien_Y\",\n",
    "    17: \"Doi_Mat_Am_Duong\",\n",
    "    18: \"Em_Chua_18\",\n",
    "    19: \"Em_La_Cua_Em\",\n",
    "    20: \"Gai_Gia_Lam_Chieu_3\",\n",
    "    21: \"Gia_Ngheo_Gap_Phat\",\n",
    "    22: \"Hem_Cut\",\n",
    "    23: \"Hoan_Doi\",\n",
    "    24: \"Ke_An_Danh\",\n",
    "    25: \"Ke_An_Hon\",\n",
    "    26: \"Lam_Giau_Voi_Ma\",\n",
    "    27: \"Lat_Mat_1\",\n",
    "    28: \"Lo_Mat\",\n",
    "    29: \"Ma_Da\",\n",
    "    30: \"Mat_Biec\",\n",
    "    31: \"Nghe_Sieu_De\",\n",
    "    32: \"Nhung_Nu_Hon_Ruc_Ro\",\n",
    "    33: \"Ong_Ngoai_Tuoi_30\",\n",
    "    34: \"Phap_Su_Tap_Su\",\n",
    "    35: \"Quy_Cau\",\n",
    "    36: \"Quy_Co_Thua_Ke\",\n",
    "    37: \"Ra_Mat_Gia_Tien\",\n",
    "    38: \"Sieu_Lua_Gap_Sieu_Lay\",\n",
    "    39: \"Sieu_Tro_Ly\",\n",
    "    40: \"Tam_Cam_Chuyen_Chua_Ke\",\n",
    "    41: \"Taxi_Em_Ten_Gi\",\n",
    "    42: \"The_Call\",\n",
    "    43: \"Thien_Menh_Anh_Hung\",\n",
    "    44: \"Tieu_Thu_Va_Ba_Dau_Gau\",\n",
    "    45: \"Tren_Ban_Nhau_Duoi_Ban_Muu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T07:31:29.450687Z",
     "iopub.status.busy": "2025-06-14T07:31:29.449975Z",
     "iopub.status.idle": "2025-06-14T07:31:29.457766Z",
     "shell.execute_reply": "2025-06-14T07:31:29.456730Z",
     "shell.execute_reply.started": "2025-06-14T07:31:29.450653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classes_test = {\n",
    "    1: \"21_Ngay_Yeu_Em\",\n",
    "    2: \"4_Nam_2_Chang_1_Tinh_Yeu\",\n",
    "    3: \"An_Tet_Ben_Con\",\n",
    "    4: \"Bay_Ngot_Ngao\",\n",
    "    5: \"Benh_Vien_Ma\",\n",
    "    6: \"Bi_Mat_Lai_Bi_Mat\",\n",
    "    7: \"Bi_Mat_Trong_Suong_Mu\",\n",
    "    8: \"Bo_Tu_Oan_Gia\",\n",
    "    9: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    10: \"Chu_Tich_Giao_Hang\",\n",
    "    11: \"Chuyen_Tet\",\n",
    "    12: \"Co_Ba_Sai_Gon\",\n",
    "    13: \"Dao_Pho_Va_Piano\",\n",
    "    14: \"Dat_Rung_Phuong_Nam\",\n",
    "    15: \"Dia_Dao\",\n",
    "    16: \"Dinh_Menh_Thien_Y\",\n",
    "    17: \"Doi_Mat_Am_Duong\",\n",
    "    18: \"Em_Chua_18\",\n",
    "    19: \"Em_La_Cua_Em\",\n",
    "    20: \"Gai_Gia_Lam_Chieu_3\",\n",
    "    21: \"Gia_Ngheo_Gap_Phat\",\n",
    "    22: \"Hem_Cut\",\n",
    "    23: \"Hoan_Doi\",\n",
    "    24: \"Ke_An_Danh\",\n",
    "    25: \"Ke_An_Hon\",\n",
    "    26: \"Lam_Giau_Voi_Ma\",\n",
    "    27: \"Lat_Mat_1\",\n",
    "    28: \"Lo_Mat\",\n",
    "    29: \"Ma_Da\",\n",
    "    30: \"Mat_Biec\",\n",
    "    31: \"Nghe_Sieu_De\",\n",
    "    32: \"Nhung_Nu_Hon_Ruc_Ro\",\n",
    "    33: \"Ong_Ngoai_Tuoi_30\",\n",
    "    34: \"Phap_Su_Tap_Su\",\n",
    "    35: \"Quy_Cau\",\n",
    "    36: \"Quy_Co_Thua_Ke\",\n",
    "    37: \"Ra_Mat_Gia_Tien\",\n",
    "    38: \"Sieu_Lua_Gap_Sieu_Lay\",\n",
    "    39: \"Sieu_Tro_Ly\",\n",
    "    40: \"Tam_Cam_Chuyen_Chua_Ke\",\n",
    "    41: \"Taxi_Em_Ten_Gi\",\n",
    "    42: \"The_Call\",\n",
    "    43: \"Thien_Menh_Anh_Hung\",\n",
    "    44: \"Tieu_Thu_Va_Ba_Dau_Gau\",\n",
    "    45: \"Tren_Ban_Nhau_Duoi_Ban_Muu\",\n",
    "    46: \"Khac\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T04:19:04.405182Z",
     "iopub.status.busy": "2025-06-10T04:19:04.404494Z",
     "iopub.status.idle": "2025-06-10T04:19:35.691733Z",
     "shell.execute_reply": "2025-06-10T04:19:35.691159Z",
     "shell.execute_reply.started": "2025-06-10T04:19:04.405148Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89853 files belonging to 45 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = '/kaggle/input/processed-frames-224/Process_Frames_1/Train'\n",
    "\n",
    "# T·∫°o dataset t·ª´ th∆∞ m·ª•c\n",
    "train_df = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical', \n",
    "    seed=1,\n",
    "    image_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, \n",
    ")\n",
    "\n",
    "# Chu·∫©n h√≥a ·∫£nh v·ªÅ [0, 1] + prefetch\n",
    "# normalization_layer = L.Rescaling(1./255)\n",
    "# train_df = train_df.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# train_df = train_df.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T13:19:18.012479Z",
     "iopub.status.busy": "2025-06-09T13:19:18.011935Z",
     "iopub.status.idle": "2025-06-09T13:19:18.743834Z",
     "shell.execute_reply": "2025-06-09T13:19:18.742982Z",
     "shell.execute_reply.started": "2025-06-09T13:19:18.012453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_df))  \n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(5, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for img, ax in zip(images[:9], axes): \n",
    "    ax.imshow(img.numpy().astype(\"uint8\"))  \n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X√≥a file trong th∆∞ m·ª•c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T07:31:02.490422Z",
     "iopub.status.busy": "2025-06-14T07:31:02.489824Z",
     "iopub.status.idle": "2025-06-14T07:31:02.516734Z",
     "shell.execute_reply": "2025-06-14T07:31:02.515892Z",
     "shell.execute_reply.started": "2025-06-14T07:31:02.490397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def delete_all_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # x√≥a file ho·∫∑c symbolic link\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # x√≥a th∆∞ m·ª•c v√† to√†n b·ªô n·ªôi dung b√™n trong\n",
    "        except Exception as e:\n",
    "            print(f\"Kh√¥ng th·ªÉ x√≥a {file_path}: {e}\")\n",
    "\n",
    "# V√≠ d·ª•:\n",
    "folder = \"/kaggle/working/\"\n",
    "delete_all_in_folder(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T14:00:51.019594Z",
     "iopub.status.busy": "2025-06-10T14:00:51.019079Z",
     "iopub.status.idle": "2025-06-10T14:00:51.713004Z",
     "shell.execute_reply": "2025-06-10T14:00:51.712329Z",
     "shell.execute_reply.started": "2025-06-10T14:00:51.019571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50 + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T02:46:40.114693Z",
     "iopub.status.busy": "2025-06-10T02:46:40.114421Z",
     "iopub.status.idle": "2025-06-10T02:56:44.232662Z",
     "shell.execute_reply": "2025-06-10T02:56:44.232016Z",
     "shell.execute_reply.started": "2025-06-10T02:46:40.114665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# S·ªë batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Kh·ªüi t·∫°o model trong context c·ªßa strategy\n",
    "with strategy.scope():\n",
    "    base_model_resnet50 = ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# N∆°i l∆∞u ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# L·∫∑p qua t·ª´ng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_resnet50(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# ƒê√°nh nh√£n ·ª©ng v·ªõi ƒë·∫∑c tr∆∞ng\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T02:56:49.743021Z",
     "iopub.status.busy": "2025-06-10T02:56:49.742745Z",
     "iopub.status.idle": "2025-06-10T02:56:51.744663Z",
     "shell.execute_reply": "2025-06-10T02:56:51.743862Z",
     "shell.execute_reply.started": "2025-06-10T02:56:49.743002Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ th√™m 89853 vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chu·∫©n h√≥a L2 cho m·ªói vector (ƒë·ªô d√†i = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "\n",
    "# Chu·∫©n h√≥a vector ƒë·∫∑c tr∆∞ng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# K√≠ch th∆∞·ªõc vector ƒë·∫∑c tr∆∞ng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Kh·ªüi t·∫°o index FAISS s·ª≠ d·ª•ng kho·∫£ng c√°ch Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Th√™m vector ƒë√£ chu·∫©n h√≥a v√†o index\n",
    "index.add(normalized_features)\n",
    "print(\"ƒê√£ th√™m\", index.ntotal, \"vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\")\n",
    "\n",
    "# L∆∞u index v√† nh√£n\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:10:28.309080Z",
     "iopub.status.busy": "2025-05-10T13:10:28.308815Z",
     "iopub.status.idle": "2025-05-10T13:10:28.659179Z",
     "shell.execute_reply": "2025-05-10T13:10:28.658379Z",
     "shell.execute_reply.started": "2025-05-10T13:10:28.309060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ƒê·ªçc l·∫°i FAISS index v√† nh√£n\n",
    "index = faiss.read_index(\"faiss_features.index\")\n",
    "labels_array = np.load(\"faiss_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-10T04:06:04.660Z",
     "iopub.execute_input": "2025-06-10T02:56:59.576914Z",
     "iopub.status.busy": "2025-06-10T02:56:59.576630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c tham s·ªë\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/processed-frames-224/Process_Frames_1/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.8  \n",
    "\n",
    "# ==== Load m√¥ h√¨nh v√† FAISS index ====\n",
    "# Load m√¥ h√¨nh ResNet50 \n",
    "model = ResNet50(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n FAISS index v√† labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y FAISS index t·∫°i: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y nh√£n t·∫°i: {label_path}\")\n",
    "\n",
    "# Load FAISS index v√† labels\n",
    "try:\n",
    "    # Load index tr·ª±c ti·∫øp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n",
    "    print(f\"   - S·ªë l∆∞·ª£ng vectors: {index.ntotal}\")\n",
    "    print(f\"   - K√≠ch th∆∞·ªõc vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi t·∫£i FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duy·ªát t·∫≠p test v√† l·∫•y t·∫•t c·∫£ ·∫£nh t·ª´ m·ªói th∆∞ m·ª•c ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # L·∫•y danh s√°ch t·∫•t c·∫£ ·∫£nh trong th∆∞ m·ª•c n√†y\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== D·ª± ƒëo√°n ====\n",
    "processing_times = []  # Danh s√°ch l∆∞u th·ªùi gian x·ª≠ l√Ω t·ª´ng ·∫£nh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # B·∫Øt ƒë·∫ßu ƒëo th·ªùi gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"‚ùå Folder kh√¥ng h·ª£p l·ªá: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Tr√≠ch ƒë·∫∑c tr∆∞ng v·ªõi m√¥ h√¨nh\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chu·∫©n h√≥a L2 ƒë·∫∑c tr∆∞ng m·∫´u truy v·∫•n ƒë·ªÉ ph√π h·ª£p v·ªõi vector ƒë√£ chu·∫©n h√≥a trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "        \n",
    "        # ƒê·∫£m b·∫£o ƒë·∫∑c tr∆∞ng c√≥ ƒë·ªãnh d·∫°ng ph√π h·ª£p sau khi chu·∫©n h√≥a\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # T√¨m ki·∫øm k=1 ƒëi·ªÉm g·∫ßn nh·∫•t trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Ki·ªÉm tra similarity score t·ª´ kho·∫£ng c√°ch L2 v·ªõi vector ƒë√£ chu·∫©n h√≥a\n",
    "        # V·ªõi vector ƒë√£ chu·∫©n h√≥a L2, kho·∫£ng c√°ch Euclidean b√¨nh ph∆∞∆°ng (D) v√† cosine similarity c√≥ quan h·ªá:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Kho·∫£ng c√°ch FAISS l√† b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuy·ªÉn ƒë·ªïi kho·∫£ng c√°ch th√†nh cosine similarity\n",
    "        \n",
    "        # N·∫øu similarity d∆∞·ªõi ng∆∞·ª°ng, g√°n nh√£n \"Kh√°c\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nh√£n \"Kh√°c\"\n",
    "        else:\n",
    "            # L·∫•y nh√£n d·ª± ƒëo√°n t·ª´ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n v√† nh√£n th·ª±c t·∫ø\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # K·∫øt th√∫c ƒëo th·ªùi gian v√† l∆∞u l·∫°i\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In th√¥ng tin cho debug (c√≥ th·ªÉ b·ªè comment n·∫øu c·∫ßn ki·ªÉm tra)\n",
    "        # print(f\"·∫¢nh: {img_path}, Similarity: {similarity_score:.4f}, Nh√£n d·ª± ƒëo√°n: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói ·∫£nh {img_path}: {e}\")\n",
    "        print(f\"  H√¨nh d·∫°ng ƒë·∫∑c tr∆∞ng: {feature.shape}, Ki·ªÉu d·ªØ li·ªáu: {feature.dtype}\")\n",
    "\n",
    "# ==== ƒê√°nh gi√° ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√°!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n‚úÖ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"‚úÖ ƒê√∫ng: {np.sum(y_true == y_pred)} / ‚ùå Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # T√≠nh th·ªùi gian x·ª≠ l√Ω trung b√¨nh cho m·ªôt ·∫£nh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    print(f\"‚è±Ô∏è T·ªïng th·ªùi gian x·ª≠ l√Ω: {sum(processing_times):.2f} gi√¢y cho {len(processing_times)} ·∫£nh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\nüñºÔ∏è Confusion matrix ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\nüìÑ Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuy·ªÉn b√°o c√°o ph√¢n lo·∫°i th√†nh DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hi·ªÉn th·ªã c√°c tham s·ªë chung cho to√†n b·ªô ch∆∞∆°ng tr√¨nh\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\nüìù C√°c tham s·ªë ƒë√°nh gi√° chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    \n",
    "    # L∆∞u b√°o c√°o chi ti·∫øt v√†o t·ªáp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{csv_output_path}'\")\n",
    "    \n",
    "    # L∆∞u c√°c tham s·ªë ƒë√°nh gi√° chung v√†o t·ªáp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\nüìä C√°c tham s·ªë ƒë√°nh gi√° chung ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # L∆∞u c·∫£ b√°o c√°o ph√¢n lo·∫°i v√† c√°c tham s·ªë chung v√†o m·ªôt t·ªáp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNetB4 + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T04:19:50.628098Z",
     "iopub.status.busy": "2025-06-10T04:19:50.627521Z",
     "iopub.status.idle": "2025-06-10T04:44:16.515835Z",
     "shell.execute_reply": "2025-06-10T04:44:16.515204Z",
     "shell.execute_reply.started": "2025-06-10T04:19:50.628070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "# S·ªë batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Kh·ªüi t·∫°o EfficientNetB4 trong context c·ªßa strategy\n",
    "with strategy.scope():\n",
    "    base_model_efficientnetb4 = EfficientNetB4(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# N∆°i l∆∞u ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# L·∫∑p qua t·ª´ng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features with EfficientNetB4\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_efficientnetb4(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# K·∫øt h·ª£p ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T04:44:54.798377Z",
     "iopub.status.busy": "2025-06-10T04:44:54.797794Z",
     "iopub.status.idle": "2025-06-10T04:44:56.485234Z",
     "shell.execute_reply": "2025-06-10T04:44:56.484648Z",
     "shell.execute_reply.started": "2025-06-10T04:44:54.798353Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ th√™m 89853 vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chu·∫©n h√≥a L2 cho m·ªói vector (ƒë·ªô d√†i = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "\n",
    "# Chu·∫©n h√≥a vector ƒë·∫∑c tr∆∞ng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# K√≠ch th∆∞·ªõc vector ƒë·∫∑c tr∆∞ng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Kh·ªüi t·∫°o index FAISS s·ª≠ d·ª•ng kho·∫£ng c√°ch Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Th√™m vector ƒë√£ chu·∫©n h√≥a v√†o index\n",
    "index.add(normalized_features)\n",
    "print(\"ƒê√£ th√™m\", index.ntotal, \"vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\")\n",
    "\n",
    "# L∆∞u index v√† nh√£n\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:44:01.654138Z",
     "iopub.status.busy": "2025-05-10T13:44:01.653880Z",
     "iopub.status.idle": "2025-05-10T13:44:01.962370Z",
     "shell.execute_reply": "2025-05-10T13:44:01.961578Z",
     "shell.execute_reply.started": "2025-05-10T13:44:01.654121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ƒê·ªçc l·∫°i FAISS index v√† nh√£n\n",
    "index = faiss.read_index(\"faiss_features.index\")\n",
    "labels_array = np.load(\"faiss_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T04:45:16.658088Z",
     "iopub.status.busy": "2025-06-10T04:45:16.657296Z",
     "iopub.status.idle": "2025-06-10T05:51:30.282327Z",
     "shell.execute_reply": "2025-06-10T05:51:30.281744Z",
     "shell.execute_reply.started": "2025-06-10T04:45:16.658065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c tham s·ªë\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/processed-frames-224/Process_Frames_1/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.8  \n",
    "\n",
    "# ==== Load m√¥ h√¨nh v√† FAISS index ====\n",
    "# Load m√¥ h√¨nh EffcientNetB4\n",
    "model = EfficientNetB4(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n FAISS index v√† labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y FAISS index t·∫°i: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y nh√£n t·∫°i: {label_path}\")\n",
    "\n",
    "# Load FAISS index v√† labels\n",
    "try:\n",
    "    # Load index tr·ª±c ti·∫øp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n",
    "    print(f\"   - S·ªë l∆∞·ª£ng vectors: {index.ntotal}\")\n",
    "    print(f\"   - K√≠ch th∆∞·ªõc vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi t·∫£i FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duy·ªát t·∫≠p test v√† l·∫•y t·∫•t c·∫£ ·∫£nh t·ª´ m·ªói th∆∞ m·ª•c ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # L·∫•y danh s√°ch t·∫•t c·∫£ ·∫£nh trong th∆∞ m·ª•c n√†y\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== D·ª± ƒëo√°n ====\n",
    "processing_times = []  # Danh s√°ch l∆∞u th·ªùi gian x·ª≠ l√Ω t·ª´ng ·∫£nh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # B·∫Øt ƒë·∫ßu ƒëo th·ªùi gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"‚ùå Folder kh√¥ng h·ª£p l·ªá: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Tr√≠ch ƒë·∫∑c tr∆∞ng v·ªõi m√¥ h√¨nh\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chu·∫©n h√≥a L2 ƒë·∫∑c tr∆∞ng m·∫´u truy v·∫•n ƒë·ªÉ ph√π h·ª£p v·ªõi vector ƒë√£ chu·∫©n h√≥a trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "        \n",
    "        # ƒê·∫£m b·∫£o ƒë·∫∑c tr∆∞ng c√≥ ƒë·ªãnh d·∫°ng ph√π h·ª£p sau khi chu·∫©n h√≥a\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # T√¨m ki·∫øm k=1 ƒëi·ªÉm g·∫ßn nh·∫•t trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Ki·ªÉm tra similarity score t·ª´ kho·∫£ng c√°ch L2 v·ªõi vector ƒë√£ chu·∫©n h√≥a\n",
    "        # V·ªõi vector ƒë√£ chu·∫©n h√≥a L2, kho·∫£ng c√°ch Euclidean b√¨nh ph∆∞∆°ng (D) v√† cosine similarity c√≥ quan h·ªá:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Kho·∫£ng c√°ch FAISS l√† b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuy·ªÉn ƒë·ªïi kho·∫£ng c√°ch th√†nh cosine similarity\n",
    "        \n",
    "        # N·∫øu similarity d∆∞·ªõi ng∆∞·ª°ng, g√°n nh√£n \"Kh√°c\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nh√£n \"Kh√°c\"\n",
    "        else:\n",
    "            # L·∫•y nh√£n d·ª± ƒëo√°n t·ª´ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n v√† nh√£n th·ª±c t·∫ø\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # K·∫øt th√∫c ƒëo th·ªùi gian v√† l∆∞u l·∫°i\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In th√¥ng tin cho debug (c√≥ th·ªÉ b·ªè comment n·∫øu c·∫ßn ki·ªÉm tra)\n",
    "        # print(f\"·∫¢nh: {img_path}, Similarity: {similarity_score:.4f}, Nh√£n d·ª± ƒëo√°n: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói ·∫£nh {img_path}: {e}\")\n",
    "        print(f\"  H√¨nh d·∫°ng ƒë·∫∑c tr∆∞ng: {feature.shape}, Ki·ªÉu d·ªØ li·ªáu: {feature.dtype}\")\n",
    "\n",
    "# ==== ƒê√°nh gi√° ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√°!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n‚úÖ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"‚úÖ ƒê√∫ng: {np.sum(y_true == y_pred)} / ‚ùå Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # T√≠nh th·ªùi gian x·ª≠ l√Ω trung b√¨nh cho m·ªôt ·∫£nh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    print(f\"‚è±Ô∏è T·ªïng th·ªùi gian x·ª≠ l√Ω: {sum(processing_times):.2f} gi√¢y cho {len(processing_times)} ·∫£nh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\nüñºÔ∏è Confusion matrix ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\nüìÑ Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuy·ªÉn b√°o c√°o ph√¢n lo·∫°i th√†nh DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hi·ªÉn th·ªã c√°c tham s·ªë chung cho to√†n b·ªô ch∆∞∆°ng tr√¨nh\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\nüìù C√°c tham s·ªë ƒë√°nh gi√° chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    \n",
    "    # L∆∞u b√°o c√°o chi ti·∫øt v√†o t·ªáp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{csv_output_path}'\")\n",
    "    \n",
    "    # L∆∞u c√°c tham s·ªë ƒë√°nh gi√° chung v√†o t·ªáp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\nüìä C√°c tham s·ªë ƒë√°nh gi√° chung ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # L∆∞u c·∫£ b√°o c√°o ph√¢n lo·∫°i v√† c√°c tham s·ªë chung v√†o m·ªôt t·ªáp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InceptionV3 + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T05:53:31.071433Z",
     "iopub.status.busy": "2025-06-10T05:53:31.071190Z",
     "iopub.status.idle": "2025-06-10T06:07:19.730012Z",
     "shell.execute_reply": "2025-06-10T06:07:19.729384Z",
     "shell.execute_reply.started": "2025-06-10T05:53:31.071416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "# Gi·∫£ s·ª≠ bi·∫øn train_df l√† tf.data.Dataset ƒë√£ chu·∫©n h√≥a v√† batch ƒë√∫ng\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Kh·ªüi t·∫°o InceptionV3 trong context c·ªßa strategy\n",
    "with strategy.scope():\n",
    "    base_model_inceptionv3 = InceptionV3(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Tr√≠ch ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features with InceptionV3\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_inceptionv3(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "features_array = np.concatenate(all_features, axis=0)   \n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T06:07:48.294634Z",
     "iopub.status.busy": "2025-06-10T06:07:48.294353Z",
     "iopub.status.idle": "2025-06-10T06:07:50.280586Z",
     "shell.execute_reply": "2025-06-10T06:07:50.279907Z",
     "shell.execute_reply.started": "2025-06-10T06:07:48.294613Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ th√™m 89853 vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chu·∫©n h√≥a L2 cho m·ªói vector (ƒë·ªô d√†i = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "\n",
    "# Chu·∫©n h√≥a vector ƒë·∫∑c tr∆∞ng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# K√≠ch th∆∞·ªõc vector ƒë·∫∑c tr∆∞ng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Kh·ªüi t·∫°o index FAISS s·ª≠ d·ª•ng kho·∫£ng c√°ch Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Th√™m vector ƒë√£ chu·∫©n h√≥a v√†o index\n",
    "index.add(normalized_features)\n",
    "print(\"ƒê√£ th√™m\", index.ntotal, \"vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\")\n",
    "\n",
    "# L∆∞u index v√† nh√£n\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ƒê·ªçc l·∫°i FAISS index v√† nh√£n\n",
    "index = faiss.read_index(\"faiss_features.index\")\n",
    "labels_array = np.load(\"faiss_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T06:07:52.619080Z",
     "iopub.status.busy": "2025-06-10T06:07:52.618468Z",
     "iopub.status.idle": "2025-06-10T07:16:03.361295Z",
     "shell.execute_reply": "2025-06-10T07:16:03.360531Z",
     "shell.execute_reply.started": "2025-06-10T06:07:52.619059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c tham s·ªë\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/processed-frames-224/Process_Frames_1/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.8  \n",
    "\n",
    "# ==== Load m√¥ h√¨nh v√† FAISS index ====\n",
    "# Load m√¥ h√¨nh InceptionV3\n",
    "model = InceptionV3(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n FAISS index v√† labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y FAISS index t·∫°i: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y nh√£n t·∫°i: {label_path}\")\n",
    "\n",
    "# Load FAISS index v√† labels\n",
    "try:\n",
    "    # Load index tr·ª±c ti·∫øp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n",
    "    print(f\"   - S·ªë l∆∞·ª£ng vectors: {index.ntotal}\")\n",
    "    print(f\"   - K√≠ch th∆∞·ªõc vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi t·∫£i FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duy·ªát t·∫≠p test v√† l·∫•y t·∫•t c·∫£ ·∫£nh t·ª´ m·ªói th∆∞ m·ª•c ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # L·∫•y danh s√°ch t·∫•t c·∫£ ·∫£nh trong th∆∞ m·ª•c n√†y\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== D·ª± ƒëo√°n ====\n",
    "processing_times = []  # Danh s√°ch l∆∞u th·ªùi gian x·ª≠ l√Ω t·ª´ng ·∫£nh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # B·∫Øt ƒë·∫ßu ƒëo th·ªùi gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"‚ùå Folder kh√¥ng h·ª£p l·ªá: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Tr√≠ch ƒë·∫∑c tr∆∞ng v·ªõi m√¥ h√¨nh\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chu·∫©n h√≥a L2 ƒë·∫∑c tr∆∞ng m·∫´u truy v·∫•n ƒë·ªÉ ph√π h·ª£p v·ªõi vector ƒë√£ chu·∫©n h√≥a trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "        \n",
    "        # ƒê·∫£m b·∫£o ƒë·∫∑c tr∆∞ng c√≥ ƒë·ªãnh d·∫°ng ph√π h·ª£p sau khi chu·∫©n h√≥a\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # T√¨m ki·∫øm k=1 ƒëi·ªÉm g·∫ßn nh·∫•t trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Ki·ªÉm tra similarity score t·ª´ kho·∫£ng c√°ch L2 v·ªõi vector ƒë√£ chu·∫©n h√≥a\n",
    "        # V·ªõi vector ƒë√£ chu·∫©n h√≥a L2, kho·∫£ng c√°ch Euclidean b√¨nh ph∆∞∆°ng (D) v√† cosine similarity c√≥ quan h·ªá:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Kho·∫£ng c√°ch FAISS l√† b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuy·ªÉn ƒë·ªïi kho·∫£ng c√°ch th√†nh cosine similarity\n",
    "        \n",
    "        # N·∫øu similarity d∆∞·ªõi ng∆∞·ª°ng, g√°n nh√£n \"Kh√°c\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nh√£n \"Kh√°c\"\n",
    "        else:\n",
    "            # L·∫•y nh√£n d·ª± ƒëo√°n t·ª´ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n v√† nh√£n th·ª±c t·∫ø\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # K·∫øt th√∫c ƒëo th·ªùi gian v√† l∆∞u l·∫°i\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In th√¥ng tin cho debug (c√≥ th·ªÉ b·ªè comment n·∫øu c·∫ßn ki·ªÉm tra)\n",
    "        # print(f\"·∫¢nh: {img_path}, Similarity: {similarity_score:.4f}, Nh√£n d·ª± ƒëo√°n: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói ·∫£nh {img_path}: {e}\")\n",
    "        print(f\"  H√¨nh d·∫°ng ƒë·∫∑c tr∆∞ng: {feature.shape}, Ki·ªÉu d·ªØ li·ªáu: {feature.dtype}\")\n",
    "\n",
    "# ==== ƒê√°nh gi√° ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√°!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n‚úÖ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"‚úÖ ƒê√∫ng: {np.sum(y_true == y_pred)} / ‚ùå Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # T√≠nh th·ªùi gian x·ª≠ l√Ω trung b√¨nh cho m·ªôt ·∫£nh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    print(f\"‚è±Ô∏è T·ªïng th·ªùi gian x·ª≠ l√Ω: {sum(processing_times):.2f} gi√¢y cho {len(processing_times)} ·∫£nh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\nüñºÔ∏è Confusion matrix ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\nüìÑ Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuy·ªÉn b√°o c√°o ph√¢n lo·∫°i th√†nh DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hi·ªÉn th·ªã c√°c tham s·ªë chung cho to√†n b·ªô ch∆∞∆°ng tr√¨nh\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\nüìù C√°c tham s·ªë ƒë√°nh gi√° chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    \n",
    "    # L∆∞u b√°o c√°o chi ti·∫øt v√†o t·ªáp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{csv_output_path}'\")\n",
    "    \n",
    "    # L∆∞u c√°c tham s·ªë ƒë√°nh gi√° chung v√†o t·ªáp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\nüìä C√°c tham s·ªë ƒë√°nh gi√° chung ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # L∆∞u c·∫£ b√°o c√°o ph√¢n lo·∫°i v√† c√°c tham s·ªë chung v√†o m·ªôt t·ªáp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:17:33.253993Z",
     "iopub.status.busy": "2025-06-10T07:17:33.253738Z",
     "iopub.status.idle": "2025-06-10T07:21:54.594497Z",
     "shell.execute_reply": "2025-06-10T07:21:54.593740Z",
     "shell.execute_reply.started": "2025-06-10T07:17:33.253976Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2808/2808 [04:18<00:00, 10.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# S·ªë batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    base_model_vgg16 = VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# N∆°i l∆∞u ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_vgg16(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# N·ªëi ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:22:04.360201Z",
     "iopub.status.busy": "2025-06-10T07:22:04.359537Z",
     "iopub.status.idle": "2025-06-10T07:22:04.920202Z",
     "shell.execute_reply": "2025-06-10T07:22:04.919484Z",
     "shell.execute_reply.started": "2025-06-10T07:22:04.360170Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ th√™m 89853 vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chu·∫©n h√≥a L2 cho m·ªói vector (ƒë·ªô d√†i = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "\n",
    "# Chu·∫©n h√≥a vector ƒë·∫∑c tr∆∞ng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# K√≠ch th∆∞·ªõc vector ƒë·∫∑c tr∆∞ng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Kh·ªüi t·∫°o index FAISS s·ª≠ d·ª•ng kho·∫£ng c√°ch Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Th√™m vector ƒë√£ chu·∫©n h√≥a v√†o index\n",
    "index.add(normalized_features)\n",
    "print(\"ƒê√£ th√™m\", index.ntotal, \"vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\")\n",
    "\n",
    "# L∆∞u index v√† nh√£n\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ƒê·ªçc l·∫°i FAISS index v√† nh√£n\n",
    "index = faiss.read_index(\"faiss_features.index\")\n",
    "labels_array = np.load(\"faiss_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:22:46.344974Z",
     "iopub.status.busy": "2025-06-10T07:22:46.344455Z",
     "iopub.status.idle": "2025-06-10T08:05:22.270051Z",
     "shell.execute_reply": "2025-06-10T08:05:22.269250Z",
     "shell.execute_reply.started": "2025-06-10T07:22:46.344951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c tham s·ªë\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/processed-frames-224/Process_Frames_1/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.8  \n",
    "\n",
    "# ==== Load m√¥ h√¨nh v√† FAISS index ====\n",
    "# Load m√¥ h√¨nh InceptionV3\n",
    "model = VGG16(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n FAISS index v√† labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y FAISS index t·∫°i: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y nh√£n t·∫°i: {label_path}\")\n",
    "\n",
    "# Load FAISS index v√† labels\n",
    "try:\n",
    "    # Load index tr·ª±c ti·∫øp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n",
    "    print(f\"   - S·ªë l∆∞·ª£ng vectors: {index.ntotal}\")\n",
    "    print(f\"   - K√≠ch th∆∞·ªõc vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi t·∫£i FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duy·ªát t·∫≠p test v√† l·∫•y t·∫•t c·∫£ ·∫£nh t·ª´ m·ªói th∆∞ m·ª•c ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # L·∫•y danh s√°ch t·∫•t c·∫£ ·∫£nh trong th∆∞ m·ª•c n√†y\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== D·ª± ƒëo√°n ====\n",
    "processing_times = []  # Danh s√°ch l∆∞u th·ªùi gian x·ª≠ l√Ω t·ª´ng ·∫£nh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # B·∫Øt ƒë·∫ßu ƒëo th·ªùi gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"‚ùå Folder kh√¥ng h·ª£p l·ªá: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Tr√≠ch ƒë·∫∑c tr∆∞ng v·ªõi m√¥ h√¨nh\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chu·∫©n h√≥a L2 ƒë·∫∑c tr∆∞ng m·∫´u truy v·∫•n ƒë·ªÉ ph√π h·ª£p v·ªõi vector ƒë√£ chu·∫©n h√≥a trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "        \n",
    "        # ƒê·∫£m b·∫£o ƒë·∫∑c tr∆∞ng c√≥ ƒë·ªãnh d·∫°ng ph√π h·ª£p sau khi chu·∫©n h√≥a\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # T√¨m ki·∫øm k=1 ƒëi·ªÉm g·∫ßn nh·∫•t trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Ki·ªÉm tra similarity score t·ª´ kho·∫£ng c√°ch L2 v·ªõi vector ƒë√£ chu·∫©n h√≥a\n",
    "        # V·ªõi vector ƒë√£ chu·∫©n h√≥a L2, kho·∫£ng c√°ch Euclidean b√¨nh ph∆∞∆°ng (D) v√† cosine similarity c√≥ quan h·ªá:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Kho·∫£ng c√°ch FAISS l√† b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuy·ªÉn ƒë·ªïi kho·∫£ng c√°ch th√†nh cosine similarity\n",
    "        \n",
    "        # N·∫øu similarity d∆∞·ªõi ng∆∞·ª°ng, g√°n nh√£n \"Kh√°c\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nh√£n \"Kh√°c\"\n",
    "        else:\n",
    "            # L·∫•y nh√£n d·ª± ƒëo√°n t·ª´ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n v√† nh√£n th·ª±c t·∫ø\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # K·∫øt th√∫c ƒëo th·ªùi gian v√† l∆∞u l·∫°i\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In th√¥ng tin cho debug (c√≥ th·ªÉ b·ªè comment n·∫øu c·∫ßn ki·ªÉm tra)\n",
    "        # print(f\"·∫¢nh: {img_path}, Similarity: {similarity_score:.4f}, Nh√£n d·ª± ƒëo√°n: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói ·∫£nh {img_path}: {e}\")\n",
    "        print(f\"  H√¨nh d·∫°ng ƒë·∫∑c tr∆∞ng: {feature.shape}, Ki·ªÉu d·ªØ li·ªáu: {feature.dtype}\")\n",
    "\n",
    "# ==== ƒê√°nh gi√° ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√°!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n‚úÖ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"‚úÖ ƒê√∫ng: {np.sum(y_true == y_pred)} / ‚ùå Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # T√≠nh th·ªùi gian x·ª≠ l√Ω trung b√¨nh cho m·ªôt ·∫£nh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    print(f\"‚è±Ô∏è T·ªïng th·ªùi gian x·ª≠ l√Ω: {sum(processing_times):.2f} gi√¢y cho {len(processing_times)} ·∫£nh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\nüñºÔ∏è Confusion matrix ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\nüìÑ Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuy·ªÉn b√°o c√°o ph√¢n lo·∫°i th√†nh DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hi·ªÉn th·ªã c√°c tham s·ªë chung cho to√†n b·ªô ch∆∞∆°ng tr√¨nh\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\nüìù C√°c tham s·ªë ƒë√°nh gi√° chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    \n",
    "    # L∆∞u b√°o c√°o chi ti·∫øt v√†o t·ªáp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{csv_output_path}'\")\n",
    "    \n",
    "    # L∆∞u c√°c tham s·ªë ƒë√°nh gi√° chung v√†o t·ªáp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\nüìä C√°c tham s·ªë ƒë√°nh gi√° chung ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # L∆∞u c·∫£ b√°o c√°o ph√¢n lo·∫°i v√† c√°c tham s·ªë chung v√†o m·ªôt t·ªáp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-14T07:58:55.623Z",
     "iopub.execute_input": "2025-06-14T07:44:07.483201Z",
     "iopub.status.busy": "2025-06-14T07:44:07.482889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import onnx\n",
    "\n",
    "# ----------- C·∫•u h√¨nh ----------\n",
    "image_size = 224\n",
    "n_clusters = 128\n",
    "train_path = '/kaggle/input/processed-frames-224/Process_Frames_1/Train'\n",
    "max_descriptors = 5000000\n",
    "random.seed(42)\n",
    "\n",
    "# Mapping labels\n",
    "classes = classes_train\n",
    "\n",
    "# ----------- Ti·ªÅn x·ª≠ l√Ω ----------\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)\n",
    "    \n",
    "# L·ªçc nhi·ªÖu nh∆∞ng gi·ªØ c·∫°nh\n",
    "def denoise_bilateral(img):\n",
    "    return cv2.bilateralFilter(img, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "# L√†m n√©t ·∫£nh\n",
    "def sharpen_image(img):\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5, -1],\n",
    "                       [0, -1, 0]])\n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "# TƒÉng t∆∞∆°ng ph·∫£n b·∫±ng CLAHE\n",
    "def enhance_grayscale_contrast(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    return clahe.apply(gray)\n",
    "\n",
    "# C√¢n b·∫±ng tr·∫Øng\n",
    "def white_balance(img):\n",
    "    result = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    avg_a = np.average(result[:, :, 1])\n",
    "    avg_b = np.average(result[:, :, 2])\n",
    "    result[:, :, 1] = result[:, :, 1] - ((avg_a - 128) * (result[:, :, 0] / 255.0) * 1.1)\n",
    "    result[:, :, 2] = result[:, :, 2] - ((avg_b - 128) * (result[:, :, 0] / 255.0) * 1.1)\n",
    "    return cv2.cvtColor(result, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# H√†m ti·ªÅn x·ª≠ l√Ω t·ªïng h·ª£p\n",
    "def preprocess_image(img):\n",
    "    img = sharpen_image(img)\n",
    "    img = white_balance(img)\n",
    "    img = enhance_grayscale_contrast(img)\n",
    "    img = denoise_bilateral(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    return img\n",
    "\n",
    "num_images = sum(len(files) for _, _, files in os.walk(train_path) if files)\n",
    "print(f\"üì∏ T·ªïng s·ªë ·∫£nh: {num_images}, t·ªëi ƒëa descriptor d√πng ƒë·ªÉ hu·∫•n luy·ªán: {max_descriptors}\")\n",
    "\n",
    "# ----------- Kh·ªüi t·∫°o ----------\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# RootSIFT normalization\n",
    "def compute_rootsift(descriptors):\n",
    "    eps = 1e-7\n",
    "    descriptors = descriptors.astype(np.float32)\n",
    "    l1_norm = np.linalg.norm(descriptors, ord=1, axis=1, keepdims=True)\n",
    "    descriptors /= (l1_norm + eps)\n",
    "    descriptors = np.sqrt(descriptors)\n",
    "    return descriptors\n",
    "    \n",
    "descriptors_pool = []\n",
    "image_descriptors = []\n",
    "image_labels = []\n",
    "\n",
    "# ----------- B∆∞·ªõc 1: Tr√≠ch xu·∫•t descriptor ----------\n",
    "print(\"üß© Tr√≠ch xu·∫•t SIFT t·ª´ ·∫£nh...\")\n",
    "for label_id, label_name in classes.items():\n",
    "    label_dir = os.path.join(train_path, label_name)\n",
    "    if not os.path.isdir(label_dir):\n",
    "        continue\n",
    "    for filename in tqdm(os.listdir(label_dir), desc=f\"Processing {label_name}\"):\n",
    "        img_path = os.path.join(label_dir, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "        img = preprocess_image(img) \n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "        if descriptors is not None and len(descriptors) > 0:\n",
    "            descriptors = compute_rootsift(descriptors)  \n",
    "            image_descriptors.append(descriptors)\n",
    "            image_labels.append(label_id - 1)\n",
    "        \n",
    "            sampled = descriptors\n",
    "            if len(descriptors_pool) < max_descriptors:\n",
    "                if len(descriptors) > 100:\n",
    "                    indices = np.random.choice(len(descriptors), 100, replace=False)\n",
    "                    sampled = descriptors[indices]\n",
    "                descriptors_pool.extend(sampled)\n",
    "\n",
    "# ----------- B∆∞·ªõc 2: Hu·∫•n luy·ªán MiniBatchKMeans ----------\n",
    "print(\"üèóÔ∏è Hu·∫•n luy·ªán MiniBatchKMeans...\")\n",
    "descriptors_array = np.array(descriptors_pool)\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, n_init=10, \n",
    "                         batch_size=4096, max_iter=100, verbose=1,\n",
    "                        init_size=3 * n_clusters,\n",
    "                        tol=1e-3,\n",
    "                        max_no_improvement=10)\n",
    "kmeans.fit(descriptors_array)\n",
    "\n",
    "# ----------- B∆∞·ªõc 3: L∆∞u m√¥ h√¨nh ONNX ----------\n",
    "print(\"üíæ Chuy·ªÉn v√† l∆∞u KMeans sang ONNX...\")\n",
    "initial_type = [('input', FloatTensorType([None, descriptors_array.shape[1]]))]\n",
    "onnx_model = convert_sklearn(kmeans, initial_types=initial_type)\n",
    "onnx.save_model(onnx_model, \"kmeans_model.onnx\")\n",
    "\n",
    "# ----------- B∆∞·ªõc 4: T·∫°o histogram ƒë·∫∑c tr∆∞ng ----------\n",
    "print(\"üì¶ Chuy·ªÉn descriptor th√†nh vector ƒë·∫∑c tr∆∞ng...\")\n",
    "def extract_bow_histogram(descriptors, kmeans_model, n_clusters):\n",
    "    hist = np.zeros(n_clusters)\n",
    "    if descriptors is not None and len(descriptors) > 0:\n",
    "        clusters = kmeans_model.predict(descriptors)\n",
    "        for c in clusters:\n",
    "            hist[c] += 1\n",
    "        hist = normalize(hist.reshape(1, -1), norm='l2')[0]\n",
    "    return hist\n",
    "\n",
    "features_array = np.array([\n",
    "    extract_bow_histogram(des, kmeans, n_clusters)\n",
    "    for des in image_descriptors\n",
    "])\n",
    "\n",
    "labels_array = np.array(image_labels)\n",
    "\n",
    "# ----------- K·∫øt qu·∫£ ----------\n",
    "print(\"‚úÖ Ho√†n t·∫•t tr√≠ch ƒë·∫∑c tr∆∞ng.\")\n",
    "print(\"features_array shape:\", features_array.shape)\n",
    "print(\"labels_array shape:\", labels_array.shape)\n",
    "print(\"üìù M√¥ h√¨nh KMeans ƒë√£ ƒë∆∞·ª£c l∆∞u ·ªü: kmeans_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T14:05:36.063145Z",
     "iopub.status.busy": "2025-06-11T14:05:36.062679Z",
     "iopub.status.idle": "2025-06-11T14:05:36.378125Z",
     "shell.execute_reply": "2025-06-11T14:05:36.376586Z",
     "shell.execute_reply.started": "2025-06-11T14:05:36.063110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "print(\"üîç T√≠ch h·ª£p v·ªõi FAISS...\")\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu v√† chu·∫©n h√≥a L2\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# K√≠ch th∆∞·ªõc vector ƒë·∫∑c tr∆∞ng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Kh·ªüi t·∫°o index FAISS s·ª≠ d·ª•ng kho·∫£ng c√°ch Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Th√™m vector ƒë√£ chu·∫©n h√≥a v√†o index\n",
    "index.add(normalized_features)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ th√™m {index.ntotal} vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\")\n",
    "\n",
    "# L∆∞u index v√† nh√£n\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)\n",
    "\n",
    "print(\"üíæ ƒê√£ l∆∞u FAISS index v√† labels:\")\n",
    "print(\"- faiss_features.index\")\n",
    "print(\"- faiss_labels.npy\")\n",
    "\n",
    "# ----------- K·∫øt qu·∫£ ----------\n",
    "print(\"‚úÖ Ho√†n t·∫•t tr√≠ch ƒë·∫∑c tr∆∞ng v√† t√≠ch h·ª£p FAISS.\")\n",
    "print(\"features_array shape:\", features_array.shape)\n",
    "print(\"labels_array shape:\", labels_array.shape)\n",
    "print(\"normalized_features shape:\", normalized_features.shape)\n",
    "print(\"FAISS index dimension:\", d)\n",
    "print(\"Total vectors in index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T14:05:55.718253Z",
     "iopub.status.busy": "2025-06-11T14:05:55.717872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "import onnxruntime as ort\n",
    "\n",
    "# C√†i ƒë·∫∑t c√°c tham s·ªë\n",
    "image_size = 224\n",
    "n_clusters = 128\n",
    "test_path = \"/kaggle/input/processed-frames-224/Process_Frames_1/Test\"\n",
    "index_path = \"faiss_features.index\"\n",
    "label_path = \"faiss_labels.npy\"\n",
    "kmeans_path = \"kmeans_model.onnx\"  # ƒê∆∞·ªùng d·∫´n l∆∞u m√¥ h√¨nh KMeans ONNX\n",
    "confusion_output_path = \"confusion_matrix_sift_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_sift_faiss.csv\"\n",
    "similarity_threshold = 0.8\n",
    "\n",
    "# Chu·∫©n h√≥a L2 cho m·ªói vector (ƒë·ªô d√†i = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # epsilon tr√°nh chia cho 0\n",
    "\n",
    "# TƒÉng ƒë·ªô s√°ng\n",
    "def increase_brightness(img, value=30):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    v = np.clip(v + value, 0, 255).astype(np.uint8)\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    return cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "# L√†m n√©t ·∫£nh\n",
    "def sharpen_image(img):\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 4, -1],\n",
    "                       [0, -1, 0]])\n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "# TƒÉng t∆∞∆°ng ph·∫£n b·∫±ng CLAHE\n",
    "def enhance_contrast(img):\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# H√†m ti·ªÅn x·ª≠ l√Ω t·ªïng h·ª£p\n",
    "def preprocess_image(img):\n",
    "    # img = increase_brightness(img, value=30)\n",
    "    img = enhance_contrast(img)\n",
    "    img = sharpen_image(img)\n",
    "    return img\n",
    "\n",
    "# ==== Kh·ªüi t·∫°o SIFT detector ====\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "def compute_rootsift(descriptors):\n",
    "    eps = 1e-7\n",
    "    descriptors = descriptors.astype(np.float32)\n",
    "    l1_norm = np.linalg.norm(descriptors, ord=1, axis=1, keepdims=True)\n",
    "    descriptors /= (l1_norm + eps)\n",
    "    descriptors = np.sqrt(descriptors)\n",
    "    return descriptors\n",
    "\n",
    "# H√†m tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng SIFT BOW cho m·ªôt ·∫£nh (d√πng RootSIFT)\n",
    "def extract_sift_bow_features(img_path, sift_detector, kmeans_session, n_clusters):\n",
    "    \"\"\"Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng RootSIFT BOW cho m·ªôt ·∫£nh\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "        img = preprocess_image(img)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        keypoints, descriptors = sift_detector.detectAndCompute(gray, None)\n",
    "        if descriptors is None or len(descriptors) == 0:\n",
    "            return np.zeros(n_clusters, dtype=np.float32)\n",
    "\n",
    "        # RootSIFT\n",
    "        descriptors = compute_rootsift(descriptors)\n",
    "\n",
    "        # D·ª± ƒëo√°n cluster b·∫±ng KMeans ONNX\n",
    "        input_name = kmeans_session.get_inputs()[0].name\n",
    "        clusters = kmeans_session.run(None, {input_name: descriptors.astype(np.float32)})[0]\n",
    "\n",
    "        # T√≠nh histogram\n",
    "        hist = np.zeros(n_clusters, dtype=np.float32)\n",
    "        for c in clusters:\n",
    "            hist[int(c)] += 1\n",
    "\n",
    "        # Chu·∫©n h√≥a L2\n",
    "        hist = normalize(hist.reshape(1, -1), norm='l2')[0]\n",
    "        return hist.astype(np.float32)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==== Load FAISS index, labels, v√† KMeans ONNX model ====\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y FAISS index t·∫°i: {index_path}\")\n",
    "    exit()\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y nh√£n t·∫°i: {label_path}\")\n",
    "    exit()\n",
    "if not os.path.exists(kmeans_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y KMeans ONNX model t·∫°i: {kmeans_path}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    # Load KMeans ONNX model\n",
    "    kmeans_session = ort.InferenceSession(kmeans_path)\n",
    "\n",
    "    print(f\"‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n",
    "    print(f\"   - S·ªë l∆∞·ª£ng vectors: {index.ntotal}\")\n",
    "    print(f\"   - K√≠ch th∆∞·ªõc vector: {index.d}\")\n",
    "    print(f\"‚úÖ KMeans ONNX model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi t·∫£i models: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duy·ªát t·∫≠p test v√† l·∫•y t·∫•t c·∫£ ·∫£nh t·ª´ m·ªói th∆∞ m·ª•c ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "print(f\"üì∏ T·ªïng s·ªë ·∫£nh test: {len(all_images)}\")\n",
    "\n",
    "# ==== D·ª± ƒëo√°n ====\n",
    "processing_times = []\n",
    "\n",
    "for img_path in tqdm(all_images, desc=\"Testing with SIFT+FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"‚ùå Folder kh√¥ng h·ª£p l·ªá: {folder_name}\")\n",
    "            continue\n",
    "\n",
    "        feature = extract_sift_bow_features(img_path, sift, kmeans_session, n_clusters)\n",
    "        if feature is None:\n",
    "            print(f\"‚ùå Kh√¥ng th·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)\n",
    "        feature = feature.reshape(1, -1).astype(np.float32)\n",
    "\n",
    "        D, I = index.search(feature, 1)\n",
    "\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared / 2\n",
    "\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nh√£n \"Kh√°c\"\n",
    "        else:\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data) + 1\n",
    "\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "\n",
    "        end_time = time.time()\n",
    "        processing_times.append(end_time - start_time)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói x·ª≠ l√Ω ·∫£nh {img_path}: {e}\")\n",
    "\n",
    "# ==== ƒê√°nh gi√° ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√°!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    \n",
    "    print(f\"\\n‚úÖ SIFT+FAISS Results:\")\n",
    "    print(f\"   üìä Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"   ‚úÖ ƒê√∫ng: {np.sum(y_true == y_pred)} / ‚ùå Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # T√≠nh th·ªùi gian x·ª≠ l√Ω trung b√¨nh cho m·ªôt ·∫£nh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n‚è±Ô∏è Performance:\")\n",
    "    print(f\"   üöÄ Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    print(f\"   ‚è±Ô∏è T·ªïng th·ªùi gian x·ª≠ l√Ω: {sum(processing_times):.2f} gi√¢y cho {len(processing_times)} ·∫£nh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\nüñºÔ∏è Confusion matrix ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\nüìÑ Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=4, output_dict=True, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Chuy·ªÉn b√°o c√°o ph√¢n lo·∫°i th√†nh DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    # Hi·ªÉn th·ªã b√°o c√°o\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"support\"]].round(4))\n",
    "    \n",
    "    # C√°c tham s·ªë ƒë√°nh gi√° chung\n",
    "    macro_avg = report['macro avg']\n",
    "    weighted_avg = report['weighted avg']\n",
    "    \n",
    "    print(f\"\\nüìù T·ªïng k·∫øt ƒë√°nh gi√° (SIFT+FAISS):\")\n",
    "    print(f\"   üéØ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"   üìä Macro Precision: {macro_avg['precision']:.4f}\")\n",
    "    print(f\"   üìä Macro Recall: {macro_avg['recall']:.4f}\")\n",
    "    print(f\"   üìä Macro F1-score: {macro_avg['f1-score']:.4f}\")\n",
    "    print(f\"   üìä Weighted Precision: {weighted_avg['precision']:.4f}\")\n",
    "    print(f\"   üìä Weighted Recall: {weighted_avg['recall']:.4f}\")\n",
    "    print(f\"   üìä Weighted F1-score: {weighted_avg['f1-score']:.4f}\")\n",
    "    print(f\"   üöÄ Avg Processing Time: {avg_processing_time:.4f} sec/image\")\n",
    "    \n",
    "    # L∆∞u b√°o c√°o chi ti·∫øt v√†o t·ªáp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{csv_output_path}'\")\n",
    "    \n",
    "    # L∆∞u c√°c tham s·ªë ƒë√°nh gi√° chung v√†o t·ªáp CSV\n",
    "    evaluation_metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_precision': macro_avg['precision'],\n",
    "        'macro_recall': macro_avg['recall'],\n",
    "        'macro_f1_score': macro_avg['f1-score'],\n",
    "        'weighted_precision': weighted_avg['precision'],\n",
    "        'weighted_recall': weighted_avg['recall'],\n",
    "        'weighted_f1_score': weighted_avg['f1-score'],\n",
    "        'avg_processing_time': avg_processing_time,\n",
    "        'total_images': len(processing_times),\n",
    "        'similarity_threshold': similarity_threshold,\n",
    "    }\n",
    "    \n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_sift_faiss.csv', index=False)\n",
    "    print(f\"üìä C√°c tham s·ªë ƒë√°nh gi√° chung ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'evaluation_metrics_sift_faiss.csv'\")\n",
    "    \n",
    "    # L∆∞u b√°o c√°o cu·ªëi c√πng\n",
    "    final_df = report_df.copy()\n",
    "    for key, value in evaluation_metrics.items():\n",
    "        final_df[key] = value\n",
    "    \n",
    "    final_df.to_csv('final_classification_report_sift_faiss.csv', index=True)\n",
    "    print(f\"üìä B√°o c√°o ph√¢n lo·∫°i cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'final_classification_report_sift_faiss.csv'\")\n",
    "    \n",
    "    print(f\"\\nüéâ Ho√†n t·∫•t ƒë√°nh gi√° v·ªõi SIFT+FAISS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOG + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T12:21:20.856024Z",
     "iopub.status.busy": "2025-06-10T12:21:20.855735Z",
     "iopub.status.idle": "2025-06-10T12:51:06.737995Z",
     "shell.execute_reply": "2025-06-10T12:51:06.737128Z",
     "shell.execute_reply.started": "2025-06-10T12:21:20.856004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "# ----------- C·∫•u h√¨nh ----------\n",
    "image_size = 224\n",
    "train_path = '/kaggle/input/processed-frames-224/Process_Frames_1/Train'\n",
    "random.seed(42)\n",
    "\n",
    "classes = classes_train\n",
    "\n",
    "# ----------- Tr√≠ch xu·∫•t HOG ----------\n",
    "print(\"üì∏ Tr√≠ch xu·∫•t HOG t·ª´ ·∫£nh...\")\n",
    "hog_features = []\n",
    "hog_labels = []\n",
    "\n",
    "for label_id, label_name in classes.items():\n",
    "    label_dir = os.path.join(train_path, label_name)\n",
    "    if not os.path.isdir(label_dir):\n",
    "        continue\n",
    "    for filename in tqdm(os.listdir(label_dir), desc=f\"Processing {label_name}\"):\n",
    "        img_path = os.path.join(label_dir, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img_resized = resize(img, (image_size, image_size), anti_aliasing=True)\n",
    "        gray = rgb2gray(img_resized)\n",
    "\n",
    "        # Tr√≠ch HOG\n",
    "        features = hog(\n",
    "            gray,\n",
    "            orientations=9,                \n",
    "            pixels_per_cell=(24, 24),       \n",
    "            cells_per_block=(3, 3),\n",
    "            block_norm='L2-Hys',\n",
    "            feature_vector=True\n",
    "        )\n",
    "        hog_features.append(features)\n",
    "        hog_labels.append(label_id - 1)\n",
    "\n",
    "hog_features = np.array(hog_features)\n",
    "hog_labels = np.array(hog_labels)\n",
    "print(\"‚úÖ Tr√≠ch xu·∫•t HOG ho√†n t·∫•t.\")\n",
    "print(\"HOG feature shape:\", hog_features.shape)\n",
    "print(\"Labels shape:\", hog_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T12:53:16.329732Z",
     "iopub.status.busy": "2025-06-10T12:53:16.329355Z",
     "iopub.status.idle": "2025-06-10T12:53:20.069749Z",
     "shell.execute_reply": "2025-06-10T12:53:20.069089Z",
     "shell.execute_reply.started": "2025-06-10T12:53:16.329710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "print(\"üîç T√≠ch h·ª£p v·ªõi FAISS...\")\n",
    "\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  \n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu v√† chu·∫©n h√≥a L2\n",
    "normalized_features = l2_normalize(hog_features.astype('float32'))\n",
    "\n",
    "# K√≠ch th∆∞·ªõc vector ƒë·∫∑c tr∆∞ng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Kh·ªüi t·∫°o index FAISS s·ª≠ d·ª•ng kho·∫£ng c√°ch Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Th√™m vector ƒë√£ chu·∫©n h√≥a v√†o index\n",
    "index.add(normalized_features)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ th√™m {index.ntotal} vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\")\n",
    "\n",
    "# L∆∞u index v√† nh√£n\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", hog_labels)\n",
    "\n",
    "print(\"üíæ ƒê√£ l∆∞u FAISS index v√† labels:\")\n",
    "print(\"- faiss_features.index\")\n",
    "print(\"- faiss_labels.npy\")\n",
    "\n",
    "# ----------- K·∫øt qu·∫£ ----------\n",
    "print(\"‚úÖ Ho√†n t·∫•t tr√≠ch ƒë·∫∑c tr∆∞ng v√† t√≠ch h·ª£p FAISS.\")\n",
    "print(\"features_array shape:\", hog_features.shape)\n",
    "print(\"labels_array shape:\", hog_labels.shape)\n",
    "print(\"normalized_features shape:\", normalized_features.shape)\n",
    "print(\"FAISS index dimension:\", d)\n",
    "print(\"Total vectors in index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T12:53:47.351558Z",
     "iopub.status.busy": "2025-06-10T12:53:47.351248Z",
     "iopub.status.idle": "2025-06-10T13:57:34.960848Z",
     "shell.execute_reply": "2025-06-10T13:57:34.960106Z",
     "shell.execute_reply.started": "2025-06-10T12:53:47.351538Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import normalize\n",
    "from skimage.feature import hog\n",
    "from skimage import color\n",
    "import traceback\n",
    "\n",
    "# C√†i ƒë·∫∑t c√°c tham s·ªë\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/processed-frames-224/Process_Frames_1/Test\"\n",
    "index_path = \"faiss_features.index\"\n",
    "label_path = \"faiss_labels.npy\"\n",
    "confusion_output_path = \"confusion_matrix_hog_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_hog_faiss.csv\"\n",
    "similarity_threshold = 0.8\n",
    "\n",
    "# Chu·∫©n h√≥a L2 cho m·ªói vector (ƒë·ªô d√†i = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # epsilon tr√°nh chia cho 0\n",
    "\n",
    "# H√†m tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng HOG BOW cho m·ªôt ·∫£nh\n",
    "def extract_hog_features(img_path):\n",
    "    \"\"\"Tr√≠ch xu·∫•t vector HOG tr·ª±c ti·∫øp t·ª´ ·∫£nh\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "        gray = color.rgb2gray(img)\n",
    "\n",
    "        # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng HOG (vector 1 chi·ªÅu)\n",
    "        hog_vector = hog(gray,\n",
    "                        orientations=9,                \n",
    "                        pixels_per_cell=(24, 24),       \n",
    "                        cells_per_block=(3, 3),\n",
    "                        block_norm='L2-Hys',\n",
    "                        feature_vector=True)\n",
    "\n",
    "        if hog_vector is None or len(hog_vector) == 0:\n",
    "            return None\n",
    "\n",
    "        return hog_vector.astype(np.float32)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói tr√≠ch xu·∫•t HOG t·ª´ {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==== Load FAISS index, labels ====\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y FAISS index t·∫°i: {index_path}\")\n",
    "    exit()\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y nh√£n t·∫°i: {label_path}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n",
    "    print(f\"   - S·ªë l∆∞·ª£ng vectors: {index.ntotal}\")\n",
    "    print(f\"   - K√≠ch th∆∞·ªõc vector: {index.d}\")\n",
    "\n",
    "except Exception as e:\n",
    "    exit()\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duy·ªát t·∫≠p test v√† l·∫•y t·∫•t c·∫£ ·∫£nh t·ª´ m·ªói th∆∞ m·ª•c ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "print(f\"üì∏ T·ªïng s·ªë ·∫£nh test: {len(all_images)}\")\n",
    "\n",
    "# ==== D·ª± ƒëo√°n ====\n",
    "processing_times = []\n",
    "\n",
    "for img_path in tqdm(all_images, desc=\"Testing with HOG+FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"‚ùå Folder kh√¥ng h·ª£p l·ªá: {folder_name}\")\n",
    "            continue\n",
    "\n",
    "        # Tr√≠ch xu·∫•t vector HOG tr·ª±c ti·∫øp\n",
    "        feature = extract_hog_features(img_path)\n",
    "\n",
    "        if feature is None:\n",
    "            print(f\"‚ùå Kh√¥ng th·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Ki·ªÉm tra k√≠ch th∆∞·ªõc vector\n",
    "        if feature.shape[0] != index.d:\n",
    "            print(f\"‚ö†Ô∏è Vector kh√¥ng ƒë√∫ng k√≠ch th∆∞·ªõc ({feature.shape[0]} vs {index.d}) t·∫°i: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Chu·∫©n h√≥a L2\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)\n",
    "        feature = feature.reshape(1, -1).astype(np.float32)\n",
    "\n",
    "        # T√¨m ·∫£nh g·∫ßn nh·∫•t trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared / 2\n",
    "\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # \"Kh√°c\"\n",
    "        else:\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data) + 1\n",
    "\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "\n",
    "        end_time = time.time()\n",
    "        processing_times.append(end_time - start_time)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói x·ª≠ l√Ω ·∫£nh {img_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ==== ƒê√°nh gi√° ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√°!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    \n",
    "    print(f\"\\n‚úÖ HOG+FAISS Results:\")\n",
    "    print(f\"   üìä Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"   ‚úÖ ƒê√∫ng: {np.sum(y_true == y_pred)} / ‚ùå Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # T√≠nh th·ªùi gian x·ª≠ l√Ω trung b√¨nh cho m·ªôt ·∫£nh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n‚è±Ô∏è Performance:\")\n",
    "    print(f\"   üöÄ Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    print(f\"   ‚è±Ô∏è T·ªïng th·ªùi gian x·ª≠ l√Ω: {sum(processing_times):.2f} gi√¢y cho {len(processing_times)} ·∫£nh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\nüñºÔ∏è Confusion matrix ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\nüìÑ Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=4, output_dict=True, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Chuy·ªÉn b√°o c√°o ph√¢n lo·∫°i th√†nh DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    # Hi·ªÉn th·ªã b√°o c√°o\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"support\"]].round(4))\n",
    "    \n",
    "    # C√°c tham s·ªë ƒë√°nh gi√° chung\n",
    "    macro_avg = report['macro avg']\n",
    "    weighted_avg = report['weighted avg']\n",
    "    \n",
    "    print(f\"\\nüìù T·ªïng k·∫øt ƒë√°nh gi√° (HOG+FAISS):\")\n",
    "    print(f\"   üéØ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"   üìä Macro Precision: {macro_avg['precision']:.4f}\")\n",
    "    print(f\"   üìä Macro Recall: {macro_avg['recall']:.4f}\")\n",
    "    print(f\"   üìä Macro F1-score: {macro_avg['f1-score']:.4f}\")\n",
    "    print(f\"   üìä Weighted Precision: {weighted_avg['precision']:.4f}\")\n",
    "    print(f\"   üìä Weighted Recall: {weighted_avg['recall']:.4f}\")\n",
    "    print(f\"   üìä Weighted F1-score: {weighted_avg['f1-score']:.4f}\")\n",
    "    print(f\"   üöÄ Avg Processing Time: {avg_processing_time:.4f} sec/image\")\n",
    "    \n",
    "    # L∆∞u b√°o c√°o chi ti·∫øt v√†o t·ªáp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{csv_output_path}'\")\n",
    "    \n",
    "    # L∆∞u c√°c tham s·ªë ƒë√°nh gi√° chung v√†o t·ªáp CSV\n",
    "    evaluation_metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_precision': macro_avg['precision'],\n",
    "        'macro_recall': macro_avg['recall'],\n",
    "        'macro_f1_score': macro_avg['f1-score'],\n",
    "        'weighted_precision': weighted_avg['precision'],\n",
    "        'weighted_recall': weighted_avg['recall'],\n",
    "        'weighted_f1_score': weighted_avg['f1-score'],\n",
    "        'avg_processing_time': avg_processing_time,\n",
    "        'total_images': len(processing_times),\n",
    "        'similarity_threshold': similarity_threshold,\n",
    "    }\n",
    "    \n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_hog_faiss.csv', index=False)\n",
    "    print(f\"üìä C√°c tham s·ªë ƒë√°nh gi√° chung ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'evaluation_metrics_hog_faiss.csv'\")\n",
    "    \n",
    "    # L∆∞u b√°o c√°o cu·ªëi c√πng\n",
    "    final_df = report_df.copy()\n",
    "    for key, value in evaluation_metrics.items():\n",
    "        final_df[key] = value\n",
    "    \n",
    "    final_df.to_csv('final_classification_report_hog_faiss.csv', index=True)\n",
    "    print(f\"üìä B√°o c√°o ph√¢n lo·∫°i cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'final_classification_report_hog_faiss.csv'\")\n",
    "    \n",
    "    print(f\"\\nüéâ Ho√†n t·∫•t ƒë√°nh gi√° v·ªõi HOG+FAISS!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7622968,
     "sourceId": 12107806,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
