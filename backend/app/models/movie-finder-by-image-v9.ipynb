{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-10T11:14:24.337942Z",
     "iopub.status.busy": "2025-05-10T11:14:24.337718Z",
     "iopub.status.idle": "2025-05-10T11:16:58.916126Z",
     "shell.execute_reply": "2025-05-10T11:16:58.915462Z",
     "shell.execute_reply.started": "2025-05-10T11:14:24.337919Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T07:28:29.940265Z",
     "iopub.status.busy": "2025-06-14T07:28:29.940077Z",
     "iopub.status.idle": "2025-06-14T07:28:46.256695Z",
     "shell.execute_reply": "2025-06-14T07:28:46.255436Z",
     "shell.execute_reply.started": "2025-06-14T07:28:29.940247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q faiss-cpu\n",
    "!pip install -q tensorflow==2.18.0\n",
    "!pip install -q skl2onnx onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T11:27:09.732347Z",
     "iopub.status.busy": "2025-06-11T11:27:09.731728Z",
     "iopub.status.idle": "2025-06-11T11:27:25.000972Z",
     "shell.execute_reply": "2025-06-11T11:27:25.000096Z",
     "shell.execute_reply.started": "2025-06-11T11:27:09.732318Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Kiểm tra phiên bản TensorFlow\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Cấu hình memory growth để sử dụng GPU hiệu quả\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"Tìm thấy {len(gpus)} GPU:\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "    \n",
    "    # Cấu hình memory growth\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # Một số tùy chọn để tối ưu hiệu suất cho GPU T4\n",
    "        os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "        os.environ['TF_GPU_THREAD_COUNT'] = '2'  # Tương ứng với số GPU\n",
    "        \n",
    "        # Hiển thị các GPU logic\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"Số lượng GPU vật lý: {len(gpus)}, số lượng GPU logic: {len(logical_gpus)}\")\n",
    "        \n",
    "        # Thông tin chi tiết về GPU\n",
    "        from tensorflow.python.client import device_lib\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        gpu_list = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "        print(f\"Danh sách GPU: {gpu_list}\")\n",
    "        \n",
    "        # Hiển thị thông tin CUDA và cuDNN\n",
    "        build_info = tf.sysconfig.get_build_info()\n",
    "        print(f\"CUDA version: {build_info.get('cuda_version', 'N/A')}\")\n",
    "        print(f\"cuDNN version: {build_info.get('cudnn_version', 'N/A')}\")\n",
    "        \n",
    "        # Kiểm tra xem GPU có thực sự được sử dụng hay không\n",
    "        print(\"\\nXác nhận GPU đang hoạt động bằng phép tính nhỏ:\")\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "            b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(f\"Tính toán trên GPU: {c}\")\n",
    "            print(f\"Đang chạy trên thiết bị: {c.device}\")\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        print(f\"Lỗi khi cấu hình GPU: {e}\")\n",
    "else:\n",
    "    print(\"Không tìm thấy GPU! Đang sử dụng CPU.\")\n",
    "    \n",
    "    # Kiểm tra thông tin CPU\n",
    "    cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "    print(f\"Tìm thấy {len(cpu_devices)} CPU: {cpu_devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T07:30:38.542085Z",
     "iopub.status.busy": "2025-06-14T07:30:38.541176Z",
     "iopub.status.idle": "2025-06-14T07:30:57.331754Z",
     "shell.execute_reply": "2025-06-14T07:30:57.330957Z",
     "shell.execute_reply.started": "2025-06-14T07:30:38.542050Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import gc\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm \n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, Flatten, Dropout, AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, Resizing\n",
    "from tensorflow.keras.layers import MaxPooling2D, Activation, BatchNormalization, Attention, Reshape, RepeatVector, Lambda, Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import joblib\n",
    "\n",
    "import onnxruntime as ort\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "# Bỏ qua các cảnh báo\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# In phiên bản TensorFlow hiện tại\n",
    "print('TensorFlow Version ' + tf.__version__)\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    # Thiết lập seed để đảm bảo tính tái lập (reproducibility)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Gọi hàm seed_everything để thiết lập seed mặc định\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trực quan dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:18:22.540345Z",
     "iopub.status.busy": "2025-05-25T12:18:22.539283Z",
     "iopub.status.idle": "2025-05-25T12:20:49.559835Z",
     "shell.execute_reply": "2025-05-25T12:20:49.559177Z",
     "shell.execute_reply.started": "2025-05-25T12:18:22.540318Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Thư mục Train====\n",
      "==> Tổng số ảnh: 89852\n",
      "====Thư mục Test====\n",
      "==> Tổng số ảnh: 27612\n"
     ]
    }
   ],
   "source": [
    "def count_images_per_folder(root_dir, image_extensions=None):\n",
    "    if image_extensions is None:\n",
    "        image_extensions = ['.jpg']\n",
    "\n",
    "    total = 0\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        count = sum(1 for file in files if any(file.lower().endswith(ext) for ext in image_extensions))\n",
    "        if count > 0:\n",
    "            relative_path = os.path.relpath(subdir, root_dir)\n",
    "            # print(f\"Thư mục '{relative_path}': {count} ảnh\")\n",
    "            total += count\n",
    "\n",
    "    print(f\"==> Tổng số ảnh: {total}\")\n",
    "\n",
    "print(\"====Thư mục Train====\")\n",
    "folder_train_path = '/kaggle/input/processed-frames-224/Process_Frames_1/Train'\n",
    "count_images_per_folder(folder_train_path)\n",
    "\n",
    "print(\"====Thư mục Test====\")\n",
    "folder_test_path = '/kaggle/input/processed-frames-224/Process_Frames_1/Test'\n",
    "count_images_per_folder(folder_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:54.511900Z",
     "iopub.status.busy": "2025-05-25T12:20:54.511211Z",
     "iopub.status.idle": "2025-05-25T12:21:25.593994Z",
     "shell.execute_reply": "2025-05-25T12:21:25.593247Z",
     "shell.execute_reply.started": "2025-05-25T12:20:54.511876Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_images_per_folder(root_dir, image_extensions=None):\n",
    "    if image_extensions is None:\n",
    "        image_extensions = ['.jpg']\n",
    "\n",
    "    folder_counts = {}\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        if subdir == root_dir:\n",
    "            continue  # bỏ qua thư mục gốc\n",
    "        count = sum(1 for file in files if any(file.lower().endswith(ext) for ext in image_extensions))\n",
    "        if count > 0:\n",
    "            folder_name = os.path.basename(subdir)\n",
    "            folder_counts[folder_name] = count\n",
    "\n",
    "    return folder_counts\n",
    "\n",
    "def plot_image_counts(folder_counts):\n",
    "    folders = list(folder_counts.keys())\n",
    "    counts = list(folder_counts.values())\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.bar(folders, counts, color='skyblue')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('Tên thư mục con')\n",
    "    plt.ylabel('Số lượng ảnh')\n",
    "    plt.title('Số lượng ảnh trong từng thư mục con')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Tập Train\n",
    "counts = count_images_per_folder(folder_train_path)\n",
    "plot_image_counts(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:21:29.919157Z",
     "iopub.status.busy": "2025-05-25T12:21:29.918847Z",
     "iopub.status.idle": "2025-05-25T12:21:43.343924Z",
     "shell.execute_reply": "2025-05-25T12:21:43.343218Z",
     "shell.execute_reply.started": "2025-05-25T12:21:29.919131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tập Test\n",
    "counts = count_images_per_folder(folder_test_path)\n",
    "plot_image_counts(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tạo tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T07:31:27.484516Z",
     "iopub.status.busy": "2025-06-14T07:31:27.484161Z",
     "iopub.status.idle": "2025-06-14T07:31:27.491497Z",
     "shell.execute_reply": "2025-06-14T07:31:27.490578Z",
     "shell.execute_reply.started": "2025-06-14T07:31:27.484492Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "n_classes = 45\n",
    "batch_size = 32\n",
    "\n",
    "classes_train = {\n",
    "    1: \"21_Ngay_Yeu_Em\",\n",
    "    2: \"4_Nam_2_Chang_1_Tinh_Yeu\",\n",
    "    3: \"An_Tet_Ben_Con\",\n",
    "    4: \"Bay_Ngot_Ngao\",\n",
    "    5: \"Benh_Vien_Ma\",\n",
    "    6: \"Bi_Mat_Lai_Bi_Mat\",\n",
    "    7: \"Bi_Mat_Trong_Suong_Mu\",\n",
    "    8: \"Bo_Tu_Oan_Gia\",\n",
    "    9: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    10: \"Chu_Tich_Giao_Hang\",\n",
    "    11: \"Chuyen_Tet\",\n",
    "    12: \"Co_Ba_Sai_Gon\",\n",
    "    13: \"Dao_Pho_Va_Piano\",\n",
    "    14: \"Dat_Rung_Phuong_Nam\",\n",
    "    15: \"Dia_Dao\",\n",
    "    16: \"Dinh_Menh_Thien_Y\",\n",
    "    17: \"Doi_Mat_Am_Duong\",\n",
    "    18: \"Em_Chua_18\",\n",
    "    19: \"Em_La_Cua_Em\",\n",
    "    20: \"Gai_Gia_Lam_Chieu_3\",\n",
    "    21: \"Gia_Ngheo_Gap_Phat\",\n",
    "    22: \"Hem_Cut\",\n",
    "    23: \"Hoan_Doi\",\n",
    "    24: \"Ke_An_Danh\",\n",
    "    25: \"Ke_An_Hon\",\n",
    "    26: \"Lam_Giau_Voi_Ma\",\n",
    "    27: \"Lat_Mat_1\",\n",
    "    28: \"Lo_Mat\",\n",
    "    29: \"Ma_Da\",\n",
    "    30: \"Mat_Biec\",\n",
    "    31: \"Nghe_Sieu_De\",\n",
    "    32: \"Nhung_Nu_Hon_Ruc_Ro\",\n",
    "    33: \"Ong_Ngoai_Tuoi_30\",\n",
    "    34: \"Phap_Su_Tap_Su\",\n",
    "    35: \"Quy_Cau\",\n",
    "    36: \"Quy_Co_Thua_Ke\",\n",
    "    37: \"Ra_Mat_Gia_Tien\",\n",
    "    38: \"Sieu_Lua_Gap_Sieu_Lay\",\n",
    "    39: \"Sieu_Tro_Ly\",\n",
    "    40: \"Tam_Cam_Chuyen_Chua_Ke\",\n",
    "    41: \"Taxi_Em_Ten_Gi\",\n",
    "    42: \"The_Call\",\n",
    "    43: \"Thien_Menh_Anh_Hung\",\n",
    "    44: \"Tieu_Thu_Va_Ba_Dau_Gau\",\n",
    "    45: \"Tren_Ban_Nhau_Duoi_Ban_Muu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T07:31:29.450687Z",
     "iopub.status.busy": "2025-06-14T07:31:29.449975Z",
     "iopub.status.idle": "2025-06-14T07:31:29.457766Z",
     "shell.execute_reply": "2025-06-14T07:31:29.456730Z",
     "shell.execute_reply.started": "2025-06-14T07:31:29.450653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classes_test = {\n",
    "    1: \"21_Ngay_Yeu_Em\",\n",
    "    2: \"4_Nam_2_Chang_1_Tinh_Yeu\",\n",
    "    3: \"An_Tet_Ben_Con\",\n",
    "    4: \"Bay_Ngot_Ngao\",\n",
    "    5: \"Benh_Vien_Ma\",\n",
    "    6: \"Bi_Mat_Lai_Bi_Mat\",\n",
    "    7: \"Bi_Mat_Trong_Suong_Mu\",\n",
    "    8: \"Bo_Tu_Oan_Gia\",\n",
    "    9: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    10: \"Chu_Tich_Giao_Hang\",\n",
    "    11: \"Chuyen_Tet\",\n",
    "    12: \"Co_Ba_Sai_Gon\",\n",
    "    13: \"Dao_Pho_Va_Piano\",\n",
    "    14: \"Dat_Rung_Phuong_Nam\",\n",
    "    15: \"Dia_Dao\",\n",
    "    16: \"Dinh_Menh_Thien_Y\",\n",
    "    17: \"Doi_Mat_Am_Duong\",\n",
    "    18: \"Em_Chua_18\",\n",
    "    19: \"Em_La_Cua_Em\",\n",
    "    20: \"Gai_Gia_Lam_Chieu_3\",\n",
    "    21: \"Gia_Ngheo_Gap_Phat\",\n",
    "    22: \"Hem_Cut\",\n",
    "    23: \"Hoan_Doi\",\n",
    "    24: \"Ke_An_Danh\",\n",
    "    25: \"Ke_An_Hon\",\n",
    "    26: \"Lam_Giau_Voi_Ma\",\n",
    "    27: \"Lat_Mat_1\",\n",
    "    28: \"Lo_Mat\",\n",
    "    29: \"Ma_Da\",\n",
    "    30: \"Mat_Biec\",\n",
    "    31: \"Nghe_Sieu_De\",\n",
    "    32: \"Nhung_Nu_Hon_Ruc_Ro\",\n",
    "    33: \"Ong_Ngoai_Tuoi_30\",\n",
    "    34: \"Phap_Su_Tap_Su\",\n",
    "    35: \"Quy_Cau\",\n",
    "    36: \"Quy_Co_Thua_Ke\",\n",
    "    37: \"Ra_Mat_Gia_Tien\",\n",
    "    38: \"Sieu_Lua_Gap_Sieu_Lay\",\n",
    "    39: \"Sieu_Tro_Ly\",\n",
    "    40: \"Tam_Cam_Chuyen_Chua_Ke\",\n",
    "    41: \"Taxi_Em_Ten_Gi\",\n",
    "    42: \"The_Call\",\n",
    "    43: \"Thien_Menh_Anh_Hung\",\n",
    "    44: \"Tieu_Thu_Va_Ba_Dau_Gau\",\n",
    "    45: \"Tren_Ban_Nhau_Duoi_Ban_Muu\",\n",
    "    46: \"Khac\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T04:19:04.405182Z",
     "iopub.status.busy": "2025-06-10T04:19:04.404494Z",
     "iopub.status.idle": "2025-06-10T04:19:35.691733Z",
     "shell.execute_reply": "2025-06-10T04:19:35.691159Z",
     "shell.execute_reply.started": "2025-06-10T04:19:04.405148Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89853 files belonging to 45 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = '/kaggle/input/processed-frames-224/Process_Frames_1/Train'\n",
    "\n",
    "# Tạo dataset từ thư mục\n",
    "train_df = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical', \n",
    "    seed=1,\n",
    "    image_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, \n",
    ")\n",
    "\n",
    "# Chuẩn hóa ảnh về [0, 1] + prefetch\n",
    "# normalization_layer = L.Rescaling(1./255)\n",
    "# train_df = train_df.map(lambda x, y: (normalization_layer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# train_df = train_df.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T13:19:18.012479Z",
     "iopub.status.busy": "2025-06-09T13:19:18.011935Z",
     "iopub.status.idle": "2025-06-09T13:19:18.743834Z",
     "shell.execute_reply": "2025-06-09T13:19:18.742982Z",
     "shell.execute_reply.started": "2025-06-09T13:19:18.012453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_df))  \n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(5, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for img, ax in zip(images[:9], axes): \n",
    "    ax.imshow(img.numpy().astype(\"uint8\"))  \n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xóa file trong thư mục"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T07:31:02.490422Z",
     "iopub.status.busy": "2025-06-14T07:31:02.489824Z",
     "iopub.status.idle": "2025-06-14T07:31:02.516734Z",
     "shell.execute_reply": "2025-06-14T07:31:02.515892Z",
     "shell.execute_reply.started": "2025-06-14T07:31:02.490397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def delete_all_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # xóa file hoặc symbolic link\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # xóa thư mục và toàn bộ nội dung bên trong\n",
    "        except Exception as e:\n",
    "            print(f\"Không thể xóa {file_path}: {e}\")\n",
    "\n",
    "# Ví dụ:\n",
    "folder = \"/kaggle/working/\"\n",
    "delete_all_in_folder(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T14:00:51.019594Z",
     "iopub.status.busy": "2025-06-10T14:00:51.019079Z",
     "iopub.status.idle": "2025-06-10T14:00:51.713004Z",
     "shell.execute_reply": "2025-06-10T14:00:51.712329Z",
     "shell.execute_reply.started": "2025-06-10T14:00:51.019571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50 + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T02:46:40.114693Z",
     "iopub.status.busy": "2025-06-10T02:46:40.114421Z",
     "iopub.status.idle": "2025-06-10T02:56:44.232662Z",
     "shell.execute_reply": "2025-06-10T02:56:44.232016Z",
     "shell.execute_reply.started": "2025-06-10T02:46:40.114665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Số batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Khởi tạo model trong context của strategy\n",
    "with strategy.scope():\n",
    "    base_model_resnet50 = ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Nơi lưu đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# Lặp qua từng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_resnet50(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# Đánh nhãn ứng với đặc trưng\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T02:56:49.743021Z",
     "iopub.status.busy": "2025-06-10T02:56:49.742745Z",
     "iopub.status.idle": "2025-06-10T02:56:51.744663Z",
     "shell.execute_reply": "2025-06-10T02:56:51.743862Z",
     "shell.execute_reply.started": "2025-06-10T02:56:49.743002Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã thêm 89853 vector đã chuẩn hóa vào FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chuẩn hóa L2 cho mỗi vector (độ dài = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # thêm epsilon để tránh chia cho 0\n",
    "\n",
    "# Chuẩn hóa vector đặc trưng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# Kích thước vector đặc trưng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Khởi tạo index FAISS sử dụng khoảng cách Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Thêm vector đã chuẩn hóa vào index\n",
    "index.add(normalized_features)\n",
    "print(\"Đã thêm\", index.ntotal, \"vector đã chuẩn hóa vào FAISS index.\")\n",
    "\n",
    "# Lưu index và nhãn\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:10:28.309080Z",
     "iopub.status.busy": "2025-05-10T13:10:28.308815Z",
     "iopub.status.idle": "2025-05-10T13:10:28.659179Z",
     "shell.execute_reply": "2025-05-10T13:10:28.658379Z",
     "shell.execute_reply.started": "2025-05-10T13:10:28.309060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Đọc lại FAISS index và nhãn\n",
    "index = faiss.read_index(\"faiss_features.index\")\n",
    "labels_array = np.load(\"faiss_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-10T04:06:04.660Z",
     "iopub.execute_input": "2025-06-10T02:56:59.576914Z",
     "iopub.status.busy": "2025-06-10T02:56:59.576630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/processed-frames-224/Process_Frames_1/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.8  \n",
    "\n",
    "# ==== Load mô hình và FAISS index ====\n",
    "# Load mô hình ResNet50 \n",
    "model = ResNet50(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn FAISS index và labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"❌ Không tìm thấy FAISS index tại: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"❌ Không tìm thấy nhãn tại: {label_path}\")\n",
    "\n",
    "# Load FAISS index và labels\n",
    "try:\n",
    "    # Load index trực tiếp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"✅ FAISS index đã được tải thành công!\")\n",
    "    print(f\"   - Số lượng vectors: {index.ntotal}\")\n",
    "    print(f\"   - Kích thước vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy tất cả ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # Lấy danh sách tất cả ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "processing_times = []  # Danh sách lưu thời gian xử lý từng ảnh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # Bắt đầu đo thời gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Trích đặc trưng với mô hình\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chuẩn hóa L2 đặc trưng mẫu truy vấn để phù hợp với vector đã chuẩn hóa trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Thêm epsilon để tránh chia cho 0\n",
    "        \n",
    "        # Đảm bảo đặc trưng có định dạng phù hợp sau khi chuẩn hóa\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # Tìm kiếm k=1 điểm gần nhất trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Kiểm tra similarity score từ khoảng cách L2 với vector đã chuẩn hóa\n",
    "        # Với vector đã chuẩn hóa L2, khoảng cách Euclidean bình phương (D) và cosine similarity có quan hệ:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Khoảng cách FAISS là bình phương khoảng cách Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuyển đổi khoảng cách thành cosine similarity\n",
    "        \n",
    "        # Nếu similarity dưới ngưỡng, gán nhãn \"Khác\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nhãn \"Khác\"\n",
    "        else:\n",
    "            # Lấy nhãn dự đoán từ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # Lưu kết quả dự đoán và nhãn thực tế\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # Kết thúc đo thời gian và lưu lại\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In thông tin cho debug (có thể bỏ comment nếu cần kiểm tra)\n",
    "        # print(f\"Ảnh: {img_path}, Similarity: {similarity_score:.4f}, Nhãn dự đoán: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # Tính thời gian xử lý trung bình cho một ảnh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n⏱️ Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    print(f\"⏱️ Tổng thời gian xử lý: {sum(processing_times):.2f} giây cho {len(processing_times)} ảnh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    \n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "    \n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNetB4 + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T04:19:50.628098Z",
     "iopub.status.busy": "2025-06-10T04:19:50.627521Z",
     "iopub.status.idle": "2025-06-10T04:44:16.515835Z",
     "shell.execute_reply": "2025-06-10T04:44:16.515204Z",
     "shell.execute_reply.started": "2025-06-10T04:19:50.628070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "# Số batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Khởi tạo EfficientNetB4 trong context của strategy\n",
    "with strategy.scope():\n",
    "    base_model_efficientnetb4 = EfficientNetB4(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Nơi lưu đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# Lặp qua từng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features with EfficientNetB4\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_efficientnetb4(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# Kết hợp đặc trưng và nhãn\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T04:44:54.798377Z",
     "iopub.status.busy": "2025-06-10T04:44:54.797794Z",
     "iopub.status.idle": "2025-06-10T04:44:56.485234Z",
     "shell.execute_reply": "2025-06-10T04:44:56.484648Z",
     "shell.execute_reply.started": "2025-06-10T04:44:54.798353Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã thêm 89853 vector đã chuẩn hóa vào FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chuẩn hóa L2 cho mỗi vector (độ dài = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # thêm epsilon để tránh chia cho 0\n",
    "\n",
    "# Chuẩn hóa vector đặc trưng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# Kích thước vector đặc trưng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Khởi tạo index FAISS sử dụng khoảng cách Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Thêm vector đã chuẩn hóa vào index\n",
    "index.add(normalized_features)\n",
    "print(\"Đã thêm\", index.ntotal, \"vector đã chuẩn hóa vào FAISS index.\")\n",
    "\n",
    "# Lưu index và nhãn\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:44:01.654138Z",
     "iopub.status.busy": "2025-05-10T13:44:01.653880Z",
     "iopub.status.idle": "2025-05-10T13:44:01.962370Z",
     "shell.execute_reply": "2025-05-10T13:44:01.961578Z",
     "shell.execute_reply.started": "2025-05-10T13:44:01.654121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Đọc lại FAISS index và nhãn\n",
    "index = faiss.read_index(\"faiss_features.index\")\n",
    "labels_array = np.load(\"faiss_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T04:45:16.658088Z",
     "iopub.status.busy": "2025-06-10T04:45:16.657296Z",
     "iopub.status.idle": "2025-06-10T05:51:30.282327Z",
     "shell.execute_reply": "2025-06-10T05:51:30.281744Z",
     "shell.execute_reply.started": "2025-06-10T04:45:16.658065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/processed-frames-224/Process_Frames_1/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.8  \n",
    "\n",
    "# ==== Load mô hình và FAISS index ====\n",
    "# Load mô hình EffcientNetB4\n",
    "model = EfficientNetB4(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn FAISS index và labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"❌ Không tìm thấy FAISS index tại: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"❌ Không tìm thấy nhãn tại: {label_path}\")\n",
    "\n",
    "# Load FAISS index và labels\n",
    "try:\n",
    "    # Load index trực tiếp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"✅ FAISS index đã được tải thành công!\")\n",
    "    print(f\"   - Số lượng vectors: {index.ntotal}\")\n",
    "    print(f\"   - Kích thước vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy tất cả ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # Lấy danh sách tất cả ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "processing_times = []  # Danh sách lưu thời gian xử lý từng ảnh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # Bắt đầu đo thời gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Trích đặc trưng với mô hình\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chuẩn hóa L2 đặc trưng mẫu truy vấn để phù hợp với vector đã chuẩn hóa trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Thêm epsilon để tránh chia cho 0\n",
    "        \n",
    "        # Đảm bảo đặc trưng có định dạng phù hợp sau khi chuẩn hóa\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # Tìm kiếm k=1 điểm gần nhất trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Kiểm tra similarity score từ khoảng cách L2 với vector đã chuẩn hóa\n",
    "        # Với vector đã chuẩn hóa L2, khoảng cách Euclidean bình phương (D) và cosine similarity có quan hệ:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Khoảng cách FAISS là bình phương khoảng cách Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuyển đổi khoảng cách thành cosine similarity\n",
    "        \n",
    "        # Nếu similarity dưới ngưỡng, gán nhãn \"Khác\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nhãn \"Khác\"\n",
    "        else:\n",
    "            # Lấy nhãn dự đoán từ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # Lưu kết quả dự đoán và nhãn thực tế\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # Kết thúc đo thời gian và lưu lại\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In thông tin cho debug (có thể bỏ comment nếu cần kiểm tra)\n",
    "        # print(f\"Ảnh: {img_path}, Similarity: {similarity_score:.4f}, Nhãn dự đoán: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # Tính thời gian xử lý trung bình cho một ảnh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n⏱️ Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    print(f\"⏱️ Tổng thời gian xử lý: {sum(processing_times):.2f} giây cho {len(processing_times)} ảnh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    \n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "    \n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InceptionV3 + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T05:53:31.071433Z",
     "iopub.status.busy": "2025-06-10T05:53:31.071190Z",
     "iopub.status.idle": "2025-06-10T06:07:19.730012Z",
     "shell.execute_reply": "2025-06-10T06:07:19.729384Z",
     "shell.execute_reply.started": "2025-06-10T05:53:31.071416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "# Giả sử biến train_df là tf.data.Dataset đã chuẩn hóa và batch đúng\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Khởi tạo InceptionV3 trong context của strategy\n",
    "with strategy.scope():\n",
    "    base_model_inceptionv3 = InceptionV3(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Trích đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features with InceptionV3\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_inceptionv3(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "features_array = np.concatenate(all_features, axis=0)   \n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T06:07:48.294634Z",
     "iopub.status.busy": "2025-06-10T06:07:48.294353Z",
     "iopub.status.idle": "2025-06-10T06:07:50.280586Z",
     "shell.execute_reply": "2025-06-10T06:07:50.279907Z",
     "shell.execute_reply.started": "2025-06-10T06:07:48.294613Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã thêm 89853 vector đã chuẩn hóa vào FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chuẩn hóa L2 cho mỗi vector (độ dài = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # thêm epsilon để tránh chia cho 0\n",
    "\n",
    "# Chuẩn hóa vector đặc trưng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# Kích thước vector đặc trưng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Khởi tạo index FAISS sử dụng khoảng cách Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Thêm vector đã chuẩn hóa vào index\n",
    "index.add(normalized_features)\n",
    "print(\"Đã thêm\", index.ntotal, \"vector đã chuẩn hóa vào FAISS index.\")\n",
    "\n",
    "# Lưu index và nhãn\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Đọc lại FAISS index và nhãn\n",
    "index = faiss.read_index(\"faiss_features.index\")\n",
    "labels_array = np.load(\"faiss_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T06:07:52.619080Z",
     "iopub.status.busy": "2025-06-10T06:07:52.618468Z",
     "iopub.status.idle": "2025-06-10T07:16:03.361295Z",
     "shell.execute_reply": "2025-06-10T07:16:03.360531Z",
     "shell.execute_reply.started": "2025-06-10T06:07:52.619059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/processed-frames-224/Process_Frames_1/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.8  \n",
    "\n",
    "# ==== Load mô hình và FAISS index ====\n",
    "# Load mô hình InceptionV3\n",
    "model = InceptionV3(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn FAISS index và labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"❌ Không tìm thấy FAISS index tại: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"❌ Không tìm thấy nhãn tại: {label_path}\")\n",
    "\n",
    "# Load FAISS index và labels\n",
    "try:\n",
    "    # Load index trực tiếp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"✅ FAISS index đã được tải thành công!\")\n",
    "    print(f\"   - Số lượng vectors: {index.ntotal}\")\n",
    "    print(f\"   - Kích thước vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy tất cả ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # Lấy danh sách tất cả ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "processing_times = []  # Danh sách lưu thời gian xử lý từng ảnh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # Bắt đầu đo thời gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Trích đặc trưng với mô hình\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chuẩn hóa L2 đặc trưng mẫu truy vấn để phù hợp với vector đã chuẩn hóa trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Thêm epsilon để tránh chia cho 0\n",
    "        \n",
    "        # Đảm bảo đặc trưng có định dạng phù hợp sau khi chuẩn hóa\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # Tìm kiếm k=1 điểm gần nhất trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Kiểm tra similarity score từ khoảng cách L2 với vector đã chuẩn hóa\n",
    "        # Với vector đã chuẩn hóa L2, khoảng cách Euclidean bình phương (D) và cosine similarity có quan hệ:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Khoảng cách FAISS là bình phương khoảng cách Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuyển đổi khoảng cách thành cosine similarity\n",
    "        \n",
    "        # Nếu similarity dưới ngưỡng, gán nhãn \"Khác\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nhãn \"Khác\"\n",
    "        else:\n",
    "            # Lấy nhãn dự đoán từ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # Lưu kết quả dự đoán và nhãn thực tế\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # Kết thúc đo thời gian và lưu lại\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In thông tin cho debug (có thể bỏ comment nếu cần kiểm tra)\n",
    "        # print(f\"Ảnh: {img_path}, Similarity: {similarity_score:.4f}, Nhãn dự đoán: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # Tính thời gian xử lý trung bình cho một ảnh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n⏱️ Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    print(f\"⏱️ Tổng thời gian xử lý: {sum(processing_times):.2f} giây cho {len(processing_times)} ảnh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    \n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "    \n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:17:33.253993Z",
     "iopub.status.busy": "2025-06-10T07:17:33.253738Z",
     "iopub.status.idle": "2025-06-10T07:21:54.594497Z",
     "shell.execute_reply": "2025-06-10T07:21:54.593740Z",
     "shell.execute_reply.started": "2025-06-10T07:17:33.253976Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 2808/2808 [04:18<00:00, 10.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Số batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    base_model_vgg16 = VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Nơi lưu đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_vgg16(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# Nối đặc trưng và nhãn\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:22:04.360201Z",
     "iopub.status.busy": "2025-06-10T07:22:04.359537Z",
     "iopub.status.idle": "2025-06-10T07:22:04.920202Z",
     "shell.execute_reply": "2025-06-10T07:22:04.919484Z",
     "shell.execute_reply.started": "2025-06-10T07:22:04.360170Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã thêm 89853 vector đã chuẩn hóa vào FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chuẩn hóa L2 cho mỗi vector (độ dài = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # thêm epsilon để tránh chia cho 0\n",
    "\n",
    "# Chuẩn hóa vector đặc trưng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# Kích thước vector đặc trưng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Khởi tạo index FAISS sử dụng khoảng cách Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Thêm vector đã chuẩn hóa vào index\n",
    "index.add(normalized_features)\n",
    "print(\"Đã thêm\", index.ntotal, \"vector đã chuẩn hóa vào FAISS index.\")\n",
    "\n",
    "# Lưu index và nhãn\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Đọc lại FAISS index và nhãn\n",
    "index = faiss.read_index(\"faiss_features.index\")\n",
    "labels_array = np.load(\"faiss_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:22:46.344974Z",
     "iopub.status.busy": "2025-06-10T07:22:46.344455Z",
     "iopub.status.idle": "2025-06-10T08:05:22.270051Z",
     "shell.execute_reply": "2025-06-10T08:05:22.269250Z",
     "shell.execute_reply.started": "2025-06-10T07:22:46.344951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/processed-frames-224/Process_Frames_1/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.8  \n",
    "\n",
    "# ==== Load mô hình và FAISS index ====\n",
    "# Load mô hình InceptionV3\n",
    "model = VGG16(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn FAISS index và labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"❌ Không tìm thấy FAISS index tại: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"❌ Không tìm thấy nhãn tại: {label_path}\")\n",
    "\n",
    "# Load FAISS index và labels\n",
    "try:\n",
    "    # Load index trực tiếp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"✅ FAISS index đã được tải thành công!\")\n",
    "    print(f\"   - Số lượng vectors: {index.ntotal}\")\n",
    "    print(f\"   - Kích thước vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy tất cả ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # Lấy danh sách tất cả ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "processing_times = []  # Danh sách lưu thời gian xử lý từng ảnh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # Bắt đầu đo thời gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Trích đặc trưng với mô hình\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chuẩn hóa L2 đặc trưng mẫu truy vấn để phù hợp với vector đã chuẩn hóa trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Thêm epsilon để tránh chia cho 0\n",
    "        \n",
    "        # Đảm bảo đặc trưng có định dạng phù hợp sau khi chuẩn hóa\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # Tìm kiếm k=1 điểm gần nhất trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Kiểm tra similarity score từ khoảng cách L2 với vector đã chuẩn hóa\n",
    "        # Với vector đã chuẩn hóa L2, khoảng cách Euclidean bình phương (D) và cosine similarity có quan hệ:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Khoảng cách FAISS là bình phương khoảng cách Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuyển đổi khoảng cách thành cosine similarity\n",
    "        \n",
    "        # Nếu similarity dưới ngưỡng, gán nhãn \"Khác\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nhãn \"Khác\"\n",
    "        else:\n",
    "            # Lấy nhãn dự đoán từ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # Lưu kết quả dự đoán và nhãn thực tế\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # Kết thúc đo thời gian và lưu lại\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In thông tin cho debug (có thể bỏ comment nếu cần kiểm tra)\n",
    "        # print(f\"Ảnh: {img_path}, Similarity: {similarity_score:.4f}, Nhãn dự đoán: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # Tính thời gian xử lý trung bình cho một ảnh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n⏱️ Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    print(f\"⏱️ Tổng thời gian xử lý: {sum(processing_times):.2f} giây cho {len(processing_times)} ảnh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    \n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "    \n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-14T07:58:55.623Z",
     "iopub.execute_input": "2025-06-14T07:44:07.483201Z",
     "iopub.status.busy": "2025-06-14T07:44:07.482889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import onnx\n",
    "\n",
    "# ----------- Cấu hình ----------\n",
    "image_size = 224\n",
    "n_clusters = 128\n",
    "train_path = '/kaggle/input/processed-frames-224/Process_Frames_1/Train'\n",
    "max_descriptors = 5000000\n",
    "random.seed(42)\n",
    "\n",
    "# Mapping labels\n",
    "classes = classes_train\n",
    "\n",
    "# ----------- Tiền xử lý ----------\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)\n",
    "    \n",
    "# Lọc nhiễu nhưng giữ cạnh\n",
    "def denoise_bilateral(img):\n",
    "    return cv2.bilateralFilter(img, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "# Làm nét ảnh\n",
    "def sharpen_image(img):\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5, -1],\n",
    "                       [0, -1, 0]])\n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "# Tăng tương phản bằng CLAHE\n",
    "def enhance_grayscale_contrast(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    return clahe.apply(gray)\n",
    "\n",
    "# Cân bằng trắng\n",
    "def white_balance(img):\n",
    "    result = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    avg_a = np.average(result[:, :, 1])\n",
    "    avg_b = np.average(result[:, :, 2])\n",
    "    result[:, :, 1] = result[:, :, 1] - ((avg_a - 128) * (result[:, :, 0] / 255.0) * 1.1)\n",
    "    result[:, :, 2] = result[:, :, 2] - ((avg_b - 128) * (result[:, :, 0] / 255.0) * 1.1)\n",
    "    return cv2.cvtColor(result, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# Hàm tiền xử lý tổng hợp\n",
    "def preprocess_image(img):\n",
    "    img = sharpen_image(img)\n",
    "    img = white_balance(img)\n",
    "    img = enhance_grayscale_contrast(img)\n",
    "    img = denoise_bilateral(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    return img\n",
    "\n",
    "num_images = sum(len(files) for _, _, files in os.walk(train_path) if files)\n",
    "print(f\"📸 Tổng số ảnh: {num_images}, tối đa descriptor dùng để huấn luyện: {max_descriptors}\")\n",
    "\n",
    "# ----------- Khởi tạo ----------\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# RootSIFT normalization\n",
    "def compute_rootsift(descriptors):\n",
    "    eps = 1e-7\n",
    "    descriptors = descriptors.astype(np.float32)\n",
    "    l1_norm = np.linalg.norm(descriptors, ord=1, axis=1, keepdims=True)\n",
    "    descriptors /= (l1_norm + eps)\n",
    "    descriptors = np.sqrt(descriptors)\n",
    "    return descriptors\n",
    "    \n",
    "descriptors_pool = []\n",
    "image_descriptors = []\n",
    "image_labels = []\n",
    "\n",
    "# ----------- Bước 1: Trích xuất descriptor ----------\n",
    "print(\"🧩 Trích xuất SIFT từ ảnh...\")\n",
    "for label_id, label_name in classes.items():\n",
    "    label_dir = os.path.join(train_path, label_name)\n",
    "    if not os.path.isdir(label_dir):\n",
    "        continue\n",
    "    for filename in tqdm(os.listdir(label_dir), desc=f\"Processing {label_name}\"):\n",
    "        img_path = os.path.join(label_dir, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "        img = preprocess_image(img) \n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "        if descriptors is not None and len(descriptors) > 0:\n",
    "            descriptors = compute_rootsift(descriptors)  \n",
    "            image_descriptors.append(descriptors)\n",
    "            image_labels.append(label_id - 1)\n",
    "        \n",
    "            sampled = descriptors\n",
    "            if len(descriptors_pool) < max_descriptors:\n",
    "                if len(descriptors) > 100:\n",
    "                    indices = np.random.choice(len(descriptors), 100, replace=False)\n",
    "                    sampled = descriptors[indices]\n",
    "                descriptors_pool.extend(sampled)\n",
    "\n",
    "# ----------- Bước 2: Huấn luyện MiniBatchKMeans ----------\n",
    "print(\"🏗️ Huấn luyện MiniBatchKMeans...\")\n",
    "descriptors_array = np.array(descriptors_pool)\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, n_init=10, \n",
    "                         batch_size=4096, max_iter=100, verbose=1,\n",
    "                        init_size=3 * n_clusters,\n",
    "                        tol=1e-3,\n",
    "                        max_no_improvement=10)\n",
    "kmeans.fit(descriptors_array)\n",
    "\n",
    "# ----------- Bước 3: Lưu mô hình ONNX ----------\n",
    "print(\"💾 Chuyển và lưu KMeans sang ONNX...\")\n",
    "initial_type = [('input', FloatTensorType([None, descriptors_array.shape[1]]))]\n",
    "onnx_model = convert_sklearn(kmeans, initial_types=initial_type)\n",
    "onnx.save_model(onnx_model, \"kmeans_model.onnx\")\n",
    "\n",
    "# ----------- Bước 4: Tạo histogram đặc trưng ----------\n",
    "print(\"📦 Chuyển descriptor thành vector đặc trưng...\")\n",
    "def extract_bow_histogram(descriptors, kmeans_model, n_clusters):\n",
    "    hist = np.zeros(n_clusters)\n",
    "    if descriptors is not None and len(descriptors) > 0:\n",
    "        clusters = kmeans_model.predict(descriptors)\n",
    "        for c in clusters:\n",
    "            hist[c] += 1\n",
    "        hist = normalize(hist.reshape(1, -1), norm='l2')[0]\n",
    "    return hist\n",
    "\n",
    "features_array = np.array([\n",
    "    extract_bow_histogram(des, kmeans, n_clusters)\n",
    "    for des in image_descriptors\n",
    "])\n",
    "\n",
    "labels_array = np.array(image_labels)\n",
    "\n",
    "# ----------- Kết quả ----------\n",
    "print(\"✅ Hoàn tất trích đặc trưng.\")\n",
    "print(\"features_array shape:\", features_array.shape)\n",
    "print(\"labels_array shape:\", labels_array.shape)\n",
    "print(\"📝 Mô hình KMeans đã được lưu ở: kmeans_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T14:05:36.063145Z",
     "iopub.status.busy": "2025-06-11T14:05:36.062679Z",
     "iopub.status.idle": "2025-06-11T14:05:36.378125Z",
     "shell.execute_reply": "2025-06-11T14:05:36.376586Z",
     "shell.execute_reply.started": "2025-06-11T14:05:36.063110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "print(\"🔍 Tích hợp với FAISS...\")\n",
    "\n",
    "# Chuyển đổi kiểu dữ liệu và chuẩn hóa L2\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# Kích thước vector đặc trưng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Khởi tạo index FAISS sử dụng khoảng cách Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Thêm vector đã chuẩn hóa vào index\n",
    "index.add(normalized_features)\n",
    "\n",
    "print(f\"✅ Đã thêm {index.ntotal} vector đã chuẩn hóa vào FAISS index.\")\n",
    "\n",
    "# Lưu index và nhãn\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)\n",
    "\n",
    "print(\"💾 Đã lưu FAISS index và labels:\")\n",
    "print(\"- faiss_features.index\")\n",
    "print(\"- faiss_labels.npy\")\n",
    "\n",
    "# ----------- Kết quả ----------\n",
    "print(\"✅ Hoàn tất trích đặc trưng và tích hợp FAISS.\")\n",
    "print(\"features_array shape:\", features_array.shape)\n",
    "print(\"labels_array shape:\", labels_array.shape)\n",
    "print(\"normalized_features shape:\", normalized_features.shape)\n",
    "print(\"FAISS index dimension:\", d)\n",
    "print(\"Total vectors in index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T14:05:55.718253Z",
     "iopub.status.busy": "2025-06-11T14:05:55.717872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Cài đặt các tham số\n",
    "image_size = 224\n",
    "n_clusters = 128\n",
    "test_path = \"/kaggle/input/processed-frames-224/Process_Frames_1/Test\"\n",
    "index_path = \"faiss_features.index\"\n",
    "label_path = \"faiss_labels.npy\"\n",
    "kmeans_path = \"kmeans_model.onnx\"  # Đường dẫn lưu mô hình KMeans ONNX\n",
    "confusion_output_path = \"confusion_matrix_sift_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_sift_faiss.csv\"\n",
    "similarity_threshold = 0.8\n",
    "\n",
    "# Chuẩn hóa L2 cho mỗi vector (độ dài = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # epsilon tránh chia cho 0\n",
    "\n",
    "# Tăng độ sáng\n",
    "def increase_brightness(img, value=30):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    v = np.clip(v + value, 0, 255).astype(np.uint8)\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    return cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "# Làm nét ảnh\n",
    "def sharpen_image(img):\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 4, -1],\n",
    "                       [0, -1, 0]])\n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "# Tăng tương phản bằng CLAHE\n",
    "def enhance_contrast(img):\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# Hàm tiền xử lý tổng hợp\n",
    "def preprocess_image(img):\n",
    "    # img = increase_brightness(img, value=30)\n",
    "    img = enhance_contrast(img)\n",
    "    img = sharpen_image(img)\n",
    "    return img\n",
    "\n",
    "# ==== Khởi tạo SIFT detector ====\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "def compute_rootsift(descriptors):\n",
    "    eps = 1e-7\n",
    "    descriptors = descriptors.astype(np.float32)\n",
    "    l1_norm = np.linalg.norm(descriptors, ord=1, axis=1, keepdims=True)\n",
    "    descriptors /= (l1_norm + eps)\n",
    "    descriptors = np.sqrt(descriptors)\n",
    "    return descriptors\n",
    "\n",
    "# Hàm trích xuất đặc trưng SIFT BOW cho một ảnh (dùng RootSIFT)\n",
    "def extract_sift_bow_features(img_path, sift_detector, kmeans_session, n_clusters):\n",
    "    \"\"\"Trích xuất đặc trưng RootSIFT BOW cho một ảnh\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "        img = preprocess_image(img)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        keypoints, descriptors = sift_detector.detectAndCompute(gray, None)\n",
    "        if descriptors is None or len(descriptors) == 0:\n",
    "            return np.zeros(n_clusters, dtype=np.float32)\n",
    "\n",
    "        # RootSIFT\n",
    "        descriptors = compute_rootsift(descriptors)\n",
    "\n",
    "        # Dự đoán cluster bằng KMeans ONNX\n",
    "        input_name = kmeans_session.get_inputs()[0].name\n",
    "        clusters = kmeans_session.run(None, {input_name: descriptors.astype(np.float32)})[0]\n",
    "\n",
    "        # Tính histogram\n",
    "        hist = np.zeros(n_clusters, dtype=np.float32)\n",
    "        for c in clusters:\n",
    "            hist[int(c)] += 1\n",
    "\n",
    "        # Chuẩn hóa L2\n",
    "        hist = normalize(hist.reshape(1, -1), norm='l2')[0]\n",
    "        return hist.astype(np.float32)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi trích xuất đặc trưng từ {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==== Load FAISS index, labels, và KMeans ONNX model ====\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"❌ Không tìm thấy FAISS index tại: {index_path}\")\n",
    "    exit()\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"❌ Không tìm thấy nhãn tại: {label_path}\")\n",
    "    exit()\n",
    "if not os.path.exists(kmeans_path):\n",
    "    print(f\"❌ Không tìm thấy KMeans ONNX model tại: {kmeans_path}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    # Load KMeans ONNX model\n",
    "    kmeans_session = ort.InferenceSession(kmeans_path)\n",
    "\n",
    "    print(f\"✅ FAISS index đã được tải thành công!\")\n",
    "    print(f\"   - Số lượng vectors: {index.ntotal}\")\n",
    "    print(f\"   - Kích thước vector: {index.d}\")\n",
    "    print(f\"✅ KMeans ONNX model đã được tải thành công!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải models: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy tất cả ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "print(f\"📸 Tổng số ảnh test: {len(all_images)}\")\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "processing_times = []\n",
    "\n",
    "for img_path in tqdm(all_images, desc=\"Testing with SIFT+FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "\n",
    "        feature = extract_sift_bow_features(img_path, sift, kmeans_session, n_clusters)\n",
    "        if feature is None:\n",
    "            print(f\"❌ Không thể trích xuất đặc trưng từ: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)\n",
    "        feature = feature.reshape(1, -1).astype(np.float32)\n",
    "\n",
    "        D, I = index.search(feature, 1)\n",
    "\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared / 2\n",
    "\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nhãn \"Khác\"\n",
    "        else:\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data) + 1\n",
    "\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "\n",
    "        end_time = time.time()\n",
    "        processing_times.append(end_time - start_time)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi xử lý ảnh {img_path}: {e}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    \n",
    "    print(f\"\\n✅ SIFT+FAISS Results:\")\n",
    "    print(f\"   📊 Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"   ✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # Tính thời gian xử lý trung bình cho một ảnh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n⏱️ Performance:\")\n",
    "    print(f\"   🚀 Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    print(f\"   ⏱️ Tổng thời gian xử lý: {sum(processing_times):.2f} giây cho {len(processing_times)} ảnh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=4, output_dict=True, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    # Hiển thị báo cáo\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"support\"]].round(4))\n",
    "    \n",
    "    # Các tham số đánh giá chung\n",
    "    macro_avg = report['macro avg']\n",
    "    weighted_avg = report['weighted avg']\n",
    "    \n",
    "    print(f\"\\n📝 Tổng kết đánh giá (SIFT+FAISS):\")\n",
    "    print(f\"   🎯 Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"   📊 Macro Precision: {macro_avg['precision']:.4f}\")\n",
    "    print(f\"   📊 Macro Recall: {macro_avg['recall']:.4f}\")\n",
    "    print(f\"   📊 Macro F1-score: {macro_avg['f1-score']:.4f}\")\n",
    "    print(f\"   📊 Weighted Precision: {weighted_avg['precision']:.4f}\")\n",
    "    print(f\"   📊 Weighted Recall: {weighted_avg['recall']:.4f}\")\n",
    "    print(f\"   📊 Weighted F1-score: {weighted_avg['f1-score']:.4f}\")\n",
    "    print(f\"   🚀 Avg Processing Time: {avg_processing_time:.4f} sec/image\")\n",
    "    \n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "    \n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_precision': macro_avg['precision'],\n",
    "        'macro_recall': macro_avg['recall'],\n",
    "        'macro_f1_score': macro_avg['f1-score'],\n",
    "        'weighted_precision': weighted_avg['precision'],\n",
    "        'weighted_recall': weighted_avg['recall'],\n",
    "        'weighted_f1_score': weighted_avg['f1-score'],\n",
    "        'avg_processing_time': avg_processing_time,\n",
    "        'total_images': len(processing_times),\n",
    "        'similarity_threshold': similarity_threshold,\n",
    "    }\n",
    "    \n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_sift_faiss.csv', index=False)\n",
    "    print(f\"📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_sift_faiss.csv'\")\n",
    "    \n",
    "    # Lưu báo cáo cuối cùng\n",
    "    final_df = report_df.copy()\n",
    "    for key, value in evaluation_metrics.items():\n",
    "        final_df[key] = value\n",
    "    \n",
    "    final_df.to_csv('final_classification_report_sift_faiss.csv', index=True)\n",
    "    print(f\"📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_sift_faiss.csv'\")\n",
    "    \n",
    "    print(f\"\\n🎉 Hoàn tất đánh giá với SIFT+FAISS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOG + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T12:21:20.856024Z",
     "iopub.status.busy": "2025-06-10T12:21:20.855735Z",
     "iopub.status.idle": "2025-06-10T12:51:06.737995Z",
     "shell.execute_reply": "2025-06-10T12:51:06.737128Z",
     "shell.execute_reply.started": "2025-06-10T12:21:20.856004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "# ----------- Cấu hình ----------\n",
    "image_size = 224\n",
    "train_path = '/kaggle/input/processed-frames-224/Process_Frames_1/Train'\n",
    "random.seed(42)\n",
    "\n",
    "classes = classes_train\n",
    "\n",
    "# ----------- Trích xuất HOG ----------\n",
    "print(\"📸 Trích xuất HOG từ ảnh...\")\n",
    "hog_features = []\n",
    "hog_labels = []\n",
    "\n",
    "for label_id, label_name in classes.items():\n",
    "    label_dir = os.path.join(train_path, label_name)\n",
    "    if not os.path.isdir(label_dir):\n",
    "        continue\n",
    "    for filename in tqdm(os.listdir(label_dir), desc=f\"Processing {label_name}\"):\n",
    "        img_path = os.path.join(label_dir, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img_resized = resize(img, (image_size, image_size), anti_aliasing=True)\n",
    "        gray = rgb2gray(img_resized)\n",
    "\n",
    "        # Trích HOG\n",
    "        features = hog(\n",
    "            gray,\n",
    "            orientations=9,                \n",
    "            pixels_per_cell=(24, 24),       \n",
    "            cells_per_block=(3, 3),\n",
    "            block_norm='L2-Hys',\n",
    "            feature_vector=True\n",
    "        )\n",
    "        hog_features.append(features)\n",
    "        hog_labels.append(label_id - 1)\n",
    "\n",
    "hog_features = np.array(hog_features)\n",
    "hog_labels = np.array(hog_labels)\n",
    "print(\"✅ Trích xuất HOG hoàn tất.\")\n",
    "print(\"HOG feature shape:\", hog_features.shape)\n",
    "print(\"Labels shape:\", hog_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T12:53:16.329732Z",
     "iopub.status.busy": "2025-06-10T12:53:16.329355Z",
     "iopub.status.idle": "2025-06-10T12:53:20.069749Z",
     "shell.execute_reply": "2025-06-10T12:53:20.069089Z",
     "shell.execute_reply.started": "2025-06-10T12:53:16.329710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "print(\"🔍 Tích hợp với FAISS...\")\n",
    "\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  \n",
    "\n",
    "# Chuyển đổi kiểu dữ liệu và chuẩn hóa L2\n",
    "normalized_features = l2_normalize(hog_features.astype('float32'))\n",
    "\n",
    "# Kích thước vector đặc trưng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Khởi tạo index FAISS sử dụng khoảng cách Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Thêm vector đã chuẩn hóa vào index\n",
    "index.add(normalized_features)\n",
    "\n",
    "print(f\"✅ Đã thêm {index.ntotal} vector đã chuẩn hóa vào FAISS index.\")\n",
    "\n",
    "# Lưu index và nhãn\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", hog_labels)\n",
    "\n",
    "print(\"💾 Đã lưu FAISS index và labels:\")\n",
    "print(\"- faiss_features.index\")\n",
    "print(\"- faiss_labels.npy\")\n",
    "\n",
    "# ----------- Kết quả ----------\n",
    "print(\"✅ Hoàn tất trích đặc trưng và tích hợp FAISS.\")\n",
    "print(\"features_array shape:\", hog_features.shape)\n",
    "print(\"labels_array shape:\", hog_labels.shape)\n",
    "print(\"normalized_features shape:\", normalized_features.shape)\n",
    "print(\"FAISS index dimension:\", d)\n",
    "print(\"Total vectors in index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T12:53:47.351558Z",
     "iopub.status.busy": "2025-06-10T12:53:47.351248Z",
     "iopub.status.idle": "2025-06-10T13:57:34.960848Z",
     "shell.execute_reply": "2025-06-10T13:57:34.960106Z",
     "shell.execute_reply.started": "2025-06-10T12:53:47.351538Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import normalize\n",
    "from skimage.feature import hog\n",
    "from skimage import color\n",
    "import traceback\n",
    "\n",
    "# Cài đặt các tham số\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/processed-frames-224/Process_Frames_1/Test\"\n",
    "index_path = \"faiss_features.index\"\n",
    "label_path = \"faiss_labels.npy\"\n",
    "confusion_output_path = \"confusion_matrix_hog_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_hog_faiss.csv\"\n",
    "similarity_threshold = 0.8\n",
    "\n",
    "# Chuẩn hóa L2 cho mỗi vector (độ dài = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # epsilon tránh chia cho 0\n",
    "\n",
    "# Hàm trích xuất đặc trưng HOG BOW cho một ảnh\n",
    "def extract_hog_features(img_path):\n",
    "    \"\"\"Trích xuất vector HOG trực tiếp từ ảnh\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "        gray = color.rgb2gray(img)\n",
    "\n",
    "        # Trích xuất đặc trưng HOG (vector 1 chiều)\n",
    "        hog_vector = hog(gray,\n",
    "                        orientations=9,                \n",
    "                        pixels_per_cell=(24, 24),       \n",
    "                        cells_per_block=(3, 3),\n",
    "                        block_norm='L2-Hys',\n",
    "                        feature_vector=True)\n",
    "\n",
    "        if hog_vector is None or len(hog_vector) == 0:\n",
    "            return None\n",
    "\n",
    "        return hog_vector.astype(np.float32)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi trích xuất HOG từ {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==== Load FAISS index, labels ====\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"❌ Không tìm thấy FAISS index tại: {index_path}\")\n",
    "    exit()\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"❌ Không tìm thấy nhãn tại: {label_path}\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"✅ FAISS index đã được tải thành công!\")\n",
    "    print(f\"   - Số lượng vectors: {index.ntotal}\")\n",
    "    print(f\"   - Kích thước vector: {index.d}\")\n",
    "\n",
    "except Exception as e:\n",
    "    exit()\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy tất cả ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "print(f\"📸 Tổng số ảnh test: {len(all_images)}\")\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "processing_times = []\n",
    "\n",
    "for img_path in tqdm(all_images, desc=\"Testing with HOG+FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "\n",
    "        # Trích xuất vector HOG trực tiếp\n",
    "        feature = extract_hog_features(img_path)\n",
    "\n",
    "        if feature is None:\n",
    "            print(f\"❌ Không thể trích xuất đặc trưng từ: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Kiểm tra kích thước vector\n",
    "        if feature.shape[0] != index.d:\n",
    "            print(f\"⚠️ Vector không đúng kích thước ({feature.shape[0]} vs {index.d}) tại: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Chuẩn hóa L2\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)\n",
    "        feature = feature.reshape(1, -1).astype(np.float32)\n",
    "\n",
    "        # Tìm ảnh gần nhất trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared / 2\n",
    "\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # \"Khác\"\n",
    "        else:\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data) + 1\n",
    "\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "\n",
    "        end_time = time.time()\n",
    "        processing_times.append(end_time - start_time)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi xử lý ảnh {img_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    \n",
    "    print(f\"\\n✅ HOG+FAISS Results:\")\n",
    "    print(f\"   📊 Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"   ✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # Tính thời gian xử lý trung bình cho một ảnh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n⏱️ Performance:\")\n",
    "    print(f\"   🚀 Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    print(f\"   ⏱️ Tổng thời gian xử lý: {sum(processing_times):.2f} giây cho {len(processing_times)} ảnh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=4, output_dict=True, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    # Hiển thị báo cáo\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"support\"]].round(4))\n",
    "    \n",
    "    # Các tham số đánh giá chung\n",
    "    macro_avg = report['macro avg']\n",
    "    weighted_avg = report['weighted avg']\n",
    "    \n",
    "    print(f\"\\n📝 Tổng kết đánh giá (HOG+FAISS):\")\n",
    "    print(f\"   🎯 Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"   📊 Macro Precision: {macro_avg['precision']:.4f}\")\n",
    "    print(f\"   📊 Macro Recall: {macro_avg['recall']:.4f}\")\n",
    "    print(f\"   📊 Macro F1-score: {macro_avg['f1-score']:.4f}\")\n",
    "    print(f\"   📊 Weighted Precision: {weighted_avg['precision']:.4f}\")\n",
    "    print(f\"   📊 Weighted Recall: {weighted_avg['recall']:.4f}\")\n",
    "    print(f\"   📊 Weighted F1-score: {weighted_avg['f1-score']:.4f}\")\n",
    "    print(f\"   🚀 Avg Processing Time: {avg_processing_time:.4f} sec/image\")\n",
    "    \n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "    \n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_precision': macro_avg['precision'],\n",
    "        'macro_recall': macro_avg['recall'],\n",
    "        'macro_f1_score': macro_avg['f1-score'],\n",
    "        'weighted_precision': weighted_avg['precision'],\n",
    "        'weighted_recall': weighted_avg['recall'],\n",
    "        'weighted_f1_score': weighted_avg['f1-score'],\n",
    "        'avg_processing_time': avg_processing_time,\n",
    "        'total_images': len(processing_times),\n",
    "        'similarity_threshold': similarity_threshold,\n",
    "    }\n",
    "    \n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_hog_faiss.csv', index=False)\n",
    "    print(f\"📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_hog_faiss.csv'\")\n",
    "    \n",
    "    # Lưu báo cáo cuối cùng\n",
    "    final_df = report_df.copy()\n",
    "    for key, value in evaluation_metrics.items():\n",
    "        final_df[key] = value\n",
    "    \n",
    "    final_df.to_csv('final_classification_report_hog_faiss.csv', index=True)\n",
    "    print(f\"📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_hog_faiss.csv'\")\n",
    "    \n",
    "    print(f\"\\n🎉 Hoàn tất đánh giá với HOG+FAISS!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7622968,
     "sourceId": 12107806,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
