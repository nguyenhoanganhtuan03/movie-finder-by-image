import os
from pymongo import MongoClient
from sentence_transformers import SentenceTransformer

from langchain_community.vectorstores import FAISS
from langchain.embeddings.base import Embeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_experimental.text_splitter import SemanticChunker
from langchain.docstore.document import Document

# ========== C·∫§U H√åNH ==========
VECTOR_DIR = "vector_db/movie_vector_db"
EMBEDDING_MODEL_NAME = "AITeamVN/Vietnamese_Embedding"
DB_NAME = "movie_database"
COLLECTION_NAME = "movies"
MONGO_URI = "mongodb://localhost:27017/"

# ========== KH·ªûI T·∫†O ==========
os.makedirs(VECTOR_DIR, exist_ok=True)
client = MongoClient(MONGO_URI)
collection = client[DB_NAME][COLLECTION_NAME]
model = SentenceTransformer(EMBEDDING_MODEL_NAME)
model.max_seq_length = 2048

# ========== WRAPPER EMBEDDING CHO LANGCHAIN ==========
class SentenceTransformerEmbeddingWrapper(Embeddings):
    def __init__(self, model):
        self.model = model

    def embed_documents(self, texts):
        embeddings = self.model.encode(texts, convert_to_numpy=True)
        return embeddings.tolist()

    def embed_query(self, text):
        embedding = self.model.encode([text], convert_to_numpy=True)[0]
        return embedding.tolist()

embedding_wrapper = SentenceTransformerEmbeddingWrapper(model)

# ========== T·∫†O VECTOR DB ==========
def create_db():
    prompts = []

    for doc in collection.find():
        try:
            name = doc.get("name", "").strip()
            if not name:
                continue

            genres = ', '.join(doc.get("genre", []))
            duration = str(doc.get("duration", "")).strip()
            director = doc.get("director", "").strip()
            actors = ', '.join(doc.get("actor", []))
            year = str(doc.get("year_of_release", "")).strip()
            description = doc.get("describe", "").strip()

            # Prompt m√¥ t·∫£ th√¥ng tin chung
            info_prompt = (
                f"{name} l√† m·ªôt b·ªô phim th·ªÉ lo·∫°i {genres}, k√©o d√†i {duration} ph√∫t, "
                f"ƒë∆∞·ª£c ƒë·∫°o di·ªÖn b·ªüi {director}, v·ªõi s·ª± tham gia c·ªßa {actors}. "
                f"Phim ra m·∫Øt nƒÉm {year}. N·ªôi dung phim: {description}"
            ).strip()
            prompts.append(info_prompt)

        except Exception as e:
            print(f"L·ªói khi x·ª≠ l√Ω phim '{doc.get('name', 'Unknown')}': {e}")

    if not prompts:
        print("‚ùå Kh√¥ng c√≥ prompt n√†o ƒë·ªÉ t·∫°o index.")
        return

    # T·∫°o chunks t·ª´ c√°c prompt
    text_splitter = SemanticChunker(embeddings=embedding_wrapper,
                                    buffer_size=1,
                                    breakpoint_threshold_type="percentile",
                                    breakpoint_threshold_amount=95,
                                    min_chunk_size=500,
                                    add_start_index=True)
    
    docs = [Document(page_content=p) for p in prompts]
    chunks = text_splitter.split_documents(docs)

    # T·∫°o FAISS index LangChain v·ªõi chunks
    vectorstore = FAISS.from_documents(chunks, embedding=embedding_wrapper)
    vectorstore.save_local(VECTOR_DIR)

    print(f"‚úÖ ƒê√£ l∆∞u FAISS index t·∫°i th∆∞ m·ª•c: {VECTOR_DIR}")
    print(f"üé¨ T·ªïng s·ªë prompt ƒë√£ t·∫°o: {len(prompts)}")

# ========== CH·∫†Y CH√çNH ==========
create_db()
