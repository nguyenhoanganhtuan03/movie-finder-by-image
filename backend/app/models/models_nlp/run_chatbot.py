import os
from sentence_transformers import SentenceTransformer
from pyvi.ViTokenizer import tokenize

from langchain_community.vectorstores import FAISS
from langchain.embeddings.base import Embeddings
from langchain_community.llms import CTransformers
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# ==== C·∫§U H√åNH ====
VECTOR_DIR = "vector_db/movie_vector_db"
MODELS_LLM_DIR = "models_llm"
EMBEDDING_MODEL = "VoVanPhuc/sup-SimCSE-VietNamese-phobert-base"
LLM_MODEL_PATH = os.path.join(MODELS_LLM_DIR, "vinallama-7b-chat_q5_0.gguf")

# ==== LOAD MODEL EMBEDDING & LLM ====
model = SentenceTransformer(EMBEDDING_MODEL)

llm = CTransformers(
    model=LLM_MODEL_PATH,
    model_type="llama",
    config={"max_new_tokens": 1024, "temperature": 0.01}
)

# ========== WRAPPER EMBEDDING CHO LANGCHAIN ==========
class SentenceTransformerEmbeddingWrapper(Embeddings):
    def __init__(self, model):
        self.model = model

    def embed_documents(self, texts):
        tokenized = [tokenize(t.strip().lower()) for t in texts]
        embeddings = self.model.encode(tokenized, convert_to_numpy=True)
        return embeddings.tolist()

    def embed_query(self, text):
        tokenized = tokenize(text.strip().lower())
        embedding = self.model.encode([tokenized], convert_to_numpy=True)[0]
        return embedding.tolist()

embedding_wrapper = SentenceTransformerEmbeddingWrapper(model)

# ==== PROMPT TEMPLATE V·ªöI 2 BI·∫æN: query v√† context ====
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
        B·∫°n l√† tr·ª£ l√Ω AI. H√£y tr·∫£ l·ªùi ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c v√† c√≥ th·ªÉ tin c·∫≠y. 
        Ch·ªâ tr·∫£ l·ªùi 1 l·∫ßn duy nh·∫•t. Kh√¥ng d√†i d√≤ng.
        N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt.

        Th√¥ng tin li√™n quan:
        {context}

        C√¢u h·ªèi: {question}
        """
)

# ==== T·∫†O CHAIN TR·∫¢ L·ªúI D·ª∞A TR√äN VECTORSTORE ====
def create_qa_chain(llm, db):
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt_template}
    )
    return qa_chain

# ==== ƒê·ªåC VECTORSTORE FAISS ====
def read_vectors_db():
    if not os.path.exists(VECTOR_DIR):
        raise FileNotFoundError(f"Th∆∞ m·ª•c vector DB kh√¥ng t·ªìn t·∫°i: {VECTOR_DIR}")
    db = FAISS.load_local(VECTOR_DIR, embedding_wrapper, allow_dangerous_deserialization=True)
    return db

# ==== MAIN ====
if __name__ == "__main__":
    try:
        db = read_vectors_db()
    except Exception as e:
        print(f"‚ùå L·ªói khi load vector DB: {e}")
        exit(1)

    llm_chain = create_qa_chain(llm, db)

    while True:
        question = input("Nh·∫≠p c√¢u h·ªèi (g√µ 'quit' ƒë·ªÉ tho√°t): ").strip()
        if question.lower() == "quit":
            print("üëã K·∫øt th√∫c ch∆∞∆°ng tr√¨nh.")
            break

        # D√πng invoke v·ªõi dict c√≥ key l√† bi·∫øn query
        response = llm_chain.invoke({"query": question})
        print("\nü§ñ Tr·∫£ l·ªùi:\n", response['result'])
        print("-" * 50)
