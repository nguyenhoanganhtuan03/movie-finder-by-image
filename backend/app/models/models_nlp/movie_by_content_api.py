import os
import requests
import csv
import numpy as np
from collections import Counter, defaultdict
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
import faiss

# ==== C·∫§U H√åNH ====
load_dotenv()
base_dir = os.path.dirname(os.path.abspath(__file__))
INDEX_PATH = os.path.join(base_dir, "vector_db/index_movie.faiss")
LABELS_MAPPING_PATH = os.path.join(base_dir, "vector_db/labels_mapping.csv")
# SIMILARITY_THRESHOLD = 0.8
EMBEDDING_MODEL = "AITeamVN/Vietnamese_Embedding"
METADATA_PATH = os.path.join(base_dir, "vector_db/metadata.csv")

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY_2")
GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"

# ==== LOAD MODEL EMBEDDING v√† FAISS ====
model = SentenceTransformer(EMBEDDING_MODEL)
model.max_seq_length = 2048
index = faiss.read_index(INDEX_PATH)

# ========== H√ÄM TI·ªÜN √çCH ==========
def l2_normalize(vectors):
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    return vectors / (norms + 1e-10)

def embed_text(text):
    text = text.strip().lower()
    return model.encode([text], convert_to_numpy=True)

def load_metadata(path):
    metadata = {}
    if os.path.exists(path):
        with open(path, encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                idx = int(row['index'])
                name = row.get('name', f"Phim c√≥ ID {idx}")
                metadata[idx] = name
    return metadata

metadata_mapping = load_metadata(METADATA_PATH)

def load_labels_mapping(path):
    mapping = {}
    with open(path, encoding='utf-8') as f:
        next(f)  # B·ªè d√≤ng ti√™u ƒë·ªÅ
        for line in f:
            index, name = line.strip().split(',', 1)
            mapping[int(index)] = name
    return mapping

labels_mapping = load_labels_mapping(LABELS_MAPPING_PATH)

# ========== H√ÄM G·ªåI GEMINI API ==========
def call_gemini_api(prompt, api_key=GEMINI_API_KEY):
    """
    G·ªçi Gemini API ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi
    """
    headers = {
        "Content-Type": "application/json"
    }

    data = {
        "contents": [
            {
                "parts": [
                    {"text": prompt}
                ]
            }
        ],
        "generationConfig": {
            "temperature": 0.01,
            "topK": 40,
            "topP": 0.95,
            "maxOutputTokens": 1024,
        }
    }

    try:
        response = requests.post(
            f"{GEMINI_API_URL}?key={api_key}",
            headers=headers,
            json=data,
            timeout=30
        )

        if response.status_code == 200:
            result = response.json()
            if "candidates" in result and len(result["candidates"]) > 0:
                content = result["candidates"][0]["content"]["parts"][0]["text"]
                return content.strip()
            else:
                return "‚ùå Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ Gemini API."
        else:
            return f"‚ùå L·ªói API Gemini: {response.status_code} - {response.text}"

    except requests.exceptions.Timeout:
        return "‚ùå Timeout khi g·ªçi Gemini API."
    except requests.exceptions.RequestException as e:
        return f"‚ùå L·ªói k·∫øt n·ªëi: {str(e)}"
    except Exception as e:
        return f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}"


# ========== PH√ÇN T√çCH KEYWORD V√Ä T·∫†O PROMPT ==========
def create_keyword_analysis_prompt(user_query):
    """
    T·∫°o prompt ƒë·ªÉ Gemini ph√¢n t√≠ch keyword v√† tr√≠ch xu·∫•t th√¥ng tin phim
    """
    prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch th√¥ng tin phim ·∫£nh. Nhi·ªám v·ª• c·ªßa b·∫°n l√†:

1. PH√ÇN T√çCH c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ t√¨m t√™n phim
2. TR·∫¢ V·ªÄ k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng y√™u c·∫ßu

C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG: "{user_query}"

NHI·ªÜM V·ª§:
- T·ª´ c√¢u h·ªèi, h√£y x√°c ƒë·ªãnh c√°c keywords ch√≠nh trong c√¢u h·ªèi
- Tr√≠ch xu·∫•t c√°c th√¥ng tin: th·ªÉ lo·∫°i, th·ªùi l∆∞·ª£ng, ƒë·∫°o di·ªÖn, di·ªÖn vi√™n, nƒÉm ra m·∫Øt, n·ªôi dung

Y√äU C·∫¶U ƒê·ªäNH D·∫†NG TR·∫¢ L·ªúI: c√°c t·ª´ kh√≥a ch√≠nh, ngƒÉn c√°ch b·∫±ng d·∫•u ph·∫©y

L∆∞U √ù:
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong c√¢u h·ªèi ng∆∞·ªùi d√πng.
V√≠ d·ª•:
    - C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG: phim kinh d·ªã, h√†i, c√≥ Tr·∫•n Th√†nh ƒë√≥ng, l·∫•y b·ªëi c·∫£nh b·ªánh vi·ªán
    - TR·∫¢ L·ªúI: kinh d·ªã, h√†i, Tr·∫•n Th√†nh, b·ªánh vi·ªán

TR·∫¢ L·ªúI:"""

    return call_gemini_api(prompt)

# ========== H√ÄM PH√ÇN T√çCH C√ÇU H·ªéI ==========
def search_movies_by_user_query(user_query, SIMILARITY_THRESHOLD, n_movies):
    search_prompt = create_keyword_analysis_prompt(user_query)
    keywords = [kw.strip().lower() for kw in search_prompt.split(",") if kw.strip()]

    movie_stats = defaultdict(lambda: {"count": 0, "min_distance": float("inf")})

    for keyword in keywords:
        query_vec = l2_normalize(embed_text(keyword).astype('float32')).reshape(1, -1)
        distances, indices = index.search(query_vec, n_movies*2)

        for dist, idx in zip(distances[0], indices[0]):
            similarity = 1 - dist / 2
            if similarity >= SIMILARITY_THRESHOLD:
                movie_name = labels_mapping.get(idx, f"Phim c√≥ ID {idx}")
                movie_stats[movie_name]["count"] += 1
                movie_stats[movie_name]["min_distance"] = min(movie_stats[movie_name]["min_distance"], dist)

    sorted_movies = sorted(
        movie_stats.items(),
        key=lambda x: (-x[1]["count"], x[1]["min_distance"])
    )

    # T√°ch phim xu·∫•t hi·ªán nhi·ªÅu nh·∫•t (∆∞u ti√™n vector g·∫ßn h∆°n khi b·∫±ng nhau)
    top_n_movies = [(name, stats["count"]) for name, stats in sorted_movies[:n_movies]]

    # T·∫•t c·∫£ phim ph√π h·ª£p 
    all_matched_movies = [(name, stats["count"]) for name, stats in sorted_movies]

    print(f"T·ª´ kh√≥a: {search_prompt}")
    print(all_matched_movies)

    return search_prompt, top_n_movies, all_matched_movies

# ========== MAIN ==========
while True:
    user_input = input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n (ho·∫∑c 'quit' ƒë·ªÉ tho√°t): ").strip()
    if user_input.lower() == "quit":
        print("üëã Tho√°t ch∆∞∆°ng tr√¨nh.")
        break

    prompt, top_movies, all_matched_movies = search_movies_by_user_query(user_input, 0.8, 5)
    name_movies = [name for name, _ in top_movies]
    print(name_movies)

    print("\nüß† Prompt d√πng ƒë·ªÉ truy v·∫•n:", prompt)
    if top_movies:
        print("üé¨ K·∫øt qu·∫£ t√¨m ƒë∆∞·ª£c:")
        for name, score in top_movies:
            print(f"- {name}")
    else:
        print("‚ùå Kh√¥ng t√¨m th·∫•y phim ph√π h·ª£p.\n")
