import os
import requests
import csv
import numpy as np
from collections import Counter
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
import faiss

# ==== C·∫§U H√åNH ====
load_dotenv()
base_dir = os.path.dirname(os.path.abspath(__file__))
INDEX_PATH = os.path.join(base_dir, "vector_db/index_movie.faiss")
LABELS_MAPPING_PATH = os.path.join(base_dir, "vector_db/labels_mapping.csv")
SIMILARITY_THRESHOLD = 0.5
EMBEDDING_MODEL = "AITeamVN/Vietnamese_Embedding"
METADATA_PATH = os.path.join(base_dir, "vector_db/metadata.csv")

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY_2")
GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"

# ==== LOAD MODEL EMBEDDING v√† FAISS ====
model = SentenceTransformer(EMBEDDING_MODEL)
model.max_seq_length = 2048
index = faiss.read_index(INDEX_PATH)

# ========== H√ÄM TI·ªÜN √çCH ==========
def l2_normalize(vectors):
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    return vectors / (norms + 1e-10)

def embed_text(text):
    text = text.strip().lower()
    return model.encode([text], convert_to_numpy=True)

def load_metadata(path):
    metadata = {}
    if os.path.exists(path):
        with open(path, encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                idx = int(row['index'])
                name = row.get('name', f"Phim c√≥ ID {idx}")
                metadata[idx] = name
    return metadata

metadata_mapping = load_metadata(METADATA_PATH)

def load_labels_mapping(path):
    mapping = {}
    with open(path, encoding='utf-8') as f:
        next(f)  # B·ªè d√≤ng ti√™u ƒë·ªÅ
        for line in f:
            index, name = line.strip().split(',', 1)
            mapping[int(index)] = name
    return mapping

labels_mapping = load_labels_mapping(LABELS_MAPPING_PATH)

# ========== H√ÄM G·ªåI GEMINI API ==========
def call_gemini_api(prompt, api_key=GEMINI_API_KEY):
    """
    G·ªçi Gemini API ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi
    """
    headers = {
        "Content-Type": "application/json"
    }

    data = {
        "contents": [
            {
                "parts": [
                    {"text": prompt}
                ]
            }
        ],
        "generationConfig": {
            "temperature": 0.01,
            "topK": 40,
            "topP": 0.95,
            "maxOutputTokens": 1024,
        }
    }

    try:
        response = requests.post(
            f"{GEMINI_API_URL}?key={api_key}",
            headers=headers,
            json=data,
            timeout=30
        )

        if response.status_code == 200:
            result = response.json()
            if "candidates" in result and len(result["candidates"]) > 0:
                content = result["candidates"][0]["content"]["parts"][0]["text"]
                return content.strip()
            else:
                return "‚ùå Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ Gemini API."
        else:
            return f"‚ùå L·ªói API Gemini: {response.status_code} - {response.text}"

    except requests.exceptions.Timeout:
        return "‚ùå Timeout khi g·ªçi Gemini API."
    except requests.exceptions.RequestException as e:
        return f"‚ùå L·ªói k·∫øt n·ªëi: {str(e)}"
    except Exception as e:
        return f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}"


# ========== PH√ÇN T√çCH KEYWORD V√Ä T·∫†O PROMPT ==========
def create_keyword_analysis_prompt(user_query):
    """
    T·∫°o prompt ƒë·ªÉ Gemini ph√¢n t√≠ch keyword v√† tr√≠ch xu·∫•t th√¥ng tin phim
    """
    prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch th√¥ng tin phim ·∫£nh. Nhi·ªám v·ª• c·ªßa b·∫°n l√†:

1. PH√ÇN T√çCH c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ t√¨m t√™n phim
2. T√åM KI·∫æM th√¥ng tin phim t·ª´ d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p
3. TR√çCH XU·∫§T c√°c th√¥ng tin ch√≠nh c·ªßa phim
4. TR·∫¢ V·ªÄ k·∫øt qu·∫£ theo ƒë·ªãnh d·∫°ng y√™u c·∫ßu

C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG: "{user_query}"

NHI·ªÜM V·ª§:
- T·ª´ c√¢u h·ªèi, h√£y x√°c ƒë·ªãnh c√°c keywords ch√≠nh trong c√¢u h·ªèi
- Tr√≠ch xu·∫•t c√°c th√¥ng tin: th·ªÉ lo·∫°i, th·ªùi l∆∞·ª£ng, ƒë·∫°o di·ªÖn, di·ªÖn vi√™n, nƒÉm ra m·∫Øt, n·ªôi dung

Y√äU C·∫¶U ƒê·ªäNH D·∫†NG TR·∫¢ L·ªúI: c√°c t·ª´ kh√≥a ch√≠nh, ngƒÉn c√°ch b·∫±ng d·∫•u ph·∫©y

L∆∞U √ù:
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong c√¢u h·ªèi ng∆∞·ªùi d√πng.
V√≠ d·ª•:
    - C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG: phim kinh d·ªã c√≥ Vi·ªát H∆∞∆°ng ƒë√≥ng
    - TR·∫¢ L·ªúI: kinh d·ªã, Vi·ªát H∆∞∆°ng

TR·∫¢ L·ªúI:"""

    return call_gemini_api(prompt)

# ========== H√ÄM PH√ÇN T√çCH C√ÇU H·ªéI ==========
def search_movies_by_user_query(user_query, top_k=5):
    # 1. T·∫°o prompt truy v·∫•n (d·∫°ng: "h√†nh ƒë·ªông, v√µ thu·∫≠t, h·ªìi h·ªôp")
    search_prompt = create_keyword_analysis_prompt(user_query)

    # 2. T√°ch t·ª´ kh√≥a theo d·∫•u ph·∫©y
    keywords = [kw.strip().lower() for kw in search_prompt.split(",") if kw.strip()]

    # 3. Truy v·∫•n FAISS cho t·ª´ng t·ª´ kh√≥a
    matched_names = []

    for keyword in keywords:
        query_vec = l2_normalize(embed_text(keyword).astype('float32')).reshape(1, -1)
        distances, indices = index.search(query_vec, top_k)

        for dist, idx in zip(distances[0], indices[0]):
            similarity = 1 - dist / 2
            if similarity >= SIMILARITY_THRESHOLD:
                movie_name = labels_mapping.get(idx, f"Phim c√≥ ID {idx}")
                matched_names.append(movie_name)

    # 4. ƒê·∫øm t·∫ßn su·∫•t t√™n phim xu·∫•t hi·ªán
    movie_counts = Counter(matched_names)
    top_movies = movie_counts.most_common(top_k)

    return search_prompt, top_movies

# ========== MAIN ==========
# while True:
#     user_input = input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n (ho·∫∑c 'quit' ƒë·ªÉ tho√°t): ").strip()
#     if user_input.lower() == "quit":
#         print("üëã Tho√°t ch∆∞∆°ng tr√¨nh.")
#         break

#     prompt, top_movies = search_movies_by_user_query(user_input, top_k=8)
#     name_movies = [name for name, _ in top_movies]
#     print(name_movies)

#     print("\nüß† Prompt d√πng ƒë·ªÉ truy v·∫•n:", prompt)
#     if top_movies:
#         print("üé¨ K·∫øt qu·∫£ t√¨m ƒë∆∞·ª£c:")
#         for name, score in top_movies:
#             print(f"- {name}")
#     else:
#         print("‚ùå Kh√¥ng t√¨m th·∫•y phim ph√π h·ª£p.\n")
