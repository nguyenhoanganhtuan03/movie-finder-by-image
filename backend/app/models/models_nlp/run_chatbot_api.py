import os
import requests
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
import re
from langchain_community.vectorstores import FAISS
from langchain.embeddings.base import Embeddings


# ==== Cáº¤U HÃŒNH ====
load_dotenv()
base_dir = os.path.dirname(os.path.abspath(__file__))
MOVIE_VECTOR_DB = os.path.join(base_dir, "vector_db/movie_vector_db")
EMBEDDING_MODEL = "AITeamVN/Vietnamese_Embedding"
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"

# ==== LOAD MODEL EMBEDDING ====
model = SentenceTransformer(EMBEDDING_MODEL)
model.max_seq_length = 2048

# ==== WRAPPER EMBEDDING CHO LANGCHAIN ====
class SentenceTransformerEmbeddingWrapper(Embeddings):
    def __init__(self, model):
        self.model = model

    def embed_documents(self, texts):
        embeddings = self.model.encode(texts, convert_to_numpy=True)
        return embeddings.tolist()

    def embed_query(self, text):
        embedding = self.model.encode([text], convert_to_numpy=True)[0]
        return embedding.tolist()

embedding_wrapper = SentenceTransformerEmbeddingWrapper(model)


# ==== HÃ€M Gá»ŒI GEMINI API (cÃ³ duy trÃ¬ lá»‹ch sá»­) ====
def call_gemini_api_with_history(message_history, api_key):
    headers = {"Content-Type": "application/json"}
    data = {
        "contents": message_history,
        "generationConfig": {
            "temperature": 0.7,
            "topK": 10,
            "topP": 0.95,
            "maxOutputTokens": 2048,
        }
    }
    try:
        response = requests.post(
            f"{GEMINI_API_URL}?key={api_key}",
            headers=headers,
            json=data,
            timeout=30
        )
        if response.status_code == 200:
            result = response.json()
            candidates = result.get("candidates", [])
            if candidates:
                return candidates[0]["content"]["parts"][0]["text"].strip()
            return "âŒ KhÃ´ng nháº­n Ä‘Æ°á»£c pháº£n há»“i tá»« Gemini API."
        return f"âŒ Lá»—i API Gemini: {response.status_code} - {response.text}"
    except Exception as e:
        return f"âŒ Lá»—i khi gá»i Gemini: {str(e)}"


# ==== PROMPT KHá»I Táº O ====
def create_qa_prompt():
    return (
        "Báº¡n lÃ  má»™t chuyÃªn gia Ä‘iá»‡n áº£nh. Tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng vá» cÃ¡c bá»™ phim, "
        "Tráº£ lá»i ngáº¯n gá»n, dá»… hiá»ƒu."
        "HÃ£y duy trÃ¬ ngá»¯ cáº£nh cá»§a cuá»™c trÃ² chuyá»‡n vÃ  tham kháº£o cÃ¡c cÃ¢u há»i trÆ°á»›c Ä‘Ã³ khi cáº§n thiáº¿t. "
        "Náº¿u ngÆ°á»i dÃ¹ng há»i vá» 'phim nÃ y', 'bá»™ phim Ä‘Ã³', 'phim vá»«a nÃ³i', hoáº·c khÃ´ng Ä‘á» cáº­p tÃªn phim. HÃ£y hiá»ƒu há» Ä‘ang nháº¯c Ä‘áº¿n phim Ä‘Æ°á»£c Ä‘á» cáº­p gáº§n nháº¥t."
    )


# ==== Há»† THá»NG QA ====
class MovieQASystem:
    def __init__(self, vector_db=None, api_key=None, max_history=10):
        self.db = vector_db or load_vector_database()
        self.api_key = api_key or GEMINI_API_KEY
        self.max_history = max_history

        # Khá»Ÿi táº¡o lá»‹ch sá»­ vá»›i system prompt
        self.message_history = [
            {"role": "user", "parts": [{"text": create_qa_prompt()}]},
            {"role": "model", "parts": [{
                        "text": "ChÃ o báº¡n! TÃ´i lÃ  chuyÃªn gia Ä‘iá»‡n áº£nh. TÃ´i sáº½ tráº£ lá»i cÃ¡c cÃ¢u há»i vá» phim. HÃ£y há»i tÃ´i báº¥t ká»³ Ä‘iá»u gÃ¬ vá» phim nhÃ©!"}]}
        ]

        self.last_used_docs = []
        self.current_movie_context = None

    def search_relevant_docs(self, query, k=20):
        try:
            return self.db.similarity_search(query, k=k)
        except Exception as e:
            print(f"âŒ Lá»—i khi tÃ¬m kiáº¿m: {e}")
            return []

    def update_history(self, role, text):
        """Cáº­p nháº­t lá»‹ch sá»­ trÃ² chuyá»‡n"""
        self.message_history.append({"role": role, "parts": [{"text": text}]})

        # Giá»¯ láº¡i system prompt vÃ  giá»›i háº¡n lá»‹ch sá»­
        if len(self.message_history) > self.max_history + 2:  # +2 cho system prompt vÃ  response
            # Giá»¯ láº¡i system prompt (2 message Ä‘áº§u) vÃ  cÃ¡c message gáº§n nháº¥t
            system_messages = self.message_history[:2]
            recent_messages = self.message_history[-(self.max_history):]
            self.message_history = system_messages + recent_messages

    def extract_movie_from_response(self, response):
        """TrÃ­ch xuáº¥t tÃªn phim tá»« cÃ¢u tráº£ lá»i Ä‘á»ƒ lÆ°u context"""
        # TÃ¬m kiáº¿m pattern phim trong cÃ¢u tráº£ lá»i
        movie_patterns = [
            r'[Pp]him\s+["\*]?([^"\*\n]+)["\*]?',
            r'[Bb]á»™ phim\s+["\*]?([^"\*\n]+)["\*]?',
            r'[Tt]Ã¡c pháº©m\s+["\*]?([^"\*\n]+)["\*]?'
        ]

        for pattern in movie_patterns:
            matches = re.findall(pattern, response)
            if matches:
                return matches[0].strip()
        return None

    def build_context_aware_prompt(self, question, docs):
        """Táº¡o prompt cÃ³ nháº­n thá»©c vá» ngá»¯ cáº£nh"""
        context_parts = []

        # ThÃªm thÃ´ng tin tá»« vector database
        if docs:
            doc_context = "\n".join(f"- {doc.page_content}" for doc in docs)
            context_parts.append(f"THÃ”NG TIN THAM KHáº¢O:\n{doc_context}")

        # ThÃªm ngá»¯ cáº£nh phim hiá»‡n táº¡i náº¿u cÃ³
        if self.current_movie_context:
            context_parts.append(f"PHIM ÄANG THáº¢O LUáº¬N: {self.current_movie_context}")

        # ThÃªm lá»i nháº¯c vá» ngá»¯ cáº£nh
        context_parts.append(
            "LÆ¯U Ã: HÃ£y tham kháº£o cÃ¡c cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i trÆ°á»›c Ä‘Ã³ trong cuá»™c trÃ² chuyá»‡n Ä‘á»ƒ hiá»ƒu Ä‘Ãºng ngá»¯ cáº£nh.")

        if context_parts:
            full_context = "\n\n".join(context_parts)
            return f"{full_context}\n\nCÃ‚U Há»I: {question}"
        else:
            return f"CÃ‚U Há»I: {question}"

    def is_context_dependent_question(self, question):
        """Kiá»ƒm tra xem cÃ¢u há»i cÃ³ phá»¥ thuá»™c vÃ o ngá»¯ cáº£nh khÃ´ng"""
        context_indicators = [
            'phim nÃ y', 'phim Ä‘Ã³', 'phim vá»«a nÃ³i', 'bá»™ phim nÃ y', 'bá»™ phim Ä‘Ã³',
            'phim trÃªn', 'phim nÃ³i á»Ÿ trÃªn', 'cÃ³ nhá»¯ng ai', 'diá»…n viÃªn nÃ o',
            'khi nÃ o', 'nÄƒm nÃ o', 'thá»ƒ loáº¡i gÃ¬', 'Ä‘áº¡o diá»…n nÃ o'
        ]

        question_lower = question.lower()
        return any(indicator in question_lower for indicator in context_indicators)

    def answer_question(self, question):
        """Tráº£ lá»i cÃ¢u há»i vá»›i duy trÃ¬ ngá»¯ cáº£nh"""
        # TÃ¬m kiáº¿m documents liÃªn quan
        docs = self.search_relevant_docs(question)

        # Náº¿u khÃ´ng tÃ¬m tháº¥y doc má»›i vÃ  cÃ¢u há»i phá»¥ thuá»™c ngá»¯ cáº£nh, dÃ¹ng doc cÅ©
        if not docs and self.is_context_dependent_question(question):
            docs = self.last_used_docs

        # Cáº­p nháº­t doc Ä‘Æ°á»£c sá»­ dá»¥ng
        if docs:
            self.last_used_docs = docs

        # Táº¡o prompt cÃ³ nháº­n thá»©c ngá»¯ cáº£nh
        prompt = self.build_context_aware_prompt(question, docs)

        # ThÃªm cÃ¢u há»i vÃ o lá»‹ch sá»­
        self.update_history("user", prompt)

        # Gá»i API
        response = call_gemini_api_with_history(self.message_history, self.api_key)

        # Cáº­p nháº­t ngá»¯ cáº£nh phim hiá»‡n táº¡i
        movie_mentioned = self.extract_movie_from_response(response)
        if movie_mentioned:
            self.current_movie_context = movie_mentioned

        # ThÃªm cÃ¢u tráº£ lá»i vÃ o lá»‹ch sá»­
        self.update_history("model", response)

        return response

# ========== Äá»ŒC VECTORSTORE FAISS ==========
def load_vector_database():
    if not os.path.exists(MOVIE_VECTOR_DB):
        raise FileNotFoundError(f"ThÆ° má»¥c vector DB khÃ´ng tá»“n táº¡i: {MOVIE_VECTOR_DB}")
    try:
        db = FAISS.load_local(
            MOVIE_VECTOR_DB,
            embedding_wrapper,
            allow_dangerous_deserialization=True
        )
        return db
    except Exception as e:
        raise Exception(f"Lá»—i khi load vector database: {e}")

# ========== MAIN PROGRAM ==========
qa_system = None

def initialize_qa_system():
    global qa_system
    if qa_system is None:
        db = load_vector_database()
        qa_system = MovieQASystem(db, GEMINI_API_KEY)
    return qa_system

def process_user_question(question: str) -> str:
    qa = initialize_qa_system()
    return qa.answer_question(question.strip())

def main():
    print("ğŸ¬ Há»† THá»NG TRáº¢ Lá»œI CÃ‚U Há»I Vá»€ PHIM áº¢NH (Vá»›i Context Memory)")
    print("=" * 60)
    
    if not GEMINI_API_KEY:
        print("âŒ Vui lÃ²ng Ä‘áº·t biáº¿n mÃ´i trÆ°á»ng GEMINI_API_KEY.")
        return

    print("ğŸ”„ Äang load vector database...")
    try:
        initialize_qa_system()
        print("âœ… Load vector database thÃ nh cÃ´ng!")
    except Exception as e:
        print(f"âŒ {e}")
        return

    print("\nğŸ¤– Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng! HÃ£y Ä‘áº·t cÃ¢u há»i vá» phim áº£nh.")
    print("   - 'quit' hoáº·c 'exit': ThoÃ¡t chÆ°Æ¡ng trÃ¬nh")
    while True:
        try:
            question = input("â“ CÃ¢u há»i cá»§a báº¡n: ").strip()
            if question.lower() in ['quit', 'exit', 'thoÃ¡t']:
                print("ğŸ‘‹ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng há»‡ thá»‘ng!")
                break
            if not question:
                print("âš ï¸ Vui lÃ²ng nháº­p cÃ¢u há»i.")
                continue
            print("ğŸ”„ Äang tÃ¬m kiáº¿m vÃ  táº¡o cÃ¢u tráº£ lá»i...")
            answer = process_user_question(question)
            print(f"\nğŸ¤– Tráº£ lá»i:\n{answer}")
            print("-" * 50)
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ÄÃ£ dá»«ng chÆ°Æ¡ng trÃ¬nh.")
            break
        except Exception as e:
            print(f"âŒ Lá»—i: {e}")
            continue

if __name__ == "__main__":
    main()
