import os
import requests
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from langchain.embeddings.base import Embeddings
from langchain.prompts import PromptTemplate
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain.schema import HumanMessage, AIMessage
from langchain_core.runnables import RunnableLambda
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.language_models.chat_models import SimpleChatModel

# ==== C·∫§U H√åNH ====
load_dotenv()
base_dir = os.path.dirname(os.path.abspath(__file__))
MOVIE_VECTOR_DB = os.path.join(base_dir, "vector_db/movie_vector_db")
EMBEDDING_MODEL = "AITeamVN/Vietnamese_Embedding"

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY_3")
GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"

# ==== LOAD MODEL EMBEDDING ====
model = SentenceTransformer(EMBEDDING_MODEL)
model.max_seq_length = 2048

# ========== WRAPPER EMBEDDING CHO LANGCHAIN ==========
class SentenceTransformerEmbeddingWrapper(Embeddings):
    def __init__(self, model):
        self.model = model

    def embed_documents(self, texts):
        embeddings = self.model.encode(texts, convert_to_numpy=True)
        return embeddings.tolist()

    def embed_query(self, text):
        embedding = self.model.encode([text], convert_to_numpy=True)[0]
        return embedding.tolist()

embedding_wrapper = SentenceTransformerEmbeddingWrapper(model)

# ========== H√ÄM G·ªåI GEMINI API ==========
def call_gemini_api_conversational(messages, api_key):
    headers = {"Content-Type": "application/json"}
    data = {
        "contents": messages,
        "generationConfig": {
            "temperature": 0.7,
            "topK": 10,
            "topP": 0.95,
            "maxOutputTokens": 2048,
        }
    }
    try:
        response = requests.post(
            f"{GEMINI_API_URL}?key={api_key}",
            headers=headers,
            json=data,
            timeout=30
        )
        if response.status_code == 200:
            result = response.json()
            candidates = result.get("candidates", [])
            if candidates:
                return candidates[0]["content"]["parts"][0]["text"].strip()
            return "‚ùå Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ Gemini API."
        return f"‚ùå L·ªói API Gemini: {response.status_code} - {response.text}"
    except Exception as e:
        return f"‚ùå L·ªói khi g·ªçi Gemini: {str(e)}"

# ========== PROMPT TEMPLATE ==========
def create_qa_prompt():
    return (
        "B·∫°n l√† m·ªôt chuy√™n gia ƒëi·ªán ·∫£nh. Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v·ªÅ c√°c b·ªô phim, "
        "di·ªÖn vi√™n, ƒë·∫°o di·ªÖn, th·ªÉ lo·∫°i ho·∫∑c nƒÉm ph√°t h√†nh m·ªôt c√°ch ch√≠nh x√°c v√† d·ªÖ hi·ªÉu."
    )

prompt = PromptTemplate(
    input_variables=["history", "input"],
    template="""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ phim ·∫£nh. D·ª±a v√†o l·ªãch s·ª≠ h·ªôi tho·∫°i v√† c√¢u h·ªèi m·ªõi, h√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† ƒë√∫ng tr·ªçng t√¢m.

L·ªãch s·ª≠ h·ªôi tho·∫°i:
{history}

C√¢u h·ªèi: {input}
Tr·ª£ l√Ω:"""
)

# ========== GEMINI CHAT MODEL WRAPPER ==========
class GeminiChatModel(SimpleChatModel):
    @property
    def _llm_type(self) -> str:
        return "gemini-chat-model"

    def _call(self, messages, **kwargs):
        parts = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                parts.append({"role": "user", "parts": [{"text": msg.content}]})
            elif isinstance(msg, AIMessage):
                parts.append({"role": "model", "parts": [{"text": msg.content}]})

        # ‚ú® Kh√¥ng c·∫ßn nh·ªìi prompt n·∫øu ƒë√£ c√≥ l·ªãch s·ª≠
        if not parts:
            # N·∫øu l·∫ßn ƒë·∫ßu ti√™n, m·ªõi nh√©t prompt
            parts.append({"role": "user", "parts": [{"text": create_qa_prompt()}]})

        # G·ªçi Gemini API
        response = call_gemini_api_conversational(parts, GEMINI_API_KEY)
        return response

# ========== H·ªÜ TH·ªêNG QA ==========
class MovieQASystem:
    def __init__(self, vector_db=None, api_key=None):
        self.db = vector_db or load_vector_database()
        self.api_key = api_key or GEMINI_API_KEY

        llm = GeminiChatModel()

        llm_chain = RunnableLambda(
            lambda inputs: GeminiChatModel()._call(inputs["history"] + [HumanMessage(content=inputs["input"])])
        )

        self.chain = RunnableWithMessageHistory(
            llm_chain,
            lambda session_id: InMemoryChatMessageHistory(),
            input_messages_key="input",
            history_messages_key="history",
        )

    def search_relevant_docs(self, query, k=10):
        try:
            return self.db.similarity_search(query, k=k)
        except Exception as e:
            print(f"‚ùå L·ªói khi t√¨m ki·∫øm: {e}")
            return []

    def answer_question(self, question):
        docs = self.search_relevant_docs(question)
        context = "\n".join(f"- {doc.page_content}" for doc in docs) if docs else ""
        if context:
            question = f"TH√îNG TIN THAM KH·∫¢O:\n{context}\n\nC√ÇU H·ªéI: {question}"

        result = self.chain.invoke(
            {"input": question},
            config={"configurable": {"session_id": "movie_qa_session"}}
        )
        return result.content if hasattr(result, "content") else result

# ========== ƒê·ªåC VECTORSTORE FAISS ==========
def load_vector_database():
    if not os.path.exists(MOVIE_VECTOR_DB):
        raise FileNotFoundError(f"Th∆∞ m·ª•c vector DB kh√¥ng t·ªìn t·∫°i: {MOVIE_VECTOR_DB}")
    try:
        db = FAISS.load_local(
            MOVIE_VECTOR_DB,
            embedding_wrapper,
            allow_dangerous_deserialization=True
        )
        return db
    except Exception as e:
        raise Exception(f"L·ªói khi load vector database: {e}")

# ========== MAIN PROGRAM ==========
def main():
    print("\U0001f3ac H·ªÜ TH·ªêNG TR·∫¢ L·ªúI C√ÇU H·ªéI V·ªÄ PHIM ·∫¢NH (V·ªõi Context Memory)")
    print("=" * 60)
    if not GEMINI_API_KEY:
        print("‚ùå Vui l√≤ng ƒë·∫∑t bi·∫øn m√¥i tr∆∞·ªùng GEMINI_API_KEY.")
        return
    print("üîÑ ƒêang load vector database...")
    try:
        db = load_vector_database()
        print("‚úÖ Load vector database th√†nh c√¥ng!")
    except Exception as e:
        print(f"‚ùå {e}")
        return
    qa_system = MovieQASystem(db, GEMINI_API_KEY)
    print("\nü§ñ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng! H√£y ƒë·∫∑t c√¢u h·ªèi v·ªÅ phim ·∫£nh.")
    print("   - 'quit' ho·∫∑c 'exit': Tho√°t ch∆∞∆°ng tr√¨nh")
    while True:
        try:
            question = input("‚ùì C√¢u h·ªèi c·ªßa b·∫°n: ").strip()
            if question.lower() in ['quit', 'exit', 'tho√°t']:
                print("üëã C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng h·ªá th·ªëng!")
                break
            if not question:
                print("‚ö†Ô∏è Vui l√≤ng nh·∫≠p c√¢u h·ªèi.")
                continue
            print("üîÑ ƒêang t√¨m ki·∫øm v√† t·∫°o c√¢u tr·∫£ l·ªùi...")
            answer = qa_system.answer_question(question)
            print(f"\nü§ñ Tr·∫£ l·ªùi:")
            print(f"{answer}")
            print("-" * 50)
        except KeyboardInterrupt:
            print("\nüëã ƒê√£ d·ª´ng ch∆∞∆°ng tr√¨nh.")
            break
        except Exception as e:
            print(f"‚ùå L·ªói: {e}")
            continue

if __name__ == "__main__":
    main()
