import os
import time
import numpy as np
import faiss
import cv2
import librosa
import soundfile as sf
import tempfile
import matplotlib.pyplot as plt
from PIL import Image
import io
from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas
from collections import Counter
from tensorflow.keras.applications.resnet50 import preprocess_input
from app.models.models_audio.call_model_cnn import call_model

# ==== C·∫•u h√¨nh ====
IMAGE_SIZE = 224
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
INDEX_PATH = os.path.join(BASE_DIR, "features_faiss/resnet50/faiss_features.index")
LABEL_PATH = os.path.join(BASE_DIR, "features_faiss/resnet50/faiss_labels.npy")

# SIMILARITY_THRESHOLD = 0.8
# TOP_K = 3

CLASSES = {
    1: "21 Ng√†y Y√™u Em", 2: "4 NƒÉm 2 Ch√†ng 1 T√¨nh Y√™u", 3: "ƒÇn T·∫øt B√™n C·ªìn", 4: "B·∫´y Ng·ªçt Ng√†o", 5: "B·ªánh Vi·ªán Ma",
    6: "B√≠ M·∫≠t L·∫°i B·ªã M·∫•t", 7: "B√≠ M·∫≠t Trong S∆∞∆°ng M√π", 8: "B·ªô T·ª© Oan Gia", 9: "Ch·ªù Em ƒê·∫øn Ng√†y Mai",
    10: "Ch·ªß T·ªãch Giao H√†ng", 11: "Chuy·ªán T·∫øt", 12: "C√¥ Ba S√†i G√≤n", 13: "ƒê√†o, Ph·ªü V√† Piano", 14: "ƒê·∫•t R·ª´ng Ph∆∞∆°ng Nam",
    15: "ƒê·ªãa ƒê·∫°o", 16: "ƒê·ªãnh M·ªánh Thi√™n √ù", 17: "ƒê√¥i M·∫Øt √Çm D∆∞∆°ng", 18: "Em Ch∆∞a 18", 19: "Em L√† C·ªßa Em",
    20: "G√°i Gi√† L·∫Øm Chi√™u 3", 21: "Gi·∫£ Ngh√®o G·∫∑p Ph·∫≠t", 22: "H·∫ªm C·ª•t", 23: "Ho√°n ƒê·ªïi", 24: "K·∫ª ·∫®n Danh",
    25: "K·∫ª ƒÇn H·ªìn", 26: "L√†m Gi√†u V·ªõi Ma", 27: "L·∫≠t M·∫∑t 1", 28: "L·ªô M·∫∑t", 29: "Ma Da", 30: "M·∫Øt Bi·∫øc",
    31: "Ngh·ªÅ Si√™u D·ªÖ", 32: "Nh·ªØng N·ª• H√¥n R·ª±c R·ª°", 33: "√îng Ngo·∫°i Tu·ªïi 30", 34: "Ph√°p S∆∞ T·∫≠p S·ª±", 35: "Qu·ª∑ C·∫©u",
    36: "Qu√Ω C√¥ Th·ª´a K·∫ø", 37: "Ra M·∫Øt Gia Ti√™n", 38: "Si√™u L·ª´a G·∫∑p Si√™u L·∫ßy", 39: "Si√™u Tr·ª£ L√Ω",
    40: "T·∫•m C√°m Chuy·ªán Ch∆∞a K·ªÉ", 41: "Taxi Em T√™n G√¨", 42: "The Call", 43: "Thi√™n M·ªánh Anh H√πng",
    44: "Ti·ªÉu Th∆∞ V√† Ba ƒê·∫ßu G·∫•u", 45: "Tr√™n B√†n Nh·∫≠u D∆∞·ªõi B√†n M∆∞u", 46: "Kh√°c"
}

# ==== Load model v√† FAISS ====
def load_model_and_index():
    if not os.path.exists(INDEX_PATH) or not os.path.exists(LABEL_PATH):
        raise FileNotFoundError("‚ùå Kh√¥ng t√¨m th·∫•y FAISS index ho·∫∑c labels.")

    model = call_model()
    index = faiss.read_index(INDEX_PATH)
    labels = np.load(LABEL_PATH)
    return model, index, labels

# ==== L2 normalize ====
def l2_normalize(vectors):
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    return vectors / (norms + 1e-10)

# ==== Convert b·∫•t k·ª≥ audio v·ªÅ WAV ====
def convert_to_wav(audio_path):
    y, sr = librosa.load(audio_path, sr=None)
    temp_wav = os.path.join(tempfile.gettempdir(), f"temp_{os.path.basename(audio_path)}.wav")
    sf.write(temp_wav, y, sr)
    return temp_wav

# ==== T·∫°o spectrogram ====
def generate_spectrogram(y, sr):
    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
    S_dB = librosa.power_to_db(S, ref=np.max)

    fig = plt.figure(figsize=(2.24, 2.24), dpi=100)
    librosa.display.specshow(S_dB, sr=sr, x_axis=None, y_axis=None)
    plt.axis('off')
    plt.tight_layout(pad=0)

    # Chuy·ªÉn matplotlib figure th√†nh ·∫£nh numpy RGB
    canvas = FigureCanvas(fig)
    buf = io.BytesIO()
    canvas.print_png(buf)
    buf.seek(0)
    image = Image.open(buf).convert("RGB")
    plt.close(fig)

    img_array = np.array(image.resize((224, 224)))
    return img_array

# ==== C·∫Øt audio th√†nh 10 ƒëo·∫°n 10s c√°ch nhau 1s, t·ª´ gi·ªØa file ====
def split_audio_segments(wav_path, segment_duration=10.0, num_segments=10, hop_seconds=1.0):
    y, sr = librosa.load(wav_path, sr=22050)
    total_samples = len(y)
    segment_samples = int(segment_duration * sr)
    hop_samples = int(hop_seconds * sr)

    segments = []
    for i in range(num_segments):
        start = i * hop_samples
        end = start + segment_samples

        if start >= total_samples:
            break

        segment = y[start:end]
        if len(segment) < segment_samples:
            segment = np.pad(segment, (0, segment_samples - len(segment)), mode='constant')
        segments.append(segment)

    return segments, sr

# ==== Chuy·ªÉn m·ªói ƒëo·∫°n th√†nh ·∫£nh log-mel v√† batch CNN input ====
def segments_to_cnn_input(segments, sr):
    batch = []
    for y in segments:
        img = generate_spectrogram(y, sr)  # tr·∫£ v·ªÅ ·∫£nh RGB
        batch.append(img)
    x = np.array(batch)
    x = preprocess_input(x)
    return x

# ==== D·ª± ƒëo√°n t·ª´ batch feature ====
def predict_from_feature_batch(features, index, index_labels, n_movies, similarity_threshold):
    features = l2_normalize(features.astype(np.float32))
    D, I = index.search(features, n_movies)  

    prediction_stats = {}

    for distances, indices in zip(D, I):  
        for dist, idx in zip(distances, indices): 
            sim = 1 - dist / 2

            pred_data = index_labels[idx]
            if isinstance(pred_data, (np.ndarray, list)) and len(pred_data) > 1:
                pred_label = int(np.argmax(pred_data)) + 1
            else:
                pred_label = int(pred_data)

            film_name = CLASSES.get(pred_label, "Kh√°c")

            # N·∫øu similarity th·∫•p h∆°n ng∆∞·ª°ng, g√°n l√† "Kh√°c"
            if sim < similarity_threshold:
                film_name = CLASSES.get(46, "Kh√°c")

            # Ghi nh·∫≠n s·ªë l·∫ßn v√† ƒë·ªô t∆∞∆°ng ƒë·ªìng cao nh·∫•t cho m·ªói phim
            if film_name not in prediction_stats:
                prediction_stats[film_name] = {
                    "count": 1,
                    "max_sim": sim
                }
            else:
                prediction_stats[film_name]["count"] += 1
                prediction_stats[film_name]["max_sim"] = max(prediction_stats[film_name]["max_sim"], sim)

    # S·∫Øp x·∫øp theo: s·ªë l·∫ßn xu·∫•t hi·ªán gi·∫£m d·∫ßn ‚Üí ƒë·ªô t∆∞∆°ng ƒë·ªìng cao nh·∫•t gi·∫£m d·∫ßn
    sorted_films = sorted(
        prediction_stats.items(),
        key=lambda item: (-item[1]["count"], -item[1]["max_sim"])
    )

    # In th√¥ng tin chi ti·∫øt
    print("üìù K·∫øt qu·∫£ ph√¢n t√≠ch phim:")
    for film, stats in sorted_films:
        print(f"{film:<25}:  {stats['count']:<2}")

    result = [film for film, _ in sorted_films]
    return result[:n_movies]


# ==== H√†m ch√≠nh ====
def predict_film_from_audio(audio_path, similarity_threshold, n_movies):
    model, index, index_labels = load_model_and_index()
    try:
        start = time.time()
        wav_path = convert_to_wav(audio_path)
        segments, sr = split_audio_segments(wav_path)
        cnn_input = segments_to_cnn_input(segments, sr)
        features = model.predict(cnn_input, verbose=0)
        result = predict_from_feature_batch(features, index, index_labels, n_movies, similarity_threshold)
        print(f"Th·ªùi gian x·ª≠ l√Ω: {time.time() - start:.2f} gi√¢y")
        return result
    except Exception as e:
        return [f"‚ùå L·ªói x·ª≠ l√Ω √¢m thanh: {e}"]

# ==== Test ====
# if __name__ == "__main__":
#     test_audio = os.path.join(BASE_DIR, "img_test/0_scene_0880.mp3")
#     result = predict_film_from_audio(test_audio, 0.8, 1)
#     print("üé¨ D·ª± ƒëo√°n:", result)
