import os
import time
import numpy as np
import faiss
import cv2
import librosa
import soundfile as sf
import tempfile
import matplotlib.pyplot as plt
from PIL import Image
import io
from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas
from collections import Counter
from tensorflow.keras.applications.resnet50 import preprocess_input
from app.models.models_audio.call_model_cnn import call_model

# ==== Cáº¥u hÃ¬nh ====
IMAGE_SIZE = 224
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
INDEX_PATH = os.path.join(BASE_DIR, "features_faiss/resnet50/faiss_features.index")
LABEL_PATH = os.path.join(BASE_DIR, "features_faiss/resnet50/faiss_labels.npy")

# SIMILARITY_THRESHOLD = 0.8
# TOP_K = 3

CLASSES = {
    1: "21 NgÃ y YÃªu Em", 2: "4 NÄƒm 2 ChÃ ng 1 TÃ¬nh YÃªu", 3: "Ä‚n Táº¿t BÃªn Cá»“n", 4: "Báº«y Ngá»t NgÃ o", 5: "Bá»‡nh Viá»‡n Ma",
    6: "BÃ­ Máº­t Láº¡i Bá»‹ Máº¥t", 7: "BÃ­ Máº­t Trong SÆ°Æ¡ng MÃ¹", 8: "Bá»™ Tá»© Oan Gia", 9: "Chá» Em Äáº¿n NgÃ y Mai",
    10: "Chá»§ Tá»‹ch Giao HÃ ng", 11: "Chuyá»‡n Táº¿t", 12: "CÃ´ Ba SÃ i GÃ²n", 13: "ÄÃ o, Phá»Ÿ VÃ  Piano", 14: "Äáº¥t Rá»«ng PhÆ°Æ¡ng Nam",
    15: "Äá»‹a Äáº¡o", 16: "Äá»‹nh Má»‡nh ThiÃªn Ã", 17: "ÄÃ´i Máº¯t Ã‚m DÆ°Æ¡ng", 18: "Em ChÆ°a 18", 19: "Em LÃ  Cá»§a Em",
    20: "GÃ¡i GiÃ  Láº¯m ChiÃªu 3", 21: "Giáº£ NghÃ¨o Gáº·p Pháº­t", 22: "Háº»m Cá»¥t", 23: "HoÃ¡n Äá»•i", 24: "Káº» áº¨n Danh",
    25: "Káº» Ä‚n Há»“n", 26: "LÃ m GiÃ u Vá»›i Ma", 27: "Láº­t Máº·t 1", 28: "Lá»™ Máº·t", 29: "Ma Da", 30: "Máº¯t Biáº¿c",
    31: "Nghá» SiÃªu Dá»…", 32: "Nhá»¯ng Ná»¥ HÃ´n Rá»±c Rá»¡", 33: "Ã”ng Ngoáº¡i Tuá»•i 30", 34: "PhÃ¡p SÆ° Táº­p Sá»±", 35: "Quá»· Cáº©u",
    36: "QuÃ½ CÃ´ Thá»«a Káº¿", 37: "Ra Máº¯t Gia TiÃªn", 38: "SiÃªu Lá»«a Gáº·p SiÃªu Láº§y", 39: "SiÃªu Trá»£ LÃ½",
    40: "Táº¥m CÃ¡m Chuyá»‡n ChÆ°a Ká»ƒ", 41: "Taxi Em TÃªn GÃ¬", 42: "The Call", 43: "ThiÃªn Má»‡nh Anh HÃ¹ng",
    44: "Tiá»ƒu ThÆ° VÃ  Ba Äáº§u Gáº¥u", 45: "TrÃªn BÃ n Nháº­u DÆ°á»›i BÃ n MÆ°u", 46: "KhÃ¡c"
}

# ==== Load model vÃ  FAISS ====
def load_model_and_index():
    if not os.path.exists(INDEX_PATH) or not os.path.exists(LABEL_PATH):
        raise FileNotFoundError("âŒ KhÃ´ng tÃ¬m tháº¥y FAISS index hoáº·c labels.")

    model = call_model()
    index = faiss.read_index(INDEX_PATH)
    labels = np.load(LABEL_PATH)
    return model, index, labels

# ==== L2 normalize ====
def l2_normalize(vectors):
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    return vectors / (norms + 1e-10)

# ==== Convert báº¥t ká»³ audio vá» WAV ====
def convert_to_wav(audio_path):
    y, sr = librosa.load(audio_path, sr=None)
    temp_wav = os.path.join(tempfile.gettempdir(), f"temp_{os.path.basename(audio_path)}.wav")
    sf.write(temp_wav, y, sr)
    return temp_wav

# ==== Táº¡o spectrogram ====
def generate_spectrogram(y, sr):
    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
    S_dB = librosa.power_to_db(S, ref=np.max)

    fig = plt.figure(figsize=(2.24, 2.24), dpi=100)
    librosa.display.specshow(S_dB, sr=sr, x_axis=None, y_axis=None)
    plt.axis('off')
    plt.tight_layout(pad=0)

    # Chuyá»ƒn matplotlib figure thÃ nh áº£nh numpy RGB
    canvas = FigureCanvas(fig)
    buf = io.BytesIO()
    canvas.print_png(buf)
    buf.seek(0)
    image = Image.open(buf).convert("RGB")
    plt.close(fig)

    img_array = np.array(image.resize((224, 224)))
    return img_array

# ==== Cáº¯t audio thÃ nh 10 Ä‘oáº¡n 10s cÃ¡ch nhau 1s, tá»« giá»¯a file ====
def split_audio_segments(wav_path, segment_duration=10.0, num_segments=10, hop_seconds=1.0):
    y, sr = librosa.load(wav_path, sr=22050)
    total_samples = len(y)
    segment_samples = int(segment_duration * sr)
    hop_samples = int(hop_seconds * sr)

    segments = []
    for i in range(num_segments):
        start = i * hop_samples
        end = start + segment_samples

        if start >= total_samples:
            break

        segment = y[start:end]
        if len(segment) < segment_samples:
            segment = np.pad(segment, (0, segment_samples - len(segment)), mode='constant')
        segments.append(segment)

    return segments, sr

# ==== Chuyá»ƒn má»—i Ä‘oáº¡n thÃ nh áº£nh log-mel vÃ  batch CNN input ====
def segments_to_cnn_input(segments, sr):
    batch = []
    for y in segments:
        img = generate_spectrogram(y, sr)  # tráº£ vá» áº£nh RGB
        batch.append(img)
    x = np.array(batch)
    x = preprocess_input(x)
    return x

# ==== Dá»± Ä‘oÃ¡n tá»« batch feature ====
def predict_from_feature_batch(features, index, index_labels, n_movies, similarity_threshold):
    features = l2_normalize(features.astype(np.float32))
    D, I = index.search(features, n_movies)  

    all_predictions = []
    priority_predictions = []  # chá»©a phim cÃ³ dist == 0.99

    for distances, indices in zip(D, I):  
        for dist, idx in zip(distances, indices): 
            sim = 1 - dist / 2

            pred_data = index_labels[idx]
            if isinstance(pred_data, (np.ndarray, list)) and len(pred_data) > 1:
                pred_label = int(np.argmax(pred_data)) + 1
            else:
                pred_label = int(pred_data)

            film_name = CLASSES.get(pred_label, "KhÃ¡c")

            # Náº¿u dist == 0.99 thÃ¬ thÃªm vÃ o danh sÃ¡ch Æ°u tiÃªn
            if dist > 0.98:
                priority_predictions.append(film_name)

            # CÃ¡c trÆ°á»ng há»£p cÃ²n láº¡i xá»­ lÃ½ nhÆ° bÃ¬nh thÆ°á»ng
            elif sim >= similarity_threshold:
                all_predictions.append(film_name)
            else:
                all_predictions.append(CLASSES.get(46, "KhÃ¡c"))

    counts = Counter(all_predictions)

    # Láº¥y top-n_movies phim thÆ°á»ng xuyÃªn nháº¥t (bá» qua priority Ä‘Ã£ cÃ³)
    sorted_films = sorted(counts.items(), key=lambda item: (-item[1], all_predictions.index(item[0])))
    normal_results = [film for film, _ in sorted_films if film not in priority_predictions]

    # Káº¿t quáº£ cuá»‘i: Æ¯u tiÃªn phim cÃ³ dist = 0.99, sau Ä‘Ã³ Ä‘áº¿n phim thÆ°á»ng
    result = priority_predictions + normal_results
    return result[:n_movies]

# ==== HÃ m chÃ­nh ====
def predict_film_from_audio(audio_path, similarity_threshold, n_movies):
    model, index, index_labels = load_model_and_index()
    try:
        start = time.time()
        wav_path = convert_to_wav(audio_path)
        segments, sr = split_audio_segments(wav_path)
        cnn_input = segments_to_cnn_input(segments, sr)
        features = model.predict(cnn_input, verbose=0)
        result = predict_from_feature_batch(features, index, index_labels, n_movies, similarity_threshold)
        print(f"Thá»i gian xá»­ lÃ½: {time.time() - start:.2f} giÃ¢y")
        return result
    except Exception as e:
        return [f"âŒ Lá»—i xá»­ lÃ½ Ã¢m thanh: {e}"]

# ==== Test ====
# if __name__ == "__main__":
#     test_audio = os.path.join(BASE_DIR, "img_test/0_scene_0880.mp3")
#     result = predict_film_from_audio(test_audio, 0.8, 1)
#     print("ğŸ¬ Dá»± Ä‘oÃ¡n:", result)
