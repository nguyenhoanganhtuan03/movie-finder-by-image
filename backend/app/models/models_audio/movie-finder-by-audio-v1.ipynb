{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ccb1bca",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-11T02:56:04.256085Z",
     "iopub.status.busy": "2025-05-11T02:56:04.255672Z",
     "iopub.status.idle": "2025-05-11T02:56:06.475443Z",
     "shell.execute_reply": "2025-05-11T02:56:06.474263Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 2.23638,
     "end_time": "2025-05-11T02:56:06.477304",
     "exception": false,
     "start_time": "2025-05-11T02:56:04.240924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5901459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:47:53.604835Z",
     "iopub.status.busy": "2025-07-05T06:47:53.604682Z",
     "iopub.status.idle": "2025-07-05T06:48:07.753446Z",
     "shell.execute_reply": "2025-07-05T06:48:07.752491Z",
     "shell.execute_reply.started": "2025-07-05T06:47:53.604820Z"
    },
    "papermill": {
     "duration": 37.164931,
     "end_time": "2025-05-11T02:56:43.652725",
     "exception": false,
     "start_time": "2025-05-11T02:56:06.487794",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q faiss-cpu\n",
    "!pip install -q tensorflow==2.18.0\n",
    "!pip install -q skl2onnx onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0303f6a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:48:11.662101Z",
     "iopub.status.busy": "2025-07-05T06:48:11.661622Z",
     "iopub.status.idle": "2025-07-05T06:48:27.886260Z",
     "shell.execute_reply": "2025-07-05T06:48:27.885514Z",
     "shell.execute_reply.started": "2025-07-05T06:48:11.662068Z"
    },
    "papermill": {
     "duration": 17.588955,
     "end_time": "2025-05-11T02:57:01.252620",
     "exception": false,
     "start_time": "2025-05-11T02:56:43.663665",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 06:48:13.672056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751698093.912022      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751698093.986571      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "T√¨m th·∫•y 1 GPU:\n",
      "  GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "S·ªë l∆∞·ª£ng GPU v·∫≠t l√Ω: 1, s·ªë l∆∞·ª£ng GPU logic: 1\n",
      "Danh s√°ch GPU: ['/device:GPU:0']\n",
      "CUDA version: 12.5.1\n",
      "cuDNN version: 9\n",
      "\n",
      "X√°c nh·∫≠n GPU ƒëang ho·∫°t ƒë·ªông b·∫±ng ph√©p t√≠nh nh·ªè:\n",
      "T√≠nh to√°n tr√™n GPU: [[19. 22.]\n",
      " [43. 50.]]\n",
      "ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751698107.778276      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "I0000 00:00:1751698107.780259      35 gpu_device.cc:2022] Created device /device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Ki·ªÉm tra phi√™n b·∫£n TensorFlow\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# C·∫•u h√¨nh memory growth ƒë·ªÉ s·ª≠ d·ª•ng GPU hi·ªáu qu·∫£\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"T√¨m th·∫•y {len(gpus)} GPU:\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "    \n",
    "    # C·∫•u h√¨nh memory growth\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # M·ªôt s·ªë t√πy ch·ªçn ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t cho GPU T4\n",
    "        os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "        os.environ['TF_GPU_THREAD_COUNT'] = '2'  # T∆∞∆°ng ·ª©ng v·ªõi s·ªë GPU\n",
    "        \n",
    "        # Hi·ªÉn th·ªã c√°c GPU logic\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"S·ªë l∆∞·ª£ng GPU v·∫≠t l√Ω: {len(gpus)}, s·ªë l∆∞·ª£ng GPU logic: {len(logical_gpus)}\")\n",
    "        \n",
    "        # Th√¥ng tin chi ti·∫øt v·ªÅ GPU\n",
    "        from tensorflow.python.client import device_lib\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        gpu_list = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "        print(f\"Danh s√°ch GPU: {gpu_list}\")\n",
    "        \n",
    "        # Hi·ªÉn th·ªã th√¥ng tin CUDA v√† cuDNN\n",
    "        build_info = tf.sysconfig.get_build_info()\n",
    "        print(f\"CUDA version: {build_info.get('cuda_version', 'N/A')}\")\n",
    "        print(f\"cuDNN version: {build_info.get('cudnn_version', 'N/A')}\")\n",
    "        \n",
    "        # Ki·ªÉm tra xem GPU c√≥ th·ª±c s·ª± ƒë∆∞·ª£c s·ª≠ d·ª•ng hay kh√¥ng\n",
    "        print(\"\\nX√°c nh·∫≠n GPU ƒëang ho·∫°t ƒë·ªông b·∫±ng ph√©p t√≠nh nh·ªè:\")\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "            b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(f\"T√≠nh to√°n tr√™n GPU: {c}\")\n",
    "            print(f\"ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: {c.device}\")\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        print(f\"L·ªói khi c·∫•u h√¨nh GPU: {e}\")\n",
    "else:\n",
    "    print(\"Kh√¥ng t√¨m th·∫•y GPU! ƒêang s·ª≠ d·ª•ng CPU.\")\n",
    "    \n",
    "    # Ki·ªÉm tra th√¥ng tin CPU\n",
    "    cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "    print(f\"T√¨m th·∫•y {len(cpu_devices)} CPU: {cpu_devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d400a987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:48:36.304189Z",
     "iopub.status.busy": "2025-07-05T06:48:36.303452Z",
     "iopub.status.idle": "2025-07-05T06:48:37.730737Z",
     "shell.execute_reply": "2025-07-05T06:48:37.729856Z",
     "shell.execute_reply.started": "2025-07-05T06:48:36.304164Z"
    },
    "papermill": {
     "duration": 3.495843,
     "end_time": "2025-05-11T02:57:04.760578",
     "exception": false,
     "start_time": "2025-05-11T02:57:01.264735",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import gc\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm \n",
    "import shutil\n",
    "\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, Flatten, Dropout, AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, Resizing\n",
    "from tensorflow.keras.layers import MaxPooling2D, Activation, BatchNormalization, Attention, Reshape, RepeatVector, Lambda, Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import joblib\n",
    "\n",
    "import onnxruntime as ort\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "# B·ªè qua c√°c c·∫£nh b√°o\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# In phi√™n b·∫£n TensorFlow hi·ªán t·∫°i\n",
    "print('TensorFlow Version ' + tf.__version__)\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    # Thi·∫øt l·∫≠p seed ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh t√°i l·∫≠p (reproducibility)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# G·ªçi h√†m seed_everything ƒë·ªÉ thi·∫øt l·∫≠p seed m·∫∑c ƒë·ªãnh\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16300f50",
   "metadata": {
    "papermill": {
     "duration": 0.011256,
     "end_time": "2025-05-11T02:57:04.783743",
     "exception": false,
     "start_time": "2025-05-11T02:57:04.772487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tr·ª±c quan d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d03b50d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:48:46.378635Z",
     "iopub.status.busy": "2025-07-05T06:48:46.377615Z",
     "iopub.status.idle": "2025-07-05T06:51:04.906489Z",
     "shell.execute_reply": "2025-07-05T06:51:04.905714Z",
     "shell.execute_reply.started": "2025-07-05T06:48:46.378589Z"
    },
    "papermill": {
     "duration": 0.023891,
     "end_time": "2025-05-11T02:57:04.819087",
     "exception": false,
     "start_time": "2025-05-11T02:57:04.795196",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Th∆∞ m·ª•c Train====\n",
      "==> T·ªïng s·ªë ·∫£nh: 29693\n",
      "====Th∆∞ m·ª•c Test====\n",
      "==> T·ªïng s·ªë ·∫£nh: 11565\n"
     ]
    }
   ],
   "source": [
    "def count_images_per_folder(root_dir, image_extensions=None):\n",
    "    if image_extensions is None:\n",
    "        image_extensions = ['.jpg']\n",
    "\n",
    "    total = 0\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        count = sum(1 for file in files if any(file.lower().endswith(ext) for ext in image_extensions))\n",
    "        if count > 0:\n",
    "            relative_path = os.path.relpath(subdir, root_dir)\n",
    "            # print(f\"Th∆∞ m·ª•c '{relative_path}': {count} ·∫£nh\")\n",
    "            total += count\n",
    "\n",
    "    print(f\"==> T·ªïng s·ªë ·∫£nh: {total}\")\n",
    "\n",
    "print(\"====Th∆∞ m·ª•c Train====\")\n",
    "folder_train_path = '/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Train'\n",
    "count_images_per_folder(folder_train_path)\n",
    "\n",
    "print(\"====Th∆∞ m·ª•c Test====\")\n",
    "folder_test_path = '/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Test'\n",
    "count_images_per_folder(folder_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c03f6",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-11T02:57:04.844013Z",
     "iopub.status.busy": "2025-05-11T02:57:04.843197Z",
     "iopub.status.idle": "2025-05-11T02:57:05.173181Z",
     "shell.execute_reply": "2025-05-11T02:57:05.172139Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.343959,
     "end_time": "2025-05-11T02:57:05.174777",
     "exception": false,
     "start_time": "2025-05-11T02:57:04.830818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_images_per_folder(root_dir, image_extensions=None):\n",
    "    if image_extensions is None:\n",
    "        image_extensions = ['.jpg']\n",
    "\n",
    "    folder_counts = {}\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        if subdir == root_dir:\n",
    "            continue  # b·ªè qua th∆∞ m·ª•c g·ªëc\n",
    "        count = sum(1 for file in files if any(file.lower().endswith(ext) for ext in image_extensions))\n",
    "        if count > 0:\n",
    "            folder_name = os.path.basename(subdir)\n",
    "            folder_counts[folder_name] = count\n",
    "\n",
    "    return folder_counts\n",
    "\n",
    "def plot_image_counts(folder_counts):\n",
    "    folders = list(folder_counts.keys())\n",
    "    counts = list(folder_counts.values())\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.bar(folders, counts, color='skyblue')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('T√™n th∆∞ m·ª•c con')\n",
    "    plt.ylabel('S·ªë l∆∞·ª£ng ·∫£nh')\n",
    "    plt.title('S·ªë l∆∞·ª£ng ·∫£nh trong t·ª´ng th∆∞ m·ª•c con')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# T·∫≠p Train\n",
    "counts = count_images_per_folder(folder_train_path)\n",
    "plot_image_counts(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba3a3b5",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-11T02:57:05.201042Z",
     "iopub.status.busy": "2025-05-11T02:57:05.200540Z",
     "iopub.status.idle": "2025-05-11T02:57:05.424858Z",
     "shell.execute_reply": "2025-05-11T02:57:05.423957Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.239196,
     "end_time": "2025-05-11T02:57:05.426207",
     "exception": false,
     "start_time": "2025-05-11T02:57:05.187011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# T·∫≠p Test\n",
    "counts = count_images_per_folder(folder_test_path)\n",
    "plot_image_counts(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf24266",
   "metadata": {
    "papermill": {
     "duration": 0.012064,
     "end_time": "2025-05-11T02:57:05.450728",
     "exception": false,
     "start_time": "2025-05-11T02:57:05.438664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# T·∫°o t·∫≠p d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eafdbefb-6824-4ff5-af21-3dda3b304fb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:56:46.732348Z",
     "iopub.status.busy": "2025-07-05T06:56:46.731644Z",
     "iopub.status.idle": "2025-07-05T06:56:46.737740Z",
     "shell.execute_reply": "2025-07-05T06:56:46.737188Z",
     "shell.execute_reply.started": "2025-07-05T06:56:46.732311Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "n_classes = 45\n",
    "batch_size = 32\n",
    "\n",
    "classes_train = {\n",
    "    1: \"21_Ngay_Yeu_Em\",\n",
    "    2: \"4_Nam_2_Chang_1_Tinh_Yeu\",\n",
    "    3: \"An_Tet_Ben_Con\",\n",
    "    4: \"Bay_Ngot_Ngao\",\n",
    "    5: \"Benh_Vien_Ma\",\n",
    "    6: \"Bi_Mat_Lai_Bi_Mat\",\n",
    "    7: \"Bi_Mat_Trong_Suong_Mu\",\n",
    "    8: \"Bo_Tu_Oan_Gia\",\n",
    "    9: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    10: \"Chu_Tich_Giao_Hang\",\n",
    "    11: \"Chuyen_Tet\",\n",
    "    12: \"Co_Ba_Sai_Gon\",\n",
    "    13: \"Dao_Pho_Va_Piano\",\n",
    "    14: \"Dat_Rung_Phuong_Nam\",\n",
    "    15: \"Dia_Dao\",\n",
    "    16: \"Dinh_Menh_Thien_Y\",\n",
    "    17: \"Doi_Mat_Am_Duong\",\n",
    "    18: \"Em_Chua_18\",\n",
    "    19: \"Em_La_Cua_Em\",\n",
    "    20: \"Gai_Gia_Lam_Chieu_3\",\n",
    "    21: \"Gia_Ngheo_Gap_Phat\",\n",
    "    22: \"Hem_Cut\",\n",
    "    23: \"Hoan_Doi\",\n",
    "    24: \"Ke_An_Danh\",\n",
    "    25: \"Ke_An_Hon\",\n",
    "    26: \"Lam_Giau_Voi_Ma\",\n",
    "    27: \"Lat_Mat_1\",\n",
    "    28: \"Lo_Mat\",\n",
    "    29: \"Ma_Da\",\n",
    "    30: \"Mat_Biec\",\n",
    "    31: \"Nghe_Sieu_De\",\n",
    "    32: \"Nhung_Nu_Hon_Ruc_Ro\",\n",
    "    33: \"Ong_Ngoai_Tuoi_30\",\n",
    "    34: \"Phap_Su_Tap_Su\",\n",
    "    35: \"Quy_Cau\",\n",
    "    36: \"Quy_Co_Thua_Ke\",\n",
    "    37: \"Ra_Mat_Gia_Tien\",\n",
    "    38: \"Sieu_Lua_Gap_Sieu_Lay\",\n",
    "    39: \"Sieu_Tro_Ly\",\n",
    "    40: \"Tam_Cam_Chuyen_Chua_Ke\",\n",
    "    41: \"Taxi_Em_Ten_Gi\",\n",
    "    42: \"The_Call\",\n",
    "    43: \"Thien_Menh_Anh_Hung\",\n",
    "    44: \"Tieu_Thu_Va_Ba_Dau_Gau\",\n",
    "    45: \"Tren_Ban_Nhau_Duoi_Ban_Muu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8788e693",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:56:42.879376Z",
     "iopub.status.busy": "2025-07-05T06:56:42.878450Z",
     "iopub.status.idle": "2025-07-05T06:56:42.884619Z",
     "shell.execute_reply": "2025-07-05T06:56:42.883856Z",
     "shell.execute_reply.started": "2025-07-05T06:56:42.879344Z"
    },
    "papermill": {
     "duration": 0.02228,
     "end_time": "2025-05-11T02:57:05.485554",
     "exception": false,
     "start_time": "2025-05-11T02:57:05.463274",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classes_test = {\n",
    "    1: \"21_Ngay_Yeu_Em\",\n",
    "    2: \"4_Nam_2_Chang_1_Tinh_Yeu\",\n",
    "    3: \"An_Tet_Ben_Con\",\n",
    "    4: \"Bay_Ngot_Ngao\",\n",
    "    5: \"Benh_Vien_Ma\",\n",
    "    6: \"Bi_Mat_Lai_Bi_Mat\",\n",
    "    7: \"Bi_Mat_Trong_Suong_Mu\",\n",
    "    8: \"Bo_Tu_Oan_Gia\",\n",
    "    9: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    10: \"Chu_Tich_Giao_Hang\",\n",
    "    11: \"Chuyen_Tet\",\n",
    "    12: \"Co_Ba_Sai_Gon\",\n",
    "    13: \"Dao_Pho_Va_Piano\",\n",
    "    14: \"Dat_Rung_Phuong_Nam\",\n",
    "    15: \"Dia_Dao\",\n",
    "    16: \"Dinh_Menh_Thien_Y\",\n",
    "    17: \"Doi_Mat_Am_Duong\",\n",
    "    18: \"Em_Chua_18\",\n",
    "    19: \"Em_La_Cua_Em\",\n",
    "    20: \"Gai_Gia_Lam_Chieu_3\",\n",
    "    21: \"Gia_Ngheo_Gap_Phat\",\n",
    "    22: \"Hem_Cut\",\n",
    "    23: \"Hoan_Doi\",\n",
    "    24: \"Ke_An_Danh\",\n",
    "    25: \"Ke_An_Hon\",\n",
    "    26: \"Lam_Giau_Voi_Ma\",\n",
    "    27: \"Lat_Mat_1\",\n",
    "    28: \"Lo_Mat\",\n",
    "    29: \"Ma_Da\",\n",
    "    30: \"Mat_Biec\",\n",
    "    31: \"Nghe_Sieu_De\",\n",
    "    32: \"Nhung_Nu_Hon_Ruc_Ro\",\n",
    "    33: \"Ong_Ngoai_Tuoi_30\",\n",
    "    34: \"Phap_Su_Tap_Su\",\n",
    "    35: \"Quy_Cau\",\n",
    "    36: \"Quy_Co_Thua_Ke\",\n",
    "    37: \"Ra_Mat_Gia_Tien\",\n",
    "    38: \"Sieu_Lua_Gap_Sieu_Lay\",\n",
    "    39: \"Sieu_Tro_Ly\",\n",
    "    40: \"Tam_Cam_Chuyen_Chua_Ke\",\n",
    "    41: \"Taxi_Em_Ten_Gi\",\n",
    "    42: \"The_Call\",\n",
    "    43: \"Thien_Menh_Anh_Hung\",\n",
    "    44: \"Tieu_Thu_Va_Ba_Dau_Gau\",\n",
    "    45: \"Tren_Ban_Nhau_Duoi_Ban_Muu\",\n",
    "    46: \"Khac\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e712e4e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:51:20.006849Z",
     "iopub.status.busy": "2025-07-05T06:51:20.006582Z",
     "iopub.status.idle": "2025-07-05T06:51:24.668796Z",
     "shell.execute_reply": "2025-07-05T06:51:24.668150Z",
     "shell.execute_reply.started": "2025-07-05T06:51:20.006831Z"
    },
    "papermill": {
     "duration": 0.399419,
     "end_time": "2025-05-11T02:57:05.897695",
     "exception": true,
     "start_time": "2025-05-11T02:57:05.498276",
     "status": "failed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29693 files belonging to 45 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = '/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Train'\n",
    "\n",
    "train_df = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',  \n",
    "    seed=1,\n",
    "    image_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c786449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:08:22.399474Z",
     "iopub.status.busy": "2025-05-10T13:08:22.398955Z",
     "iopub.status.idle": "2025-05-10T13:08:22.882416Z",
     "shell.execute_reply": "2025-05-10T13:08:22.881644Z",
     "shell.execute_reply.started": "2025-05-10T13:08:22.399444Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_df))  \n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(5, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for img, ax in zip(images[:9], axes): \n",
    "    ax.imshow(img.numpy().astype(\"uint8\"))  \n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc39b51",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# X√≥a file trong th∆∞ m·ª•c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1296acbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:51:04.908033Z",
     "iopub.status.busy": "2025-07-05T06:51:04.907802Z",
     "iopub.status.idle": "2025-07-05T06:51:05.387710Z",
     "shell.execute_reply": "2025-07-05T06:51:05.387150Z",
     "shell.execute_reply.started": "2025-07-05T06:51:04.908015Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def delete_all_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # x√≥a file ho·∫∑c symbolic link\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # x√≥a th∆∞ m·ª•c v√† to√†n b·ªô n·ªôi dung b√™n trong\n",
    "        except Exception as e:\n",
    "            print(f\"Kh√¥ng th·ªÉ x√≥a {file_path}: {e}\")\n",
    "\n",
    "# V√≠ d·ª•:\n",
    "folder = \"/kaggle/working/\"\n",
    "delete_all_in_folder(folder)\n",
    "\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096302b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdad1feb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:52:26.257127Z",
     "iopub.status.busy": "2025-07-05T06:52:26.256640Z",
     "iopub.status.idle": "2025-07-05T06:55:53.169068Z",
     "shell.execute_reply": "2025-07-05T06:55:53.168261Z",
     "shell.execute_reply.started": "2025-07-05T06:52:26.257104Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 928/928 [03:25<00:00,  4.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# S·ªë batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Kh·ªüi t·∫°o model trong context c·ªßa strategy\n",
    "with strategy.scope():\n",
    "    base_model_resnet50 = ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# N∆°i l∆∞u ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# L·∫∑p qua t·ª´ng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_resnet50(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# ƒê√°nh nh√£n ·ª©ng v·ªõi ƒë·∫∑c tr∆∞ng\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08b5c569",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:57:05.909843Z",
     "iopub.status.busy": "2025-07-05T06:57:05.909339Z",
     "iopub.status.idle": "2025-07-05T06:57:06.588001Z",
     "shell.execute_reply": "2025-07-05T06:57:06.587182Z",
     "shell.execute_reply.started": "2025-07-05T06:57:05.909823Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ th√™m 29693 vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chu·∫©n h√≥a L2 cho m·ªói vector (ƒë·ªô d√†i = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "\n",
    "# Chu·∫©n h√≥a vector ƒë·∫∑c tr∆∞ng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# K√≠ch th∆∞·ªõc vector ƒë·∫∑c tr∆∞ng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Kh·ªüi t·∫°o index FAISS s·ª≠ d·ª•ng kho·∫£ng c√°ch Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Th√™m vector ƒë√£ chu·∫©n h√≥a v√†o index\n",
    "index.add(normalized_features)\n",
    "print(\"ƒê√£ th√™m\", index.ntotal, \"vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\")\n",
    "\n",
    "# L∆∞u index v√† nh√£n\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447fee46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T07:45:22.054281Z",
     "iopub.status.busy": "2025-07-05T07:45:22.053722Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\n",
      "   - S·ªë l∆∞·ª£ng vectors: 29693\n",
      "   - K√≠ch th∆∞·ªõc vector: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing with FAISS:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 10240/11565 [18:21<02:18,  9.54it/s]"
     ]
    }
   ],
   "source": [
    "# C√†i ƒë·∫∑t c√°c tham s·ªë\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.95\n",
    "\n",
    "# ==== Load m√¥ h√¨nh v√† FAISS index ====\n",
    "# Load m√¥ h√¨nh ResNet50 \n",
    "model = ResNet50(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n FAISS index v√† labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y FAISS index t·∫°i: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y nh√£n t·∫°i: {label_path}\")\n",
    "\n",
    "# Load FAISS index v√† labels\n",
    "try:\n",
    "    # Load index tr·ª±c ti·∫øp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n",
    "    print(f\"   - S·ªë l∆∞·ª£ng vectors: {index.ntotal}\")\n",
    "    print(f\"   - K√≠ch th∆∞·ªõc vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi t·∫£i FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duy·ªát t·∫≠p test v√† l·∫•y t·∫•t c·∫£ ·∫£nh t·ª´ m·ªói th∆∞ m·ª•c ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # L·∫•y danh s√°ch t·∫•t c·∫£ ·∫£nh trong th∆∞ m·ª•c n√†y\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== D·ª± ƒëo√°n ====\n",
    "processing_times = []  # Danh s√°ch l∆∞u th·ªùi gian x·ª≠ l√Ω t·ª´ng ·∫£nh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # B·∫Øt ƒë·∫ßu ƒëo th·ªùi gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"‚ùå Folder kh√¥ng h·ª£p l·ªá: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Tr√≠ch ƒë·∫∑c tr∆∞ng v·ªõi m√¥ h√¨nh\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chu·∫©n h√≥a L2 ƒë·∫∑c tr∆∞ng m·∫´u truy v·∫•n ƒë·ªÉ ph√π h·ª£p v·ªõi vector ƒë√£ chu·∫©n h√≥a trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "        \n",
    "        # ƒê·∫£m b·∫£o ƒë·∫∑c tr∆∞ng c√≥ ƒë·ªãnh d·∫°ng ph√π h·ª£p sau khi chu·∫©n h√≥a\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # T√¨m ki·∫øm k=1 ƒëi·ªÉm g·∫ßn nh·∫•t trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Ki·ªÉm tra similarity score t·ª´ kho·∫£ng c√°ch L2 v·ªõi vector ƒë√£ chu·∫©n h√≥a\n",
    "        # V·ªõi vector ƒë√£ chu·∫©n h√≥a L2, kho·∫£ng c√°ch Euclidean b√¨nh ph∆∞∆°ng (D) v√† cosine similarity c√≥ quan h·ªá:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Kho·∫£ng c√°ch FAISS l√† b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuy·ªÉn ƒë·ªïi kho·∫£ng c√°ch th√†nh cosine similarity\n",
    "        \n",
    "        # N·∫øu similarity d∆∞·ªõi ng∆∞·ª°ng, g√°n nh√£n \"Kh√°c\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nh√£n \"Kh√°c\"\n",
    "        else:\n",
    "            # L·∫•y nh√£n d·ª± ƒëo√°n t·ª´ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n v√† nh√£n th·ª±c t·∫ø\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # K·∫øt th√∫c ƒëo th·ªùi gian v√† l∆∞u l·∫°i\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In th√¥ng tin cho debug (c√≥ th·ªÉ b·ªè comment n·∫øu c·∫ßn ki·ªÉm tra)\n",
    "        # print(f\"·∫¢nh: {img_path}, Similarity: {similarity_score:.4f}, Nh√£n d·ª± ƒëo√°n: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói ·∫£nh {img_path}: {e}\")\n",
    "        print(f\"  H√¨nh d·∫°ng ƒë·∫∑c tr∆∞ng: {feature.shape}, Ki·ªÉu d·ªØ li·ªáu: {feature.dtype}\")\n",
    "\n",
    "# ==== ƒê√°nh gi√° ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√°!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n‚úÖ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"‚úÖ ƒê√∫ng: {np.sum(y_true == y_pred)} / ‚ùå Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # T√≠nh th·ªùi gian x·ª≠ l√Ω trung b√¨nh cho m·ªôt ·∫£nh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    print(f\"‚è±Ô∏è T·ªïng th·ªùi gian x·ª≠ l√Ω: {sum(processing_times):.2f} gi√¢y cho {len(processing_times)} ·∫£nh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\nüñºÔ∏è Confusion matrix ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\nüìÑ Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuy·ªÉn b√°o c√°o ph√¢n lo·∫°i th√†nh DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hi·ªÉn th·ªã c√°c tham s·ªë chung cho to√†n b·ªô ch∆∞∆°ng tr√¨nh\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\nüìù C√°c tham s·ªë ƒë√°nh gi√° chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    \n",
    "    # L∆∞u b√°o c√°o chi ti·∫øt v√†o t·ªáp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{csv_output_path}'\")\n",
    "    \n",
    "    # L∆∞u c√°c tham s·ªë ƒë√°nh gi√° chung v√†o t·ªáp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\nüìä C√°c tham s·ªë ƒë√°nh gi√° chung ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # L∆∞u c·∫£ b√°o c√°o ph√¢n lo·∫°i v√† c√°c tham s·ªë chung v√†o m·ªôt t·ªáp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e4d9d-ce7b-40da-89f0-7217f83be69d",
   "metadata": {},
   "source": [
    "# DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af818a74-cc8e-4698-8387-1cbcac5b2519",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# S·ªë batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Kh·ªüi t·∫°o DenseNet201 trong context c·ªßa strategy\n",
    "with strategy.scope():\n",
    "    base_model_densenet201 = DenseNet201(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# N∆°i l∆∞u ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# L·∫∑p qua t·ª´ng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features with DenseNet201\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_densenet201(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# K·∫øt h·ª£p ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "features_array = np.concatenate(all_features, axis=0)  \n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61975c95-04ff-4ee1-bca8-2fbd5a4f1e0b",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chu·∫©n h√≥a L2 cho m·ªói vector (ƒë·ªô d√†i = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "\n",
    "# Chu·∫©n h√≥a vector ƒë·∫∑c tr∆∞ng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# K√≠ch th∆∞·ªõc vector ƒë·∫∑c tr∆∞ng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Kh·ªüi t·∫°o index FAISS s·ª≠ d·ª•ng kho·∫£ng c√°ch Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Th√™m vector ƒë√£ chu·∫©n h√≥a v√†o index\n",
    "index.add(normalized_features)\n",
    "print(\"ƒê√£ th√™m\", index.ntotal, \"vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\")\n",
    "\n",
    "# L∆∞u index v√† nh√£n\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b091d6d-b00a-4303-9e50-81df85cc8c6a",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c tham s·ªë\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.95\n",
    "\n",
    "# ==== Load m√¥ h√¨nh v√† FAISS index ====\n",
    "model = DenseNet201(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n FAISS index v√† labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y FAISS index t·∫°i: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y nh√£n t·∫°i: {label_path}\")\n",
    "\n",
    "# Load FAISS index v√† labels\n",
    "try:\n",
    "    # Load index tr·ª±c ti·∫øp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n",
    "    print(f\"   - S·ªë l∆∞·ª£ng vectors: {index.ntotal}\")\n",
    "    print(f\"   - K√≠ch th∆∞·ªõc vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi t·∫£i FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duy·ªát t·∫≠p test v√† l·∫•y t·∫•t c·∫£ ·∫£nh t·ª´ m·ªói th∆∞ m·ª•c ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # L·∫•y danh s√°ch t·∫•t c·∫£ ·∫£nh trong th∆∞ m·ª•c n√†y\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== D·ª± ƒëo√°n ====\n",
    "processing_times = []  # Danh s√°ch l∆∞u th·ªùi gian x·ª≠ l√Ω t·ª´ng ·∫£nh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # B·∫Øt ƒë·∫ßu ƒëo th·ªùi gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"‚ùå Folder kh√¥ng h·ª£p l·ªá: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Tr√≠ch ƒë·∫∑c tr∆∞ng v·ªõi m√¥ h√¨nh\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chu·∫©n h√≥a L2 ƒë·∫∑c tr∆∞ng m·∫´u truy v·∫•n ƒë·ªÉ ph√π h·ª£p v·ªõi vector ƒë√£ chu·∫©n h√≥a trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "        \n",
    "        # ƒê·∫£m b·∫£o ƒë·∫∑c tr∆∞ng c√≥ ƒë·ªãnh d·∫°ng ph√π h·ª£p sau khi chu·∫©n h√≥a\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # T√¨m ki·∫øm k=1 ƒëi·ªÉm g·∫ßn nh·∫•t trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Ki·ªÉm tra similarity score t·ª´ kho·∫£ng c√°ch L2 v·ªõi vector ƒë√£ chu·∫©n h√≥a\n",
    "        # V·ªõi vector ƒë√£ chu·∫©n h√≥a L2, kho·∫£ng c√°ch Euclidean b√¨nh ph∆∞∆°ng (D) v√† cosine similarity c√≥ quan h·ªá:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Kho·∫£ng c√°ch FAISS l√† b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuy·ªÉn ƒë·ªïi kho·∫£ng c√°ch th√†nh cosine similarity\n",
    "        \n",
    "        # N·∫øu similarity d∆∞·ªõi ng∆∞·ª°ng, g√°n nh√£n \"Kh√°c\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nh√£n \"Kh√°c\"\n",
    "        else:\n",
    "            # L·∫•y nh√£n d·ª± ƒëo√°n t·ª´ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n v√† nh√£n th·ª±c t·∫ø\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # K·∫øt th√∫c ƒëo th·ªùi gian v√† l∆∞u l·∫°i\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In th√¥ng tin cho debug (c√≥ th·ªÉ b·ªè comment n·∫øu c·∫ßn ki·ªÉm tra)\n",
    "        # print(f\"·∫¢nh: {img_path}, Similarity: {similarity_score:.4f}, Nh√£n d·ª± ƒëo√°n: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói ·∫£nh {img_path}: {e}\")\n",
    "        print(f\"  H√¨nh d·∫°ng ƒë·∫∑c tr∆∞ng: {feature.shape}, Ki·ªÉu d·ªØ li·ªáu: {feature.dtype}\")\n",
    "\n",
    "# ==== ƒê√°nh gi√° ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√°!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n‚úÖ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"‚úÖ ƒê√∫ng: {np.sum(y_true == y_pred)} / ‚ùå Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # T√≠nh th·ªùi gian x·ª≠ l√Ω trung b√¨nh cho m·ªôt ·∫£nh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    print(f\"‚è±Ô∏è T·ªïng th·ªùi gian x·ª≠ l√Ω: {sum(processing_times):.2f} gi√¢y cho {len(processing_times)} ·∫£nh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\nüñºÔ∏è Confusion matrix ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\nüìÑ Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuy·ªÉn b√°o c√°o ph√¢n lo·∫°i th√†nh DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hi·ªÉn th·ªã c√°c tham s·ªë chung cho to√†n b·ªô ch∆∞∆°ng tr√¨nh\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\nüìù C√°c tham s·ªë ƒë√°nh gi√° chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    \n",
    "    # L∆∞u b√°o c√°o chi ti·∫øt v√†o t·ªáp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{csv_output_path}'\")\n",
    "    \n",
    "    # L∆∞u c√°c tham s·ªë ƒë√°nh gi√° chung v√†o t·ªáp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\nüìä C√°c tham s·ªë ƒë√°nh gi√° chung ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # L∆∞u c·∫£ b√°o c√°o ph√¢n lo·∫°i v√† c√°c tham s·ªë chung v√†o m·ªôt t·ªáp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4c1f5b-1c5f-4cb7-ae5d-1ce839ca9e46",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a410c7-98f2-4625-a68d-6cc67ac07995",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# S·ªë batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    base_model_vgg16 = VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# N∆°i l∆∞u ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_vgg16(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# N·ªëi ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62778bae-b9f0-40b8-9512-ef2619a71b3e",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chu·∫©n h√≥a L2 cho m·ªói vector (ƒë·ªô d√†i = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "\n",
    "# Chu·∫©n h√≥a vector ƒë·∫∑c tr∆∞ng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# K√≠ch th∆∞·ªõc vector ƒë·∫∑c tr∆∞ng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Kh·ªüi t·∫°o index FAISS s·ª≠ d·ª•ng kho·∫£ng c√°ch Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Th√™m vector ƒë√£ chu·∫©n h√≥a v√†o index\n",
    "index.add(normalized_features)\n",
    "print(\"ƒê√£ th√™m\", index.ntotal, \"vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\")\n",
    "\n",
    "# L∆∞u index v√† nh√£n\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc7acc-49d8-48e0-a8c7-9612e761d740",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c tham s·ªë\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.95\n",
    "\n",
    "# ==== Load m√¥ h√¨nh v√† FAISS index ====\n",
    "model = VGG16(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n FAISS index v√† labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y FAISS index t·∫°i: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y nh√£n t·∫°i: {label_path}\")\n",
    "\n",
    "# Load FAISS index v√† labels\n",
    "try:\n",
    "    # Load index tr·ª±c ti·∫øp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n",
    "    print(f\"   - S·ªë l∆∞·ª£ng vectors: {index.ntotal}\")\n",
    "    print(f\"   - K√≠ch th∆∞·ªõc vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi t·∫£i FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duy·ªát t·∫≠p test v√† l·∫•y t·∫•t c·∫£ ·∫£nh t·ª´ m·ªói th∆∞ m·ª•c ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # L·∫•y danh s√°ch t·∫•t c·∫£ ·∫£nh trong th∆∞ m·ª•c n√†y\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== D·ª± ƒëo√°n ====\n",
    "processing_times = []  # Danh s√°ch l∆∞u th·ªùi gian x·ª≠ l√Ω t·ª´ng ·∫£nh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # B·∫Øt ƒë·∫ßu ƒëo th·ªùi gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"‚ùå Folder kh√¥ng h·ª£p l·ªá: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Tr√≠ch ƒë·∫∑c tr∆∞ng v·ªõi m√¥ h√¨nh\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chu·∫©n h√≥a L2 ƒë·∫∑c tr∆∞ng m·∫´u truy v·∫•n ƒë·ªÉ ph√π h·ª£p v·ªõi vector ƒë√£ chu·∫©n h√≥a trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "        \n",
    "        # ƒê·∫£m b·∫£o ƒë·∫∑c tr∆∞ng c√≥ ƒë·ªãnh d·∫°ng ph√π h·ª£p sau khi chu·∫©n h√≥a\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # T√¨m ki·∫øm k=1 ƒëi·ªÉm g·∫ßn nh·∫•t trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Ki·ªÉm tra similarity score t·ª´ kho·∫£ng c√°ch L2 v·ªõi vector ƒë√£ chu·∫©n h√≥a\n",
    "        # V·ªõi vector ƒë√£ chu·∫©n h√≥a L2, kho·∫£ng c√°ch Euclidean b√¨nh ph∆∞∆°ng (D) v√† cosine similarity c√≥ quan h·ªá:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Kho·∫£ng c√°ch FAISS l√† b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuy·ªÉn ƒë·ªïi kho·∫£ng c√°ch th√†nh cosine similarity\n",
    "        \n",
    "        # N·∫øu similarity d∆∞·ªõi ng∆∞·ª°ng, g√°n nh√£n \"Kh√°c\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nh√£n \"Kh√°c\"\n",
    "        else:\n",
    "            # L·∫•y nh√£n d·ª± ƒëo√°n t·ª´ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n v√† nh√£n th·ª±c t·∫ø\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # K·∫øt th√∫c ƒëo th·ªùi gian v√† l∆∞u l·∫°i\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In th√¥ng tin cho debug (c√≥ th·ªÉ b·ªè comment n·∫øu c·∫ßn ki·ªÉm tra)\n",
    "        # print(f\"·∫¢nh: {img_path}, Similarity: {similarity_score:.4f}, Nh√£n d·ª± ƒëo√°n: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói ·∫£nh {img_path}: {e}\")\n",
    "        print(f\"  H√¨nh d·∫°ng ƒë·∫∑c tr∆∞ng: {feature.shape}, Ki·ªÉu d·ªØ li·ªáu: {feature.dtype}\")\n",
    "\n",
    "# ==== ƒê√°nh gi√° ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√°!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n‚úÖ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"‚úÖ ƒê√∫ng: {np.sum(y_true == y_pred)} / ‚ùå Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # T√≠nh th·ªùi gian x·ª≠ l√Ω trung b√¨nh cho m·ªôt ·∫£nh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    print(f\"‚è±Ô∏è T·ªïng th·ªùi gian x·ª≠ l√Ω: {sum(processing_times):.2f} gi√¢y cho {len(processing_times)} ·∫£nh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\nüñºÔ∏è Confusion matrix ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\nüìÑ Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuy·ªÉn b√°o c√°o ph√¢n lo·∫°i th√†nh DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hi·ªÉn th·ªã c√°c tham s·ªë chung cho to√†n b·ªô ch∆∞∆°ng tr√¨nh\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\nüìù C√°c tham s·ªë ƒë√°nh gi√° chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    \n",
    "    # L∆∞u b√°o c√°o chi ti·∫øt v√†o t·ªáp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{csv_output_path}'\")\n",
    "    \n",
    "    # L∆∞u c√°c tham s·ªë ƒë√°nh gi√° chung v√†o t·ªáp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\nüìä C√°c tham s·ªë ƒë√°nh gi√° chung ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # L∆∞u c·∫£ b√°o c√°o ph√¢n lo·∫°i v√† c√°c tham s·ªë chung v√†o m·ªôt t·ªáp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca63d7-9e5c-41ee-b655-ce5dd2ed6629",
   "metadata": {},
   "source": [
    "# EfficientNetB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aa10a50-0dbb-4ad5-b588-5c9ae6e4a91f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T11:42:32.148693Z",
     "iopub.status.busy": "2025-07-04T11:42:32.148121Z",
     "iopub.status.idle": "2025-07-04T11:48:17.596725Z",
     "shell.execute_reply": "2025-07-04T11:48:17.595959Z",
     "shell.execute_reply.started": "2025-07-04T11:42:32.148670Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n",
      "\u001b[1m31790344/31790344\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features with EfficientNetB5:   0%|          | 0/928 [00:00<?, ?it/s]I0000 00:00:1751629356.211669      35 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "Extracting features with EfficientNetB5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 928/928 [05:42<00:00,  2.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB2\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "# S·ªë batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Kh·ªüi t·∫°o EfficientNet trong context c·ªßa strategy\n",
    "with strategy.scope():\n",
    "    base_model_efficientnetb2 = EfficientNetB2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# N∆°i l∆∞u ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# L·∫∑p qua t·ª´ng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features with EfficientNetB5\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_efficientnetb2(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# K·∫øt h·ª£p ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "869ff37e-37a8-4d45-9f90-6f14daeacb47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T11:48:18.688136Z",
     "iopub.status.busy": "2025-07-04T11:48:18.687858Z",
     "iopub.status.idle": "2025-07-04T11:48:19.296842Z",
     "shell.execute_reply": "2025-07-04T11:48:19.296034Z",
     "shell.execute_reply.started": "2025-07-04T11:48:18.688094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ th√™m 29693 vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chu·∫©n h√≥a L2 cho m·ªói vector (ƒë·ªô d√†i = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "\n",
    "# Chu·∫©n h√≥a vector ƒë·∫∑c tr∆∞ng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# K√≠ch th∆∞·ªõc vector ƒë·∫∑c tr∆∞ng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Kh·ªüi t·∫°o index FAISS s·ª≠ d·ª•ng kho·∫£ng c√°ch Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Th√™m vector ƒë√£ chu·∫©n h√≥a v√†o index\n",
    "index.add(normalized_features)\n",
    "print(\"ƒê√£ th√™m\", index.ntotal, \"vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\")\n",
    "\n",
    "# L∆∞u index v√† nh√£n\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db3f5e-b484-46b9-97e3-7e838b1bb836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T11:48:19.297848Z",
     "iopub.status.busy": "2025-07-04T11:48:19.297636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\n",
      "   - S·ªë l∆∞·ª£ng vectors: 29693\n",
      "   - K√≠ch th∆∞·ªõc vector: 1408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing with FAISS:   0%|          | 0/11565 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751629705.566745     113 service.cc:148] XLA service 0x7a6fd8001f70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1751629705.567602     113 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1751629710.614410     113 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Testing with FAISS:   9%|‚ñâ         | 1048/11565 [01:53<16:44, 10.47it/s]"
     ]
    }
   ],
   "source": [
    "# C√†i ƒë·∫∑t c√°c tham s·ªë\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.95\n",
    "\n",
    "# ==== Load m√¥ h√¨nh v√† FAISS index ====\n",
    "model = EfficientNetB2(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n FAISS index v√† labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y FAISS index t·∫°i: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y nh√£n t·∫°i: {label_path}\")\n",
    "\n",
    "# Load FAISS index v√† labels\n",
    "try:\n",
    "    # Load index tr·ª±c ti·∫øp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n",
    "    print(f\"   - S·ªë l∆∞·ª£ng vectors: {index.ntotal}\")\n",
    "    print(f\"   - K√≠ch th∆∞·ªõc vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi t·∫£i FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duy·ªát t·∫≠p test v√† l·∫•y t·∫•t c·∫£ ·∫£nh t·ª´ m·ªói th∆∞ m·ª•c ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # L·∫•y danh s√°ch t·∫•t c·∫£ ·∫£nh trong th∆∞ m·ª•c n√†y\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== D·ª± ƒëo√°n ====\n",
    "processing_times = []  # Danh s√°ch l∆∞u th·ªùi gian x·ª≠ l√Ω t·ª´ng ·∫£nh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # B·∫Øt ƒë·∫ßu ƒëo th·ªùi gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"‚ùå Folder kh√¥ng h·ª£p l·ªá: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Tr√≠ch ƒë·∫∑c tr∆∞ng v·ªõi m√¥ h√¨nh\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chu·∫©n h√≥a L2 ƒë·∫∑c tr∆∞ng m·∫´u truy v·∫•n ƒë·ªÉ ph√π h·ª£p v·ªõi vector ƒë√£ chu·∫©n h√≥a trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "        \n",
    "        # ƒê·∫£m b·∫£o ƒë·∫∑c tr∆∞ng c√≥ ƒë·ªãnh d·∫°ng ph√π h·ª£p sau khi chu·∫©n h√≥a\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # T√¨m ki·∫øm k=1 ƒëi·ªÉm g·∫ßn nh·∫•t trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Ki·ªÉm tra similarity score t·ª´ kho·∫£ng c√°ch L2 v·ªõi vector ƒë√£ chu·∫©n h√≥a\n",
    "        # V·ªõi vector ƒë√£ chu·∫©n h√≥a L2, kho·∫£ng c√°ch Euclidean b√¨nh ph∆∞∆°ng (D) v√† cosine similarity c√≥ quan h·ªá:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Kho·∫£ng c√°ch FAISS l√† b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuy·ªÉn ƒë·ªïi kho·∫£ng c√°ch th√†nh cosine similarity\n",
    "        \n",
    "        # N·∫øu similarity d∆∞·ªõi ng∆∞·ª°ng, g√°n nh√£n \"Kh√°c\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nh√£n \"Kh√°c\"\n",
    "        else:\n",
    "            # L·∫•y nh√£n d·ª± ƒëo√°n t·ª´ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n v√† nh√£n th·ª±c t·∫ø\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # K·∫øt th√∫c ƒëo th·ªùi gian v√† l∆∞u l·∫°i\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In th√¥ng tin cho debug (c√≥ th·ªÉ b·ªè comment n·∫øu c·∫ßn ki·ªÉm tra)\n",
    "        # print(f\"·∫¢nh: {img_path}, Similarity: {similarity_score:.4f}, Nh√£n d·ª± ƒëo√°n: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói ·∫£nh {img_path}: {e}\")\n",
    "        print(f\"  H√¨nh d·∫°ng ƒë·∫∑c tr∆∞ng: {feature.shape}, Ki·ªÉu d·ªØ li·ªáu: {feature.dtype}\")\n",
    "\n",
    "# ==== ƒê√°nh gi√° ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√°!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n‚úÖ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"‚úÖ ƒê√∫ng: {np.sum(y_true == y_pred)} / ‚ùå Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # T√≠nh th·ªùi gian x·ª≠ l√Ω trung b√¨nh cho m·ªôt ·∫£nh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    print(f\"‚è±Ô∏è T·ªïng th·ªùi gian x·ª≠ l√Ω: {sum(processing_times):.2f} gi√¢y cho {len(processing_times)} ·∫£nh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\nüñºÔ∏è Confusion matrix ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\nüìÑ Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuy·ªÉn b√°o c√°o ph√¢n lo·∫°i th√†nh DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hi·ªÉn th·ªã c√°c tham s·ªë chung cho to√†n b·ªô ch∆∞∆°ng tr√¨nh\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\nüìù C√°c tham s·ªë ƒë√°nh gi√° chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    \n",
    "    # L∆∞u b√°o c√°o chi ti·∫øt v√†o t·ªáp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{csv_output_path}'\")\n",
    "    \n",
    "    # L∆∞u c√°c tham s·ªë ƒë√°nh gi√° chung v√†o t·ªáp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\nüìä C√°c tham s·ªë ƒë√°nh gi√° chung ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # L∆∞u c·∫£ b√°o c√°o ph√¢n lo·∫°i v√† c√°c tham s·ªë chung v√†o m·ªôt t·ªáp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452cb37-3beb-4f37-b802-5718e09c580a",
   "metadata": {},
   "source": [
    "# Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48406a55-2731-483b-8fa6-3c5c967ddaa3",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "\n",
    "# S·ªë batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Kh·ªüi t·∫°o Xception trong context c·ªßa strategy\n",
    "with strategy.scope():\n",
    "    base_model_xception = Xception(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# N∆°i l∆∞u ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# L·∫∑p qua t·ª´ng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features with Xception\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_xception(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# K·∫øt h·ª£p ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd8819-8f4e-4317-9332-2f2d05206808",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chu·∫©n h√≥a L2 cho m·ªói vector (ƒë·ªô d√†i = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "\n",
    "# Chu·∫©n h√≥a vector ƒë·∫∑c tr∆∞ng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# K√≠ch th∆∞·ªõc vector ƒë·∫∑c tr∆∞ng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Kh·ªüi t·∫°o index FAISS s·ª≠ d·ª•ng kho·∫£ng c√°ch Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Th√™m vector ƒë√£ chu·∫©n h√≥a v√†o index\n",
    "index.add(normalized_features)\n",
    "print(\"ƒê√£ th√™m\", index.ntotal, \"vector ƒë√£ chu·∫©n h√≥a v√†o FAISS index.\")\n",
    "\n",
    "# L∆∞u index v√† nh√£n\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c99e8-70a7-4d11-894e-996bd73102a4",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c tham s·ªë\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.95\n",
    "\n",
    "# ==== Load m√¥ h√¨nh v√† FAISS index ====\n",
    "model = Xception(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n FAISS index v√† labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y FAISS index t·∫°i: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y nh√£n t·∫°i: {label_path}\")\n",
    "\n",
    "# Load FAISS index v√† labels\n",
    "try:\n",
    "    # Load index tr·ª±c ti·∫øp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!\")\n",
    "    print(f\"   - S·ªë l∆∞·ª£ng vectors: {index.ntotal}\")\n",
    "    print(f\"   - K√≠ch th∆∞·ªõc vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi t·∫£i FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duy·ªát t·∫≠p test v√† l·∫•y t·∫•t c·∫£ ·∫£nh t·ª´ m·ªói th∆∞ m·ª•c ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # L·∫•y danh s√°ch t·∫•t c·∫£ ·∫£nh trong th∆∞ m·ª•c n√†y\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== D·ª± ƒëo√°n ====\n",
    "processing_times = []  # Danh s√°ch l∆∞u th·ªùi gian x·ª≠ l√Ω t·ª´ng ·∫£nh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # B·∫Øt ƒë·∫ßu ƒëo th·ªùi gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"‚ùå Folder kh√¥ng h·ª£p l·ªá: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Tr√≠ch ƒë·∫∑c tr∆∞ng v·ªõi m√¥ h√¨nh\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chu·∫©n h√≥a L2 ƒë·∫∑c tr∆∞ng m·∫´u truy v·∫•n ƒë·ªÉ ph√π h·ª£p v·ªõi vector ƒë√£ chu·∫©n h√≥a trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Th√™m epsilon ƒë·ªÉ tr√°nh chia cho 0\n",
    "        \n",
    "        # ƒê·∫£m b·∫£o ƒë·∫∑c tr∆∞ng c√≥ ƒë·ªãnh d·∫°ng ph√π h·ª£p sau khi chu·∫©n h√≥a\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # T√¨m ki·∫øm k=1 ƒëi·ªÉm g·∫ßn nh·∫•t trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Ki·ªÉm tra similarity score t·ª´ kho·∫£ng c√°ch L2 v·ªõi vector ƒë√£ chu·∫©n h√≥a\n",
    "        # V·ªõi vector ƒë√£ chu·∫©n h√≥a L2, kho·∫£ng c√°ch Euclidean b√¨nh ph∆∞∆°ng (D) v√† cosine similarity c√≥ quan h·ªá:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Kho·∫£ng c√°ch FAISS l√† b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuy·ªÉn ƒë·ªïi kho·∫£ng c√°ch th√†nh cosine similarity\n",
    "        \n",
    "        # N·∫øu similarity d∆∞·ªõi ng∆∞·ª°ng, g√°n nh√£n \"Kh√°c\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nh√£n \"Kh√°c\"\n",
    "        else:\n",
    "            # L·∫•y nh√£n d·ª± ƒëo√°n t·ª´ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n v√† nh√£n th·ª±c t·∫ø\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # K·∫øt th√∫c ƒëo th·ªùi gian v√† l∆∞u l·∫°i\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In th√¥ng tin cho debug (c√≥ th·ªÉ b·ªè comment n·∫øu c·∫ßn ki·ªÉm tra)\n",
    "        # print(f\"·∫¢nh: {img_path}, Similarity: {similarity_score:.4f}, Nh√£n d·ª± ƒëo√°n: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói ·∫£nh {img_path}: {e}\")\n",
    "        print(f\"  H√¨nh d·∫°ng ƒë·∫∑c tr∆∞ng: {feature.shape}, Ki·ªÉu d·ªØ li·ªáu: {feature.dtype}\")\n",
    "\n",
    "# ==== ƒê√°nh gi√° ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√°!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n‚úÖ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"‚úÖ ƒê√∫ng: {np.sum(y_true == y_pred)} / ‚ùå Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # T√≠nh th·ªùi gian x·ª≠ l√Ω trung b√¨nh cho m·ªôt ·∫£nh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    print(f\"‚è±Ô∏è T·ªïng th·ªùi gian x·ª≠ l√Ω: {sum(processing_times):.2f} gi√¢y cho {len(processing_times)} ·∫£nh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\nüñºÔ∏è Confusion matrix ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\nüìÑ Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuy·ªÉn b√°o c√°o ph√¢n lo·∫°i th√†nh DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hi·ªÉn th·ªã c√°c tham s·ªë chung cho to√†n b·ªô ch∆∞∆°ng tr√¨nh\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\nüìù C√°c tham s·ªë ƒë√°nh gi√° chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Th·ªùi gian x·ª≠ l√Ω trung b√¨nh: {avg_processing_time:.4f} gi√¢y/·∫£nh\")\n",
    "    \n",
    "    # L∆∞u b√°o c√°o chi ti·∫øt v√†o t·ªáp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{csv_output_path}'\")\n",
    "    \n",
    "    # L∆∞u c√°c tham s·ªë ƒë√°nh gi√° chung v√†o t·ªáp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\nüìä C√°c tham s·ªë ƒë√°nh gi√° chung ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # L∆∞u c·∫£ b√°o c√°o ph√¢n lo·∫°i v√† c√°c tham s·ªë chung v√†o m·ªôt t·ªáp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'final_classification_report_faiss.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7799130,
     "sourceId": 12369593,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 70.338084,
   "end_time": "2025-05-11T02:57:09.159799",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-11T02:55:58.821715",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
