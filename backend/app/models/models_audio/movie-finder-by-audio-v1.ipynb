{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ccb1bca",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-11T02:56:04.256085Z",
     "iopub.status.busy": "2025-05-11T02:56:04.255672Z",
     "iopub.status.idle": "2025-05-11T02:56:06.475443Z",
     "shell.execute_reply": "2025-05-11T02:56:06.474263Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 2.23638,
     "end_time": "2025-05-11T02:56:06.477304",
     "exception": false,
     "start_time": "2025-05-11T02:56:04.240924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5901459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:47:53.604835Z",
     "iopub.status.busy": "2025-07-05T06:47:53.604682Z",
     "iopub.status.idle": "2025-07-05T06:48:07.753446Z",
     "shell.execute_reply": "2025-07-05T06:48:07.752491Z",
     "shell.execute_reply.started": "2025-07-05T06:47:53.604820Z"
    },
    "papermill": {
     "duration": 37.164931,
     "end_time": "2025-05-11T02:56:43.652725",
     "exception": false,
     "start_time": "2025-05-11T02:56:06.487794",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q faiss-cpu\n",
    "!pip install -q tensorflow==2.18.0\n",
    "!pip install -q skl2onnx onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0303f6a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:48:11.662101Z",
     "iopub.status.busy": "2025-07-05T06:48:11.661622Z",
     "iopub.status.idle": "2025-07-05T06:48:27.886260Z",
     "shell.execute_reply": "2025-07-05T06:48:27.885514Z",
     "shell.execute_reply.started": "2025-07-05T06:48:11.662068Z"
    },
    "papermill": {
     "duration": 17.588955,
     "end_time": "2025-05-11T02:57:01.252620",
     "exception": false,
     "start_time": "2025-05-11T02:56:43.663665",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 06:48:13.672056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751698093.912022      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751698093.986571      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "Tìm thấy 1 GPU:\n",
      "  GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Số lượng GPU vật lý: 1, số lượng GPU logic: 1\n",
      "Danh sách GPU: ['/device:GPU:0']\n",
      "CUDA version: 12.5.1\n",
      "cuDNN version: 9\n",
      "\n",
      "Xác nhận GPU đang hoạt động bằng phép tính nhỏ:\n",
      "Tính toán trên GPU: [[19. 22.]\n",
      " [43. 50.]]\n",
      "Đang chạy trên thiết bị: /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751698107.778276      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "I0000 00:00:1751698107.780259      35 gpu_device.cc:2022] Created device /device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Kiểm tra phiên bản TensorFlow\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Cấu hình memory growth để sử dụng GPU hiệu quả\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"Tìm thấy {len(gpus)} GPU:\")\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {i}: {gpu}\")\n",
    "    \n",
    "    # Cấu hình memory growth\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # Một số tùy chọn để tối ưu hiệu suất cho GPU T4\n",
    "        os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "        os.environ['TF_GPU_THREAD_COUNT'] = '2'  # Tương ứng với số GPU\n",
    "        \n",
    "        # Hiển thị các GPU logic\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"Số lượng GPU vật lý: {len(gpus)}, số lượng GPU logic: {len(logical_gpus)}\")\n",
    "        \n",
    "        # Thông tin chi tiết về GPU\n",
    "        from tensorflow.python.client import device_lib\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        gpu_list = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "        print(f\"Danh sách GPU: {gpu_list}\")\n",
    "        \n",
    "        # Hiển thị thông tin CUDA và cuDNN\n",
    "        build_info = tf.sysconfig.get_build_info()\n",
    "        print(f\"CUDA version: {build_info.get('cuda_version', 'N/A')}\")\n",
    "        print(f\"cuDNN version: {build_info.get('cudnn_version', 'N/A')}\")\n",
    "        \n",
    "        # Kiểm tra xem GPU có thực sự được sử dụng hay không\n",
    "        print(\"\\nXác nhận GPU đang hoạt động bằng phép tính nhỏ:\")\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "            b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(f\"Tính toán trên GPU: {c}\")\n",
    "            print(f\"Đang chạy trên thiết bị: {c.device}\")\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        print(f\"Lỗi khi cấu hình GPU: {e}\")\n",
    "else:\n",
    "    print(\"Không tìm thấy GPU! Đang sử dụng CPU.\")\n",
    "    \n",
    "    # Kiểm tra thông tin CPU\n",
    "    cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "    print(f\"Tìm thấy {len(cpu_devices)} CPU: {cpu_devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d400a987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:48:36.304189Z",
     "iopub.status.busy": "2025-07-05T06:48:36.303452Z",
     "iopub.status.idle": "2025-07-05T06:48:37.730737Z",
     "shell.execute_reply": "2025-07-05T06:48:37.729856Z",
     "shell.execute_reply.started": "2025-07-05T06:48:36.304164Z"
    },
    "papermill": {
     "duration": 3.495843,
     "end_time": "2025-05-11T02:57:04.760578",
     "exception": false,
     "start_time": "2025-05-11T02:57:01.264735",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import gc\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm \n",
    "import shutil\n",
    "\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, Flatten, Dropout, AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, Resizing\n",
    "from tensorflow.keras.layers import MaxPooling2D, Activation, BatchNormalization, Attention, Reshape, RepeatVector, Lambda, Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import joblib\n",
    "\n",
    "import onnxruntime as ort\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "# Bỏ qua các cảnh báo\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# In phiên bản TensorFlow hiện tại\n",
    "print('TensorFlow Version ' + tf.__version__)\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    # Thiết lập seed để đảm bảo tính tái lập (reproducibility)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# Gọi hàm seed_everything để thiết lập seed mặc định\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16300f50",
   "metadata": {
    "papermill": {
     "duration": 0.011256,
     "end_time": "2025-05-11T02:57:04.783743",
     "exception": false,
     "start_time": "2025-05-11T02:57:04.772487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trực quan dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d03b50d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:48:46.378635Z",
     "iopub.status.busy": "2025-07-05T06:48:46.377615Z",
     "iopub.status.idle": "2025-07-05T06:51:04.906489Z",
     "shell.execute_reply": "2025-07-05T06:51:04.905714Z",
     "shell.execute_reply.started": "2025-07-05T06:48:46.378589Z"
    },
    "papermill": {
     "duration": 0.023891,
     "end_time": "2025-05-11T02:57:04.819087",
     "exception": false,
     "start_time": "2025-05-11T02:57:04.795196",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Thư mục Train====\n",
      "==> Tổng số ảnh: 29693\n",
      "====Thư mục Test====\n",
      "==> Tổng số ảnh: 11565\n"
     ]
    }
   ],
   "source": [
    "def count_images_per_folder(root_dir, image_extensions=None):\n",
    "    if image_extensions is None:\n",
    "        image_extensions = ['.jpg']\n",
    "\n",
    "    total = 0\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        count = sum(1 for file in files if any(file.lower().endswith(ext) for ext in image_extensions))\n",
    "        if count > 0:\n",
    "            relative_path = os.path.relpath(subdir, root_dir)\n",
    "            # print(f\"Thư mục '{relative_path}': {count} ảnh\")\n",
    "            total += count\n",
    "\n",
    "    print(f\"==> Tổng số ảnh: {total}\")\n",
    "\n",
    "print(\"====Thư mục Train====\")\n",
    "folder_train_path = '/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Train'\n",
    "count_images_per_folder(folder_train_path)\n",
    "\n",
    "print(\"====Thư mục Test====\")\n",
    "folder_test_path = '/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Test'\n",
    "count_images_per_folder(folder_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c03f6",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-11T02:57:04.844013Z",
     "iopub.status.busy": "2025-05-11T02:57:04.843197Z",
     "iopub.status.idle": "2025-05-11T02:57:05.173181Z",
     "shell.execute_reply": "2025-05-11T02:57:05.172139Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.343959,
     "end_time": "2025-05-11T02:57:05.174777",
     "exception": false,
     "start_time": "2025-05-11T02:57:04.830818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_images_per_folder(root_dir, image_extensions=None):\n",
    "    if image_extensions is None:\n",
    "        image_extensions = ['.jpg']\n",
    "\n",
    "    folder_counts = {}\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        if subdir == root_dir:\n",
    "            continue  # bỏ qua thư mục gốc\n",
    "        count = sum(1 for file in files if any(file.lower().endswith(ext) for ext in image_extensions))\n",
    "        if count > 0:\n",
    "            folder_name = os.path.basename(subdir)\n",
    "            folder_counts[folder_name] = count\n",
    "\n",
    "    return folder_counts\n",
    "\n",
    "def plot_image_counts(folder_counts):\n",
    "    folders = list(folder_counts.keys())\n",
    "    counts = list(folder_counts.values())\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.bar(folders, counts, color='skyblue')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('Tên thư mục con')\n",
    "    plt.ylabel('Số lượng ảnh')\n",
    "    plt.title('Số lượng ảnh trong từng thư mục con')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Tập Train\n",
    "counts = count_images_per_folder(folder_train_path)\n",
    "plot_image_counts(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba3a3b5",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-11T02:57:05.201042Z",
     "iopub.status.busy": "2025-05-11T02:57:05.200540Z",
     "iopub.status.idle": "2025-05-11T02:57:05.424858Z",
     "shell.execute_reply": "2025-05-11T02:57:05.423957Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.239196,
     "end_time": "2025-05-11T02:57:05.426207",
     "exception": false,
     "start_time": "2025-05-11T02:57:05.187011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tập Test\n",
    "counts = count_images_per_folder(folder_test_path)\n",
    "plot_image_counts(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf24266",
   "metadata": {
    "papermill": {
     "duration": 0.012064,
     "end_time": "2025-05-11T02:57:05.450728",
     "exception": false,
     "start_time": "2025-05-11T02:57:05.438664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tạo tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eafdbefb-6824-4ff5-af21-3dda3b304fb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:56:46.732348Z",
     "iopub.status.busy": "2025-07-05T06:56:46.731644Z",
     "iopub.status.idle": "2025-07-05T06:56:46.737740Z",
     "shell.execute_reply": "2025-07-05T06:56:46.737188Z",
     "shell.execute_reply.started": "2025-07-05T06:56:46.732311Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "n_classes = 45\n",
    "batch_size = 32\n",
    "\n",
    "classes_train = {\n",
    "    1: \"21_Ngay_Yeu_Em\",\n",
    "    2: \"4_Nam_2_Chang_1_Tinh_Yeu\",\n",
    "    3: \"An_Tet_Ben_Con\",\n",
    "    4: \"Bay_Ngot_Ngao\",\n",
    "    5: \"Benh_Vien_Ma\",\n",
    "    6: \"Bi_Mat_Lai_Bi_Mat\",\n",
    "    7: \"Bi_Mat_Trong_Suong_Mu\",\n",
    "    8: \"Bo_Tu_Oan_Gia\",\n",
    "    9: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    10: \"Chu_Tich_Giao_Hang\",\n",
    "    11: \"Chuyen_Tet\",\n",
    "    12: \"Co_Ba_Sai_Gon\",\n",
    "    13: \"Dao_Pho_Va_Piano\",\n",
    "    14: \"Dat_Rung_Phuong_Nam\",\n",
    "    15: \"Dia_Dao\",\n",
    "    16: \"Dinh_Menh_Thien_Y\",\n",
    "    17: \"Doi_Mat_Am_Duong\",\n",
    "    18: \"Em_Chua_18\",\n",
    "    19: \"Em_La_Cua_Em\",\n",
    "    20: \"Gai_Gia_Lam_Chieu_3\",\n",
    "    21: \"Gia_Ngheo_Gap_Phat\",\n",
    "    22: \"Hem_Cut\",\n",
    "    23: \"Hoan_Doi\",\n",
    "    24: \"Ke_An_Danh\",\n",
    "    25: \"Ke_An_Hon\",\n",
    "    26: \"Lam_Giau_Voi_Ma\",\n",
    "    27: \"Lat_Mat_1\",\n",
    "    28: \"Lo_Mat\",\n",
    "    29: \"Ma_Da\",\n",
    "    30: \"Mat_Biec\",\n",
    "    31: \"Nghe_Sieu_De\",\n",
    "    32: \"Nhung_Nu_Hon_Ruc_Ro\",\n",
    "    33: \"Ong_Ngoai_Tuoi_30\",\n",
    "    34: \"Phap_Su_Tap_Su\",\n",
    "    35: \"Quy_Cau\",\n",
    "    36: \"Quy_Co_Thua_Ke\",\n",
    "    37: \"Ra_Mat_Gia_Tien\",\n",
    "    38: \"Sieu_Lua_Gap_Sieu_Lay\",\n",
    "    39: \"Sieu_Tro_Ly\",\n",
    "    40: \"Tam_Cam_Chuyen_Chua_Ke\",\n",
    "    41: \"Taxi_Em_Ten_Gi\",\n",
    "    42: \"The_Call\",\n",
    "    43: \"Thien_Menh_Anh_Hung\",\n",
    "    44: \"Tieu_Thu_Va_Ba_Dau_Gau\",\n",
    "    45: \"Tren_Ban_Nhau_Duoi_Ban_Muu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8788e693",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:56:42.879376Z",
     "iopub.status.busy": "2025-07-05T06:56:42.878450Z",
     "iopub.status.idle": "2025-07-05T06:56:42.884619Z",
     "shell.execute_reply": "2025-07-05T06:56:42.883856Z",
     "shell.execute_reply.started": "2025-07-05T06:56:42.879344Z"
    },
    "papermill": {
     "duration": 0.02228,
     "end_time": "2025-05-11T02:57:05.485554",
     "exception": false,
     "start_time": "2025-05-11T02:57:05.463274",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classes_test = {\n",
    "    1: \"21_Ngay_Yeu_Em\",\n",
    "    2: \"4_Nam_2_Chang_1_Tinh_Yeu\",\n",
    "    3: \"An_Tet_Ben_Con\",\n",
    "    4: \"Bay_Ngot_Ngao\",\n",
    "    5: \"Benh_Vien_Ma\",\n",
    "    6: \"Bi_Mat_Lai_Bi_Mat\",\n",
    "    7: \"Bi_Mat_Trong_Suong_Mu\",\n",
    "    8: \"Bo_Tu_Oan_Gia\",\n",
    "    9: \"Cho_Em_Den_Ngay_Mai\",\n",
    "    10: \"Chu_Tich_Giao_Hang\",\n",
    "    11: \"Chuyen_Tet\",\n",
    "    12: \"Co_Ba_Sai_Gon\",\n",
    "    13: \"Dao_Pho_Va_Piano\",\n",
    "    14: \"Dat_Rung_Phuong_Nam\",\n",
    "    15: \"Dia_Dao\",\n",
    "    16: \"Dinh_Menh_Thien_Y\",\n",
    "    17: \"Doi_Mat_Am_Duong\",\n",
    "    18: \"Em_Chua_18\",\n",
    "    19: \"Em_La_Cua_Em\",\n",
    "    20: \"Gai_Gia_Lam_Chieu_3\",\n",
    "    21: \"Gia_Ngheo_Gap_Phat\",\n",
    "    22: \"Hem_Cut\",\n",
    "    23: \"Hoan_Doi\",\n",
    "    24: \"Ke_An_Danh\",\n",
    "    25: \"Ke_An_Hon\",\n",
    "    26: \"Lam_Giau_Voi_Ma\",\n",
    "    27: \"Lat_Mat_1\",\n",
    "    28: \"Lo_Mat\",\n",
    "    29: \"Ma_Da\",\n",
    "    30: \"Mat_Biec\",\n",
    "    31: \"Nghe_Sieu_De\",\n",
    "    32: \"Nhung_Nu_Hon_Ruc_Ro\",\n",
    "    33: \"Ong_Ngoai_Tuoi_30\",\n",
    "    34: \"Phap_Su_Tap_Su\",\n",
    "    35: \"Quy_Cau\",\n",
    "    36: \"Quy_Co_Thua_Ke\",\n",
    "    37: \"Ra_Mat_Gia_Tien\",\n",
    "    38: \"Sieu_Lua_Gap_Sieu_Lay\",\n",
    "    39: \"Sieu_Tro_Ly\",\n",
    "    40: \"Tam_Cam_Chuyen_Chua_Ke\",\n",
    "    41: \"Taxi_Em_Ten_Gi\",\n",
    "    42: \"The_Call\",\n",
    "    43: \"Thien_Menh_Anh_Hung\",\n",
    "    44: \"Tieu_Thu_Va_Ba_Dau_Gau\",\n",
    "    45: \"Tren_Ban_Nhau_Duoi_Ban_Muu\",\n",
    "    46: \"Khac\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e712e4e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:51:20.006849Z",
     "iopub.status.busy": "2025-07-05T06:51:20.006582Z",
     "iopub.status.idle": "2025-07-05T06:51:24.668796Z",
     "shell.execute_reply": "2025-07-05T06:51:24.668150Z",
     "shell.execute_reply.started": "2025-07-05T06:51:20.006831Z"
    },
    "papermill": {
     "duration": 0.399419,
     "end_time": "2025-05-11T02:57:05.897695",
     "exception": true,
     "start_time": "2025-05-11T02:57:05.498276",
     "status": "failed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29693 files belonging to 45 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = '/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Train'\n",
    "\n",
    "train_df = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',  \n",
    "    seed=1,\n",
    "    image_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c786449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:08:22.399474Z",
     "iopub.status.busy": "2025-05-10T13:08:22.398955Z",
     "iopub.status.idle": "2025-05-10T13:08:22.882416Z",
     "shell.execute_reply": "2025-05-10T13:08:22.881644Z",
     "shell.execute_reply.started": "2025-05-10T13:08:22.399444Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_df))  \n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(5, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for img, ax in zip(images[:9], axes): \n",
    "    ax.imshow(img.numpy().astype(\"uint8\"))  \n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc39b51",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Xóa file trong thư mục"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1296acbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:51:04.908033Z",
     "iopub.status.busy": "2025-07-05T06:51:04.907802Z",
     "iopub.status.idle": "2025-07-05T06:51:05.387710Z",
     "shell.execute_reply": "2025-07-05T06:51:05.387150Z",
     "shell.execute_reply.started": "2025-07-05T06:51:04.908015Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def delete_all_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # xóa file hoặc symbolic link\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # xóa thư mục và toàn bộ nội dung bên trong\n",
    "        except Exception as e:\n",
    "            print(f\"Không thể xóa {file_path}: {e}\")\n",
    "\n",
    "# Ví dụ:\n",
    "folder = \"/kaggle/working/\"\n",
    "delete_all_in_folder(folder)\n",
    "\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096302b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdad1feb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:52:26.257127Z",
     "iopub.status.busy": "2025-07-05T06:52:26.256640Z",
     "iopub.status.idle": "2025-07-05T06:55:53.169068Z",
     "shell.execute_reply": "2025-07-05T06:55:53.168261Z",
     "shell.execute_reply.started": "2025-07-05T06:52:26.257104Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 928/928 [03:25<00:00,  4.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Số batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Khởi tạo model trong context của strategy\n",
    "with strategy.scope():\n",
    "    base_model_resnet50 = ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Nơi lưu đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# Lặp qua từng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_resnet50(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# Đánh nhãn ứng với đặc trưng\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08b5c569",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T06:57:05.909843Z",
     "iopub.status.busy": "2025-07-05T06:57:05.909339Z",
     "iopub.status.idle": "2025-07-05T06:57:06.588001Z",
     "shell.execute_reply": "2025-07-05T06:57:06.587182Z",
     "shell.execute_reply.started": "2025-07-05T06:57:05.909823Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã thêm 29693 vector đã chuẩn hóa vào FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chuẩn hóa L2 cho mỗi vector (độ dài = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # thêm epsilon để tránh chia cho 0\n",
    "\n",
    "# Chuẩn hóa vector đặc trưng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# Kích thước vector đặc trưng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Khởi tạo index FAISS sử dụng khoảng cách Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Thêm vector đã chuẩn hóa vào index\n",
    "index.add(normalized_features)\n",
    "print(\"Đã thêm\", index.ntotal, \"vector đã chuẩn hóa vào FAISS index.\")\n",
    "\n",
    "# Lưu index và nhãn\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447fee46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T07:45:22.054281Z",
     "iopub.status.busy": "2025-07-05T07:45:22.053722Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index đã được tải thành công!\n",
      "   - Số lượng vectors: 29693\n",
      "   - Kích thước vector: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing with FAISS:  89%|████████▊ | 10240/11565 [18:21<02:18,  9.54it/s]"
     ]
    }
   ],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.95\n",
    "\n",
    "# ==== Load mô hình và FAISS index ====\n",
    "# Load mô hình ResNet50 \n",
    "model = ResNet50(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn FAISS index và labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"❌ Không tìm thấy FAISS index tại: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"❌ Không tìm thấy nhãn tại: {label_path}\")\n",
    "\n",
    "# Load FAISS index và labels\n",
    "try:\n",
    "    # Load index trực tiếp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"✅ FAISS index đã được tải thành công!\")\n",
    "    print(f\"   - Số lượng vectors: {index.ntotal}\")\n",
    "    print(f\"   - Kích thước vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy tất cả ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # Lấy danh sách tất cả ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "processing_times = []  # Danh sách lưu thời gian xử lý từng ảnh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # Bắt đầu đo thời gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Trích đặc trưng với mô hình\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chuẩn hóa L2 đặc trưng mẫu truy vấn để phù hợp với vector đã chuẩn hóa trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Thêm epsilon để tránh chia cho 0\n",
    "        \n",
    "        # Đảm bảo đặc trưng có định dạng phù hợp sau khi chuẩn hóa\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # Tìm kiếm k=1 điểm gần nhất trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Kiểm tra similarity score từ khoảng cách L2 với vector đã chuẩn hóa\n",
    "        # Với vector đã chuẩn hóa L2, khoảng cách Euclidean bình phương (D) và cosine similarity có quan hệ:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Khoảng cách FAISS là bình phương khoảng cách Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuyển đổi khoảng cách thành cosine similarity\n",
    "        \n",
    "        # Nếu similarity dưới ngưỡng, gán nhãn \"Khác\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nhãn \"Khác\"\n",
    "        else:\n",
    "            # Lấy nhãn dự đoán từ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # Lưu kết quả dự đoán và nhãn thực tế\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # Kết thúc đo thời gian và lưu lại\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In thông tin cho debug (có thể bỏ comment nếu cần kiểm tra)\n",
    "        # print(f\"Ảnh: {img_path}, Similarity: {similarity_score:.4f}, Nhãn dự đoán: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # Tính thời gian xử lý trung bình cho một ảnh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n⏱️ Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    print(f\"⏱️ Tổng thời gian xử lý: {sum(processing_times):.2f} giây cho {len(processing_times)} ảnh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    \n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "    \n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e4d9d-ce7b-40da-89f0-7217f83be69d",
   "metadata": {},
   "source": [
    "# DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af818a74-cc8e-4698-8387-1cbcac5b2519",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Số batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Khởi tạo DenseNet201 trong context của strategy\n",
    "with strategy.scope():\n",
    "    base_model_densenet201 = DenseNet201(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Nơi lưu đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# Lặp qua từng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features with DenseNet201\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_densenet201(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# Kết hợp đặc trưng và nhãn\n",
    "features_array = np.concatenate(all_features, axis=0)  \n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61975c95-04ff-4ee1-bca8-2fbd5a4f1e0b",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chuẩn hóa L2 cho mỗi vector (độ dài = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # thêm epsilon để tránh chia cho 0\n",
    "\n",
    "# Chuẩn hóa vector đặc trưng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# Kích thước vector đặc trưng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Khởi tạo index FAISS sử dụng khoảng cách Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Thêm vector đã chuẩn hóa vào index\n",
    "index.add(normalized_features)\n",
    "print(\"Đã thêm\", index.ntotal, \"vector đã chuẩn hóa vào FAISS index.\")\n",
    "\n",
    "# Lưu index và nhãn\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b091d6d-b00a-4303-9e50-81df85cc8c6a",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.95\n",
    "\n",
    "# ==== Load mô hình và FAISS index ====\n",
    "model = DenseNet201(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn FAISS index và labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"❌ Không tìm thấy FAISS index tại: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"❌ Không tìm thấy nhãn tại: {label_path}\")\n",
    "\n",
    "# Load FAISS index và labels\n",
    "try:\n",
    "    # Load index trực tiếp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"✅ FAISS index đã được tải thành công!\")\n",
    "    print(f\"   - Số lượng vectors: {index.ntotal}\")\n",
    "    print(f\"   - Kích thước vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy tất cả ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # Lấy danh sách tất cả ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "processing_times = []  # Danh sách lưu thời gian xử lý từng ảnh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # Bắt đầu đo thời gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Trích đặc trưng với mô hình\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chuẩn hóa L2 đặc trưng mẫu truy vấn để phù hợp với vector đã chuẩn hóa trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Thêm epsilon để tránh chia cho 0\n",
    "        \n",
    "        # Đảm bảo đặc trưng có định dạng phù hợp sau khi chuẩn hóa\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # Tìm kiếm k=1 điểm gần nhất trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Kiểm tra similarity score từ khoảng cách L2 với vector đã chuẩn hóa\n",
    "        # Với vector đã chuẩn hóa L2, khoảng cách Euclidean bình phương (D) và cosine similarity có quan hệ:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Khoảng cách FAISS là bình phương khoảng cách Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuyển đổi khoảng cách thành cosine similarity\n",
    "        \n",
    "        # Nếu similarity dưới ngưỡng, gán nhãn \"Khác\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nhãn \"Khác\"\n",
    "        else:\n",
    "            # Lấy nhãn dự đoán từ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # Lưu kết quả dự đoán và nhãn thực tế\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # Kết thúc đo thời gian và lưu lại\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In thông tin cho debug (có thể bỏ comment nếu cần kiểm tra)\n",
    "        # print(f\"Ảnh: {img_path}, Similarity: {similarity_score:.4f}, Nhãn dự đoán: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # Tính thời gian xử lý trung bình cho một ảnh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n⏱️ Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    print(f\"⏱️ Tổng thời gian xử lý: {sum(processing_times):.2f} giây cho {len(processing_times)} ảnh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    \n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "    \n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4c1f5b-1c5f-4cb7-ae5d-1ce839ca9e46",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a410c7-98f2-4625-a68d-6cc67ac07995",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Số batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    base_model_vgg16 = VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Nơi lưu đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_vgg16(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# Nối đặc trưng và nhãn\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62778bae-b9f0-40b8-9512-ef2619a71b3e",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chuẩn hóa L2 cho mỗi vector (độ dài = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # thêm epsilon để tránh chia cho 0\n",
    "\n",
    "# Chuẩn hóa vector đặc trưng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# Kích thước vector đặc trưng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Khởi tạo index FAISS sử dụng khoảng cách Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Thêm vector đã chuẩn hóa vào index\n",
    "index.add(normalized_features)\n",
    "print(\"Đã thêm\", index.ntotal, \"vector đã chuẩn hóa vào FAISS index.\")\n",
    "\n",
    "# Lưu index và nhãn\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc7acc-49d8-48e0-a8c7-9612e761d740",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.95\n",
    "\n",
    "# ==== Load mô hình và FAISS index ====\n",
    "model = VGG16(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn FAISS index và labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"❌ Không tìm thấy FAISS index tại: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"❌ Không tìm thấy nhãn tại: {label_path}\")\n",
    "\n",
    "# Load FAISS index và labels\n",
    "try:\n",
    "    # Load index trực tiếp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"✅ FAISS index đã được tải thành công!\")\n",
    "    print(f\"   - Số lượng vectors: {index.ntotal}\")\n",
    "    print(f\"   - Kích thước vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy tất cả ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # Lấy danh sách tất cả ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "processing_times = []  # Danh sách lưu thời gian xử lý từng ảnh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # Bắt đầu đo thời gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Trích đặc trưng với mô hình\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chuẩn hóa L2 đặc trưng mẫu truy vấn để phù hợp với vector đã chuẩn hóa trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Thêm epsilon để tránh chia cho 0\n",
    "        \n",
    "        # Đảm bảo đặc trưng có định dạng phù hợp sau khi chuẩn hóa\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # Tìm kiếm k=1 điểm gần nhất trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Kiểm tra similarity score từ khoảng cách L2 với vector đã chuẩn hóa\n",
    "        # Với vector đã chuẩn hóa L2, khoảng cách Euclidean bình phương (D) và cosine similarity có quan hệ:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Khoảng cách FAISS là bình phương khoảng cách Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuyển đổi khoảng cách thành cosine similarity\n",
    "        \n",
    "        # Nếu similarity dưới ngưỡng, gán nhãn \"Khác\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nhãn \"Khác\"\n",
    "        else:\n",
    "            # Lấy nhãn dự đoán từ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # Lưu kết quả dự đoán và nhãn thực tế\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # Kết thúc đo thời gian và lưu lại\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In thông tin cho debug (có thể bỏ comment nếu cần kiểm tra)\n",
    "        # print(f\"Ảnh: {img_path}, Similarity: {similarity_score:.4f}, Nhãn dự đoán: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # Tính thời gian xử lý trung bình cho một ảnh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n⏱️ Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    print(f\"⏱️ Tổng thời gian xử lý: {sum(processing_times):.2f} giây cho {len(processing_times)} ảnh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    \n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "    \n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca63d7-9e5c-41ee-b655-ce5dd2ed6629",
   "metadata": {},
   "source": [
    "# EfficientNetB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aa10a50-0dbb-4ad5-b588-5c9ae6e4a91f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T11:42:32.148693Z",
     "iopub.status.busy": "2025-07-04T11:42:32.148121Z",
     "iopub.status.idle": "2025-07-04T11:48:17.596725Z",
     "shell.execute_reply": "2025-07-04T11:48:17.595959Z",
     "shell.execute_reply.started": "2025-07-04T11:42:32.148670Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n",
      "\u001b[1m31790344/31790344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features with EfficientNetB5:   0%|          | 0/928 [00:00<?, ?it/s]I0000 00:00:1751629356.211669      35 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "Extracting features with EfficientNetB5: 100%|██████████| 928/928 [05:42<00:00,  2.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB2\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "# Số batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Khởi tạo EfficientNet trong context của strategy\n",
    "with strategy.scope():\n",
    "    base_model_efficientnetb2 = EfficientNetB2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Nơi lưu đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# Lặp qua từng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features with EfficientNetB5\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_efficientnetb2(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# Kết hợp đặc trưng và nhãn\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "869ff37e-37a8-4d45-9f90-6f14daeacb47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T11:48:18.688136Z",
     "iopub.status.busy": "2025-07-04T11:48:18.687858Z",
     "iopub.status.idle": "2025-07-04T11:48:19.296842Z",
     "shell.execute_reply": "2025-07-04T11:48:19.296034Z",
     "shell.execute_reply.started": "2025-07-04T11:48:18.688094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã thêm 29693 vector đã chuẩn hóa vào FAISS index.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chuẩn hóa L2 cho mỗi vector (độ dài = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # thêm epsilon để tránh chia cho 0\n",
    "\n",
    "# Chuẩn hóa vector đặc trưng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# Kích thước vector đặc trưng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Khởi tạo index FAISS sử dụng khoảng cách Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Thêm vector đã chuẩn hóa vào index\n",
    "index.add(normalized_features)\n",
    "print(\"Đã thêm\", index.ntotal, \"vector đã chuẩn hóa vào FAISS index.\")\n",
    "\n",
    "# Lưu index và nhãn\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db3f5e-b484-46b9-97e3-7e838b1bb836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T11:48:19.297848Z",
     "iopub.status.busy": "2025-07-04T11:48:19.297636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index đã được tải thành công!\n",
      "   - Số lượng vectors: 29693\n",
      "   - Kích thước vector: 1408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing with FAISS:   0%|          | 0/11565 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751629705.566745     113 service.cc:148] XLA service 0x7a6fd8001f70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1751629705.567602     113 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1751629710.614410     113 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Testing with FAISS:   9%|▉         | 1048/11565 [01:53<16:44, 10.47it/s]"
     ]
    }
   ],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.95\n",
    "\n",
    "# ==== Load mô hình và FAISS index ====\n",
    "model = EfficientNetB2(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn FAISS index và labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"❌ Không tìm thấy FAISS index tại: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"❌ Không tìm thấy nhãn tại: {label_path}\")\n",
    "\n",
    "# Load FAISS index và labels\n",
    "try:\n",
    "    # Load index trực tiếp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"✅ FAISS index đã được tải thành công!\")\n",
    "    print(f\"   - Số lượng vectors: {index.ntotal}\")\n",
    "    print(f\"   - Kích thước vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy tất cả ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # Lấy danh sách tất cả ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "processing_times = []  # Danh sách lưu thời gian xử lý từng ảnh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # Bắt đầu đo thời gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Trích đặc trưng với mô hình\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chuẩn hóa L2 đặc trưng mẫu truy vấn để phù hợp với vector đã chuẩn hóa trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Thêm epsilon để tránh chia cho 0\n",
    "        \n",
    "        # Đảm bảo đặc trưng có định dạng phù hợp sau khi chuẩn hóa\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # Tìm kiếm k=1 điểm gần nhất trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Kiểm tra similarity score từ khoảng cách L2 với vector đã chuẩn hóa\n",
    "        # Với vector đã chuẩn hóa L2, khoảng cách Euclidean bình phương (D) và cosine similarity có quan hệ:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Khoảng cách FAISS là bình phương khoảng cách Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuyển đổi khoảng cách thành cosine similarity\n",
    "        \n",
    "        # Nếu similarity dưới ngưỡng, gán nhãn \"Khác\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nhãn \"Khác\"\n",
    "        else:\n",
    "            # Lấy nhãn dự đoán từ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # Lưu kết quả dự đoán và nhãn thực tế\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # Kết thúc đo thời gian và lưu lại\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In thông tin cho debug (có thể bỏ comment nếu cần kiểm tra)\n",
    "        # print(f\"Ảnh: {img_path}, Similarity: {similarity_score:.4f}, Nhãn dự đoán: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # Tính thời gian xử lý trung bình cho một ảnh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n⏱️ Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    print(f\"⏱️ Tổng thời gian xử lý: {sum(processing_times):.2f} giây cho {len(processing_times)} ảnh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    \n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "    \n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_faiss.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452cb37-3beb-4f37-b802-5718e09c580a",
   "metadata": {},
   "source": [
    "# Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48406a55-2731-483b-8fa6-3c5c967ddaa3",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "\n",
    "# Số batch\n",
    "num_batches = tf.data.experimental.cardinality(train_df).numpy()\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Khởi tạo Xception trong context của strategy\n",
    "with strategy.scope():\n",
    "    base_model_xception = Xception(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg',\n",
    "        input_shape=(image_size, image_size, 3)\n",
    "    )\n",
    "\n",
    "# Nơi lưu đặc trưng và nhãn\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "# Lặp qua từng batch\n",
    "for images, labels in tqdm(train_df, total=num_batches, desc=\"Extracting features with Xception\"):\n",
    "    images_pp = preprocess_input(images)\n",
    "    features_batch = base_model_xception(images_pp, training=False)\n",
    "    all_features.append(features_batch.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# Kết hợp đặc trưng và nhãn\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd8819-8f4e-4317-9332-2f2d05206808",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Chuẩn hóa L2 cho mỗi vector (độ dài = 1)\n",
    "def l2_normalize(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / (norms + 1e-10)  # thêm epsilon để tránh chia cho 0\n",
    "\n",
    "# Chuẩn hóa vector đặc trưng\n",
    "normalized_features = l2_normalize(features_array.astype('float32'))\n",
    "\n",
    "# Kích thước vector đặc trưng\n",
    "d = normalized_features.shape[1]\n",
    "\n",
    "# Khởi tạo index FAISS sử dụng khoảng cách Euclidean\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Thêm vector đã chuẩn hóa vào index\n",
    "index.add(normalized_features)\n",
    "print(\"Đã thêm\", index.ntotal, \"vector đã chuẩn hóa vào FAISS index.\")\n",
    "\n",
    "# Lưu index và nhãn\n",
    "faiss.write_index(index, \"faiss_features.index\")\n",
    "np.save(\"faiss_labels.npy\", labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c99e8-70a7-4d11-894e-996bd73102a4",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cài đặt các tham số\n",
    "image_size = 224\n",
    "test_path = \"/kaggle/input/spectrogram-movies-dataset/Spectrogram_Dataset/Test\"\n",
    "index_path = \"faiss_features.index\" \n",
    "label_path = \"faiss_labels.npy\"      \n",
    "confusion_output_path = \"confusion_matrix_faiss.jpg\"\n",
    "csv_output_path = \"classification_report_faiss.csv\"\n",
    "similarity_threshold = 0.95\n",
    "\n",
    "# ==== Load mô hình và FAISS index ====\n",
    "model = Xception(include_top=False, weights='imagenet', pooling='avg', input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Kiểm tra đường dẫn FAISS index và labels\n",
    "if not os.path.exists(index_path):\n",
    "    print(f\"❌ Không tìm thấy FAISS index tại: {index_path}\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"❌ Không tìm thấy nhãn tại: {label_path}\")\n",
    "\n",
    "# Load FAISS index và labels\n",
    "try:\n",
    "    # Load index trực tiếp cho CPU\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load labels\n",
    "    index_labels = np.load(label_path)\n",
    "\n",
    "    print(f\"✅ FAISS index đã được tải thành công!\")\n",
    "    print(f\"   - Số lượng vectors: {index.ntotal}\")\n",
    "    print(f\"   - Kích thước vector: {index.d}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Lỗi khi tải FAISS index: {e}\")\n",
    "\n",
    "# ==== Mapping classes ====\n",
    "classes = classes_test\n",
    "class_to_idx = {name: idx for idx, name in classes.items()}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "# ==== Duyệt tập test và lấy tất cả ảnh từ mỗi thư mục ====\n",
    "y_true = []\n",
    "y_pred = []\n",
    "all_images = []\n",
    "for class_name in os.listdir(test_path):\n",
    "    class_dir = os.path.join(test_path, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    # Lấy danh sách tất cả ảnh trong thư mục này\n",
    "    image_files = [\n",
    "        os.path.join(class_dir, f)\n",
    "        for f in os.listdir(class_dir)\n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ]\n",
    "    all_images.extend(image_files)\n",
    "\n",
    "# ==== Dự đoán ====\n",
    "processing_times = []  # Danh sách lưu thời gian xử lý từng ảnh\n",
    "for img_path in tqdm(all_images, desc=\"Testing with FAISS\"):\n",
    "    try:\n",
    "        start_time = time.time()  # Bắt đầu đo thời gian\n",
    "        \n",
    "        folder_name = os.path.basename(os.path.dirname(img_path))\n",
    "        if folder_name not in class_to_idx:\n",
    "            print(f\"❌ Folder không hợp lệ: {folder_name}\")\n",
    "            continue\n",
    "        img = image.load_img(img_path, target_size=(image_size, image_size))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        # Trích đặc trưng với mô hình\n",
    "        feature = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Chuẩn hóa L2 đặc trưng mẫu truy vấn để phù hợp với vector đã chuẩn hóa trong index\n",
    "        feature = feature / (np.linalg.norm(feature) + 1e-10)  # Thêm epsilon để tránh chia cho 0\n",
    "        \n",
    "        # Đảm bảo đặc trưng có định dạng phù hợp sau khi chuẩn hóa\n",
    "        feature = feature.astype(np.float32)\n",
    "        \n",
    "        # Tìm kiếm k=1 điểm gần nhất trong FAISS index\n",
    "        D, I = index.search(feature, 1)\n",
    "        \n",
    "        # Kiểm tra similarity score từ khoảng cách L2 với vector đã chuẩn hóa\n",
    "        # Với vector đã chuẩn hóa L2, khoảng cách Euclidean bình phương (D) và cosine similarity có quan hệ:\n",
    "        # cosine_similarity = 1 - D/2\n",
    "        # Khoảng cách FAISS là bình phương khoảng cách Euclidean\n",
    "        euclidean_dist_squared = D[0][0]\n",
    "        similarity_score = 1 - euclidean_dist_squared/2  # Chuyển đổi khoảng cách thành cosine similarity\n",
    "        \n",
    "        # Nếu similarity dưới ngưỡng, gán nhãn \"Khác\" (43)\n",
    "        if similarity_score < similarity_threshold:\n",
    "            pred_label = 46  # Nhãn \"Khác\"\n",
    "        else:\n",
    "            # Lấy nhãn dự đoán từ FAISS\n",
    "            pred_label_data = index_labels[I[0][0]]\n",
    "            if isinstance(pred_label_data, (np.ndarray, list)) and len(pred_label_data) > 1:\n",
    "                pred_label = int(np.argmax(pred_label_data)) + 1\n",
    "            else:\n",
    "                pred_label = int(pred_label_data)\n",
    "\n",
    "        # Lưu kết quả dự đoán và nhãn thực tế\n",
    "        y_true.append(class_to_idx[folder_name])\n",
    "        y_pred.append(pred_label)\n",
    "        \n",
    "        # Kết thúc đo thời gian và lưu lại\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        processing_times.append(processing_time)\n",
    "        \n",
    "        # In thông tin cho debug (có thể bỏ comment nếu cần kiểm tra)\n",
    "        # print(f\"Ảnh: {img_path}, Similarity: {similarity_score:.4f}, Nhãn dự đoán: {pred_label} ({idx_to_class[pred_label]})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi ảnh {img_path}: {e}\")\n",
    "        print(f\"  Hình dạng đặc trưng: {feature.shape}, Kiểu dữ liệu: {feature.dtype}\")\n",
    "\n",
    "# ==== Đánh giá ====\n",
    "if len(y_true) == 0:\n",
    "    print(\"❌ Không có dữ liệu để đánh giá!\")\n",
    "else:\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\n✅ Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"✅ Đúng: {np.sum(y_true == y_pred)} / ❌ Sai: {np.sum(y_true != y_pred)}\")\n",
    "    \n",
    "    # Tính thời gian xử lý trung bình cho một ảnh\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    print(f\"\\n⏱️ Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    print(f\"⏱️ Tổng thời gian xử lý: {sum(processing_times):.2f} giây cho {len(processing_times)} ảnh\")\n",
    "    \n",
    "    # ==== Confusion Matrix ====\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_labels = [classes[i] for i in sorted(classes.keys())]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    disp.plot(xticks_rotation=90, cmap='Blues', ax=ax, colorbar=False)\n",
    "    plt.title(\"Confusion Matrix (FAISS)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_output_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"\\n🖼️ Confusion matrix đã được lưu vào '{confusion_output_path}'\")\n",
    "    \n",
    "    # ==== Classification Report ====\n",
    "    print(\"\\n📄 Classification Report:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, target_names=class_labels,\n",
    "        digits=2, output_dict=True\n",
    "    )\n",
    "    # Chuyển báo cáo phân loại thành DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    if \"accuracy\" not in report_df.columns:\n",
    "        report_df[\"accuracy\"] = accuracy\n",
    "    print(report_df[[\"precision\", \"recall\", \"f1-score\", \"accuracy\"]])\n",
    "    \n",
    "    # Hiển thị các tham số chung cho toàn bộ chương trình\n",
    "    avg_precision = np.mean(report_df['precision'])\n",
    "    avg_recall = np.mean(report_df['recall'])\n",
    "    avg_f1 = np.mean(report_df['f1-score'])\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(\"\\n📝 Các tham số đánh giá chung:\")\n",
    "    print(f\"  - Precision: {avg_precision:.2f}\")\n",
    "    print(f\"  - Recall: {avg_recall:.2f}\")\n",
    "    print(f\"  - F1-score: {avg_f1:.2f}\")\n",
    "    print(f\"  - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  - Thời gian xử lý trung bình: {avg_processing_time:.4f} giây/ảnh\")\n",
    "    \n",
    "    # Lưu báo cáo chi tiết vào tệp CSV\n",
    "    report_df.to_csv(csv_output_path, index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại đã được lưu vào '{csv_output_path}'\")\n",
    "    \n",
    "    # Lưu các tham số đánh giá chung vào tệp CSV\n",
    "    evaluation_metrics = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1-score': avg_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "    evaluation_df = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_df.to_csv('evaluation_metrics_faiss.csv', index=False)\n",
    "    print(f\"\\n📊 Các tham số đánh giá chung đã được lưu vào 'evaluation_metrics_faiss.csv'\")\n",
    "    \n",
    "    # Lưu cả báo cáo phân loại và các tham số chung vào một tệp CSV\n",
    "    final_df = report_df.copy()\n",
    "    final_df['average_precision'] = avg_precision\n",
    "    final_df['average_recall'] = avg_recall\n",
    "    final_df['average_f1-score'] = avg_f1\n",
    "    final_df['average_accuracy'] = accuracy\n",
    "    final_df['avg_processing_time'] = avg_processing_time\n",
    "    final_df.to_csv('final_classification_report_faiss.csv', index=True)\n",
    "    print(f\"\\n📊 Báo cáo phân loại cuối cùng đã được lưu vào 'final_classification_report_faiss.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7799130,
     "sourceId": 12369593,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 70.338084,
   "end_time": "2025-05-11T02:57:09.159799",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-11T02:55:58.821715",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
